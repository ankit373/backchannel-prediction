2017-01-19 01:02:16 >>> version=v041-lstm-continous-adadelta
2017-01-19 01:02:16 >>> loading numpy file extract_pfiles_python/out/v040-lstm-continous-stride=2/train.input.npz
2017-01-19 01:02:17 >>> loading numpy file extract_pfiles_python/out/v040-lstm-continous-stride=2/train.output.npz
2017-01-19 01:02:17 >>> loading numpy file extract_pfiles_python/out/v040-lstm-continous-stride=2/validate.input.npz
2017-01-19 01:02:17 >>> loading numpy file extract_pfiles_python/out/v040-lstm-continous-stride=2/validate.output.npz
2017-01-19 01:02:17 >>> Training network with 71952 trainable out of 72252 total params.
2017-01-19 01:02:18 >>> Using adadelta with learning_rate=1.000000
2017-01-19 01:02:18 >>> Compiling theano functions...
2017-01-19 01:10:34 >>> Starting training...
2017-01-19 01:11:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-000.pkl
2017-01-19 01:11:35 >>> epoch: 0 took 60.953s
training loss:	0.691053
validation loss:	0.687717
validation error:	0.466080
2017-01-19 01:11:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:12:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-001.pkl
2017-01-19 01:12:35 >>> epoch: 1 took 60.123s
training loss:	0.681212
validation loss:	0.682199
validation error:	0.439083
2017-01-19 01:12:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:13:35 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-002.pkl
2017-01-19 01:13:36 >>> epoch: 2 took 61.438s
training loss:	0.681006
validation loss:	0.677672
validation error:	0.434040
2017-01-19 01:13:36 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:14:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-003.pkl
2017-01-19 01:14:38 >>> epoch: 3 took 61.933s
training loss:	0.677322
validation loss:	0.676558
validation error:	0.428766
2017-01-19 01:14:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:15:39 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-004.pkl
2017-01-19 01:15:40 >>> epoch: 4 took 61.936s
training loss:	0.674731
validation loss:	0.674745
validation error:	0.424408
2017-01-19 01:15:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:16:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-005.pkl
2017-01-19 01:16:41 >>> epoch: 5 took 60.977s
training loss:	0.672893
validation loss:	0.672395
validation error:	0.421592
2017-01-19 01:16:41 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:17:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-006.pkl
2017-01-19 01:17:42 >>> epoch: 6 took 60.754s
training loss:	0.671135
validation loss:	0.671009
validation error:	0.415721
2017-01-19 01:17:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:18:42 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-007.pkl
2017-01-19 01:18:43 >>> epoch: 7 took 61.421s
training loss:	0.669615
validation loss:	0.668895
validation error:	0.411690
2017-01-19 01:18:43 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:19:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-008.pkl
2017-01-19 01:19:46 >>> epoch: 8 took 62.068s
training loss:	0.669012
validation loss:	0.667960
validation error:	0.409531
2017-01-19 01:19:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:20:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-009.pkl
2017-01-19 01:20:47 >>> epoch: 9 took 61.933s
training loss:	0.667874
validation loss:	0.664990
validation error:	0.407074
2017-01-19 01:20:47 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:21:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-010.pkl
2017-01-19 01:21:50 >>> epoch: 10 took 62.363s
training loss:	0.667254
validation loss:	0.667233
validation error:	0.408489
2017-01-19 01:21:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:22:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-011.pkl
2017-01-19 01:22:50 >>> epoch: 11 took 60.055s
training loss:	0.666163
validation loss:	0.665426
validation error:	0.406792
2017-01-19 01:22:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:23:49 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-012.pkl
2017-01-19 01:23:51 >>> epoch: 12 took 61.041s
training loss:	0.665398
validation loss:	0.665430
validation error:	0.408400
2017-01-19 01:23:51 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:24:49 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-013.pkl
2017-01-19 01:24:51 >>> epoch: 13 took 60.266s
training loss:	0.664787
validation loss:	0.665287
validation error:	0.406297
2017-01-19 01:24:51 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:25:51 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-014.pkl
2017-01-19 01:25:53 >>> epoch: 14 took 61.435s
training loss:	0.663457
validation loss:	0.662095
validation error:	0.404092
2017-01-19 01:25:53 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:26:53 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-015.pkl
2017-01-19 01:26:54 >>> epoch: 15 took 61.613s
training loss:	0.662879
validation loss:	0.661436
validation error:	0.401500
2017-01-19 01:26:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:27:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-016.pkl
2017-01-19 01:27:56 >>> epoch: 16 took 61.399s
training loss:	0.661653
validation loss:	0.661434
validation error:	0.403393
2017-01-19 01:27:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:28:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-017.pkl
2017-01-19 01:28:57 >>> epoch: 17 took 61.150s
training loss:	0.661143
validation loss:	0.659013
validation error:	0.398408
2017-01-19 01:28:57 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:29:57 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-018.pkl
2017-01-19 01:29:59 >>> epoch: 18 took 61.889s
training loss:	0.660566
validation loss:	0.662623
validation error:	0.404598
2017-01-19 01:29:59 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:30:58 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-019.pkl
2017-01-19 01:31:00 >>> epoch: 19 took 61.275s
training loss:	0.660052
validation loss:	0.657906
validation error:	0.397732
2017-01-19 01:31:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:32:00 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-020.pkl
2017-01-19 01:32:01 >>> epoch: 20 took 61.405s
training loss:	0.659599
validation loss:	0.658884
validation error:	0.400694
2017-01-19 01:32:01 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:33:01 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-021.pkl
2017-01-19 01:33:03 >>> epoch: 21 took 61.662s
training loss:	0.659016
validation loss:	0.659773
validation error:	0.401002
2017-01-19 01:33:03 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:34:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-022.pkl
2017-01-19 01:34:04 >>> epoch: 22 took 61.010s
training loss:	0.658742
validation loss:	0.659380
validation error:	0.404667
2017-01-19 01:34:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:35:04 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-023.pkl
2017-01-19 01:35:05 >>> epoch: 23 took 61.411s
training loss:	0.658177
validation loss:	0.657217
validation error:	0.399395
2017-01-19 01:35:05 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:36:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-024.pkl
2017-01-19 01:36:07 >>> epoch: 24 took 61.337s
training loss:	0.657844
validation loss:	0.655560
validation error:	0.395074
2017-01-19 01:36:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:37:06 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-025.pkl
2017-01-19 01:37:07 >>> epoch: 25 took 60.331s
training loss:	0.657448
validation loss:	0.657418
validation error:	0.396074
2017-01-19 01:37:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:38:07 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-026.pkl
2017-01-19 01:38:08 >>> epoch: 26 took 61.276s
training loss:	0.657247
validation loss:	0.655871
validation error:	0.397636
2017-01-19 01:38:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:39:07 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-027.pkl
2017-01-19 01:39:09 >>> epoch: 27 took 60.673s
training loss:	0.657061
validation loss:	0.655734
validation error:	0.395150
2017-01-19 01:39:09 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:40:07 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-028.pkl
2017-01-19 01:40:09 >>> epoch: 28 took 59.912s
training loss:	0.656736
validation loss:	0.655245
validation error:	0.394154
2017-01-19 01:40:09 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:41:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-029.pkl
2017-01-19 01:41:10 >>> epoch: 29 took 61.076s
training loss:	0.656307
validation loss:	0.655671
validation error:	0.393444
2017-01-19 01:41:10 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:42:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-030.pkl
2017-01-19 01:42:12 >>> epoch: 30 took 61.844s
training loss:	0.655589
validation loss:	0.655570
validation error:	0.398719
2017-01-19 01:42:12 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:43:11 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-031.pkl
2017-01-19 01:43:13 >>> epoch: 31 took 61.077s
training loss:	0.655157
validation loss:	0.655608
validation error:	0.394705
2017-01-19 01:43:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:44:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-032.pkl
2017-01-19 01:44:14 >>> epoch: 32 took 60.650s
training loss:	0.654772
validation loss:	0.655838
validation error:	0.398886
2017-01-19 01:44:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:45:13 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-033.pkl
2017-01-19 01:45:14 >>> epoch: 33 took 60.809s
training loss:	0.654276
validation loss:	0.653382
validation error:	0.392665
2017-01-19 01:45:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:46:14 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-034.pkl
2017-01-19 01:46:16 >>> epoch: 34 took 61.239s
training loss:	0.653828
validation loss:	0.656771
validation error:	0.400016
2017-01-19 01:46:16 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:47:16 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-035.pkl
2017-01-19 01:47:17 >>> epoch: 35 took 61.660s
training loss:	0.653513
validation loss:	0.653203
validation error:	0.391400
2017-01-19 01:47:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:48:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-036.pkl
2017-01-19 01:48:18 >>> epoch: 36 took 61.148s
training loss:	0.652916
validation loss:	0.652544
validation error:	0.390886
2017-01-19 01:48:18 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:49:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-037.pkl
2017-01-19 01:49:19 >>> epoch: 37 took 60.640s
training loss:	0.652363
validation loss:	0.652510
validation error:	0.389935
2017-01-19 01:49:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:50:18 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-038.pkl
2017-01-19 01:50:20 >>> epoch: 38 took 60.956s
training loss:	0.652305
validation loss:	0.651243
validation error:	0.390621
2017-01-19 01:50:20 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:51:19 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-039.pkl
2017-01-19 01:51:21 >>> epoch: 39 took 61.079s
training loss:	0.651869
validation loss:	0.651892
validation error:	0.389685
2017-01-19 01:51:21 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:52:21 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-040.pkl
2017-01-19 01:52:23 >>> epoch: 40 took 61.642s
training loss:	0.651512
validation loss:	0.653079
validation error:	0.392063
2017-01-19 01:52:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:53:21 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-041.pkl
2017-01-19 01:53:23 >>> epoch: 41 took 60.104s
training loss:	0.651030
validation loss:	0.650265
validation error:	0.386777
2017-01-19 01:53:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:54:21 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-042.pkl
2017-01-19 01:54:23 >>> epoch: 42 took 60.392s
training loss:	0.650880
validation loss:	0.649281
validation error:	0.388558
2017-01-19 01:54:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:55:23 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-043.pkl
2017-01-19 01:55:25 >>> epoch: 43 took 61.565s
training loss:	0.650322
validation loss:	0.651092
validation error:	0.390408
2017-01-19 01:55:25 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:56:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-044.pkl
2017-01-19 01:56:27 >>> epoch: 44 took 61.965s
training loss:	0.650054
validation loss:	0.653851
validation error:	0.392804
2017-01-19 01:56:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:57:27 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-045.pkl
2017-01-19 01:57:29 >>> epoch: 45 took 62.002s
training loss:	0.649369
validation loss:	0.654206
validation error:	0.391049
2017-01-19 01:57:29 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:58:28 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-046.pkl
2017-01-19 01:58:29 >>> epoch: 46 took 60.449s
training loss:	0.649250
validation loss:	0.650502
validation error:	0.392272
2017-01-19 01:58:29 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 01:59:28 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-047.pkl
2017-01-19 01:59:29 >>> epoch: 47 took 60.146s
training loss:	0.648702
validation loss:	0.647771
validation error:	0.384721
2017-01-19 01:59:29 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:00:29 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-048.pkl
2017-01-19 02:00:31 >>> epoch: 48 took 61.232s
training loss:	0.648028
validation loss:	0.647851
validation error:	0.385371
2017-01-19 02:00:31 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:01:29 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-049.pkl
2017-01-19 02:01:31 >>> epoch: 49 took 60.373s
training loss:	0.647681
validation loss:	0.648167
validation error:	0.388368
2017-01-19 02:01:31 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:02:29 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-050.pkl
2017-01-19 02:02:31 >>> epoch: 50 took 59.944s
training loss:	0.647168
validation loss:	0.649576
validation error:	0.388641
2017-01-19 02:02:31 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:03:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-051.pkl
2017-01-19 02:03:32 >>> epoch: 51 took 61.467s
training loss:	0.646631
validation loss:	0.646569
validation error:	0.385453
2017-01-19 02:03:32 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:04:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-052.pkl
2017-01-19 02:04:33 >>> epoch: 52 took 60.463s
training loss:	0.646232
validation loss:	0.646548
validation error:	0.383500
2017-01-19 02:04:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:05:32 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-053.pkl
2017-01-19 02:05:34 >>> epoch: 53 took 60.873s
training loss:	0.645606
validation loss:	0.644925
validation error:	0.382020
2017-01-19 02:05:34 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:06:34 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-054.pkl
2017-01-19 02:06:35 >>> epoch: 54 took 61.478s
training loss:	0.645056
validation loss:	0.644593
validation error:	0.384297
2017-01-19 02:06:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:07:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-055.pkl
2017-01-19 02:07:37 >>> epoch: 55 took 62.203s
training loss:	0.644643
validation loss:	0.644161
validation error:	0.381605
2017-01-19 02:07:37 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:08:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-056.pkl
2017-01-19 02:08:39 >>> epoch: 56 took 61.075s
training loss:	0.643984
validation loss:	0.643182
validation error:	0.380701
2017-01-19 02:08:39 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:09:38 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-057.pkl
2017-01-19 02:09:40 >>> epoch: 57 took 61.512s
training loss:	0.643491
validation loss:	0.645107
validation error:	0.382926
2017-01-19 02:09:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:10:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-058.pkl
2017-01-19 02:10:42 >>> epoch: 58 took 61.493s
training loss:	0.643172
validation loss:	0.642715
validation error:	0.379688
2017-01-19 02:10:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:11:42 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-059.pkl
2017-01-19 02:11:44 >>> epoch: 59 took 62.702s
training loss:	0.642100
validation loss:	0.643438
validation error:	0.385386
2017-01-19 02:11:44 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:12:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-060.pkl
2017-01-19 02:12:46 >>> epoch: 60 took 61.322s
training loss:	0.641569
validation loss:	0.641891
validation error:	0.379112
2017-01-19 02:12:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:13:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-061.pkl
2017-01-19 02:13:46 >>> epoch: 61 took 60.649s
training loss:	0.641117
validation loss:	0.642109
validation error:	0.381125
2017-01-19 02:13:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:14:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-062.pkl
2017-01-19 02:14:48 >>> epoch: 62 took 61.855s
training loss:	0.640685
validation loss:	0.647854
validation error:	0.389002
2017-01-19 02:14:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:15:47 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-063.pkl
2017-01-19 02:15:49 >>> epoch: 63 took 61.067s
training loss:	0.640131
validation loss:	0.640393
validation error:	0.376730
2017-01-19 02:15:49 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:16:49 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-064.pkl
2017-01-19 02:16:50 >>> epoch: 64 took 61.168s
training loss:	0.639601
validation loss:	0.638696
validation error:	0.375065
2017-01-19 02:16:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:17:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-065.pkl
2017-01-19 02:17:52 >>> epoch: 65 took 61.334s
training loss:	0.639087
validation loss:	0.640542
validation error:	0.377879
2017-01-19 02:17:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:18:51 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-066.pkl
2017-01-19 02:18:52 >>> epoch: 66 took 60.698s
training loss:	0.638433
validation loss:	0.642430
validation error:	0.379051
2017-01-19 02:18:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:19:52 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-067.pkl
2017-01-19 02:19:53 >>> epoch: 67 took 61.123s
training loss:	0.638125
validation loss:	0.642095
validation error:	0.378132
2017-01-19 02:19:53 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:20:53 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-068.pkl
2017-01-19 02:20:55 >>> epoch: 68 took 61.141s
training loss:	0.637799
validation loss:	0.639602
validation error:	0.377156
2017-01-19 02:20:55 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:21:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-069.pkl
2017-01-19 02:21:56 >>> epoch: 69 took 61.186s
training loss:	0.637210
validation loss:	0.638445
validation error:	0.374605
2017-01-19 02:21:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:22:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-070.pkl
2017-01-19 02:22:56 >>> epoch: 70 took 60.701s
training loss:	0.636929
validation loss:	0.639909
validation error:	0.377949
2017-01-19 02:22:57 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:23:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-071.pkl
2017-01-19 02:23:57 >>> epoch: 71 took 60.397s
training loss:	0.636401
validation loss:	0.637393
validation error:	0.373797
2017-01-19 02:23:57 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:24:56 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-072.pkl
2017-01-19 02:24:57 >>> epoch: 72 took 60.435s
training loss:	0.635998
validation loss:	0.637979
validation error:	0.375804
2017-01-19 02:24:57 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:25:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-073.pkl
2017-01-19 02:25:57 >>> epoch: 73 took 59.774s
training loss:	0.635625
validation loss:	0.638007
validation error:	0.375136
2017-01-19 02:25:57 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:26:56 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-074.pkl
2017-01-19 02:26:58 >>> epoch: 74 took 60.670s
training loss:	0.635327
validation loss:	0.643705
validation error:	0.378888
2017-01-19 02:26:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:27:57 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-075.pkl
2017-01-19 02:27:59 >>> epoch: 75 took 61.360s
training loss:	0.635060
validation loss:	0.642459
validation error:	0.379888
2017-01-19 02:27:59 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:28:59 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-076.pkl
2017-01-19 02:29:01 >>> epoch: 76 took 61.447s
training loss:	0.634236
validation loss:	0.636293
validation error:	0.372498
2017-01-19 02:29:01 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:30:00 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-077.pkl
2017-01-19 02:30:01 >>> epoch: 77 took 60.868s
training loss:	0.634144
validation loss:	0.638104
validation error:	0.374054
2017-01-19 02:30:01 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:31:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-078.pkl
2017-01-19 02:31:03 >>> epoch: 78 took 61.888s
training loss:	0.633853
validation loss:	0.636029
validation error:	0.371850
2017-01-19 02:31:03 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:32:03 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-079.pkl
2017-01-19 02:32:05 >>> epoch: 79 took 61.510s
training loss:	0.633332
validation loss:	0.639247
validation error:	0.374016
2017-01-19 02:32:05 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:33:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-080.pkl
2017-01-19 02:33:07 >>> epoch: 80 took 62.133s
training loss:	0.633382
validation loss:	0.636977
validation error:	0.374908
2017-01-19 02:33:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:34:06 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-081.pkl
2017-01-19 02:34:08 >>> epoch: 81 took 60.810s
training loss:	0.632799
validation loss:	0.635019
validation error:	0.370692
2017-01-19 02:34:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:35:07 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-082.pkl
2017-01-19 02:35:08 >>> epoch: 82 took 60.594s
training loss:	0.632488
validation loss:	0.635553
validation error:	0.371614
2017-01-19 02:35:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:36:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-083.pkl
2017-01-19 02:36:10 >>> epoch: 83 took 61.188s
training loss:	0.632217
validation loss:	0.639560
validation error:	0.377391
2017-01-19 02:36:10 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:37:09 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-084.pkl
2017-01-19 02:37:11 >>> epoch: 84 took 61.550s
training loss:	0.631413
validation loss:	0.637700
validation error:	0.373759
2017-01-19 02:37:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:38:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-085.pkl
2017-01-19 02:38:12 >>> epoch: 85 took 61.219s
training loss:	0.631493
validation loss:	0.635501
validation error:	0.372252
2017-01-19 02:38:12 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:39:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-086.pkl
2017-01-19 02:39:14 >>> epoch: 86 took 61.258s
training loss:	0.630997
validation loss:	0.638789
validation error:	0.378170
2017-01-19 02:39:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:40:11 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-087.pkl
2017-01-19 02:40:13 >>> epoch: 87 took 59.486s
training loss:	0.630788
validation loss:	0.637423
validation error:	0.373589
2017-01-19 02:40:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:41:13 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-088.pkl
2017-01-19 02:41:15 >>> epoch: 88 took 61.943s
training loss:	0.630478
validation loss:	0.641828
validation error:	0.378520
2017-01-19 02:41:15 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:42:14 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-089.pkl
2017-01-19 02:42:16 >>> epoch: 89 took 60.793s
training loss:	0.629962
validation loss:	0.636334
validation error:	0.372875
2017-01-19 02:42:16 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:43:16 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-090.pkl
2017-01-19 02:43:18 >>> epoch: 90 took 61.930s
training loss:	0.629961
validation loss:	0.634870
validation error:	0.371866
2017-01-19 02:43:18 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:44:16 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-091.pkl
2017-01-19 02:44:18 >>> epoch: 91 took 60.229s
training loss:	0.629453
validation loss:	0.635050
validation error:	0.372181
2017-01-19 02:44:18 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:45:18 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-092.pkl
2017-01-19 02:45:19 >>> epoch: 92 took 61.189s
training loss:	0.629148
validation loss:	0.636328
validation error:	0.372576
2017-01-19 02:45:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:46:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-093.pkl
2017-01-19 02:46:19 >>> epoch: 93 took 59.685s
training loss:	0.628786
validation loss:	0.636000
validation error:	0.371614
2017-01-19 02:46:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:47:19 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-094.pkl
2017-01-19 02:47:20 >>> epoch: 94 took 61.463s
training loss:	0.628586
validation loss:	0.635110
validation error:	0.369257
2017-01-19 02:47:20 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:48:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-095.pkl
2017-01-19 02:48:21 >>> epoch: 95 took 61.117s
training loss:	0.628019
validation loss:	0.634900
validation error:	0.371078
2017-01-19 02:48:21 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:49:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-096.pkl
2017-01-19 02:49:22 >>> epoch: 96 took 60.476s
training loss:	0.627752
validation loss:	0.634528
validation error:	0.369790
2017-01-19 02:49:22 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:50:22 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-097.pkl
2017-01-19 02:50:24 >>> epoch: 97 took 62.117s
training loss:	0.627415
validation loss:	0.635611
validation error:	0.369746
2017-01-19 02:50:24 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:51:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-098.pkl
2017-01-19 02:51:26 >>> epoch: 98 took 62.384s
training loss:	0.627036
validation loss:	0.636487
validation error:	0.371714
2017-01-19 02:51:26 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:52:27 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-099.pkl
2017-01-19 02:52:28 >>> epoch: 99 took 61.874s
training loss:	0.626642
validation loss:	0.634192
validation error:	0.371036
2017-01-19 02:52:28 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:53:28 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-100.pkl
2017-01-19 02:53:30 >>> epoch: 100 took 62.007s
training loss:	0.626488
validation loss:	0.636071
validation error:	0.372268
2017-01-19 02:53:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:54:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-101.pkl
2017-01-19 02:54:33 >>> epoch: 101 took 62.616s
training loss:	0.625807
validation loss:	0.634279
validation error:	0.369746
2017-01-19 02:54:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:55:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-102.pkl
2017-01-19 02:55:35 >>> epoch: 102 took 61.906s
training loss:	0.625672
validation loss:	0.634427
validation error:	0.370534
2017-01-19 02:55:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:56:35 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-103.pkl
2017-01-19 02:56:37 >>> epoch: 103 took 62.318s
training loss:	0.625303
validation loss:	0.635592
validation error:	0.371199
2017-01-19 02:56:37 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:57:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-104.pkl
2017-01-19 02:57:39 >>> epoch: 104 took 61.551s
training loss:	0.624902
validation loss:	0.635761
validation error:	0.371500
2017-01-19 02:57:39 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:58:39 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-105.pkl
2017-01-19 02:58:41 >>> epoch: 105 took 62.411s
training loss:	0.624526
validation loss:	0.634857
validation error:	0.369241
2017-01-19 02:58:41 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 02:59:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-106.pkl
2017-01-19 02:59:42 >>> epoch: 106 took 60.915s
training loss:	0.624292
validation loss:	0.637416
validation error:	0.372647
2017-01-19 02:59:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:00:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-107.pkl
2017-01-19 03:00:43 >>> epoch: 107 took 60.711s
training loss:	0.623896
validation loss:	0.639422
validation error:	0.374103
2017-01-19 03:00:43 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:01:43 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-108.pkl
2017-01-19 03:01:45 >>> epoch: 108 took 62.394s
training loss:	0.623679
validation loss:	0.635900
validation error:	0.370654
2017-01-19 03:01:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:02:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-109.pkl
2017-01-19 03:02:45 >>> epoch: 109 took 60.324s
training loss:	0.623163
validation loss:	0.642211
validation error:	0.375685
2017-01-19 03:02:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:03:45 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-110.pkl
2017-01-19 03:03:46 >>> epoch: 110 took 60.998s
training loss:	0.623004
validation loss:	0.635043
validation error:	0.370353
2017-01-19 03:03:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:04:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-111.pkl
2017-01-19 03:04:48 >>> epoch: 111 took 61.717s
training loss:	0.622311
validation loss:	0.636115
validation error:	0.372786
2017-01-19 03:04:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:05:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-112.pkl
2017-01-19 03:05:50 >>> epoch: 112 took 61.604s
training loss:	0.621968
validation loss:	0.636527
validation error:	0.372185
2017-01-19 03:05:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:06:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-113.pkl
2017-01-19 03:06:51 >>> epoch: 113 took 61.546s
training loss:	0.621520
validation loss:	0.636374
validation error:	0.371011
2017-01-19 03:06:51 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:07:51 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-114.pkl
2017-01-19 03:07:52 >>> epoch: 114 took 60.863s
training loss:	0.621377
validation loss:	0.635621
validation error:	0.370433
2017-01-19 03:07:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:08:52 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-115.pkl
2017-01-19 03:08:53 >>> epoch: 115 took 61.276s
training loss:	0.620780
validation loss:	0.639250
validation error:	0.372362
2017-01-19 03:08:53 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:09:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-116.pkl
2017-01-19 03:09:56 >>> epoch: 116 took 62.786s
training loss:	0.620509
validation loss:	0.638658
validation error:	0.372886
2017-01-19 03:09:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:10:56 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-117.pkl
2017-01-19 03:10:58 >>> epoch: 117 took 61.915s
training loss:	0.619991
validation loss:	0.638312
validation error:	0.374174
2017-01-19 03:10:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:11:58 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-118.pkl
2017-01-19 03:12:00 >>> epoch: 118 took 61.931s
training loss:	0.619485
validation loss:	0.638446
validation error:	0.373938
2017-01-19 03:12:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:13:00 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-119.pkl
2017-01-19 03:13:02 >>> epoch: 119 took 61.811s
training loss:	0.618943
validation loss:	0.636884
validation error:	0.371853
2017-01-19 03:13:02 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:14:01 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-120.pkl
2017-01-19 03:14:03 >>> epoch: 120 took 61.202s
training loss:	0.618620
validation loss:	0.638013
validation error:	0.373196
2017-01-19 03:14:03 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:15:03 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-121.pkl
2017-01-19 03:15:05 >>> epoch: 121 took 61.477s
training loss:	0.618172
validation loss:	0.638397
validation error:	0.375509
2017-01-19 03:15:05 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:16:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-122.pkl
2017-01-19 03:16:07 >>> epoch: 122 took 62.410s
training loss:	0.617750
validation loss:	0.639500
validation error:	0.373411
2017-01-19 03:16:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:17:07 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-123.pkl
2017-01-19 03:17:09 >>> epoch: 123 took 61.667s
training loss:	0.617143
validation loss:	0.639217
validation error:	0.375250
2017-01-19 03:17:09 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:18:09 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-124.pkl
2017-01-19 03:18:11 >>> epoch: 124 took 62.575s
training loss:	0.616968
validation loss:	0.645496
validation error:	0.377652
2017-01-19 03:18:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:19:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-125.pkl
2017-01-19 03:19:14 >>> epoch: 125 took 62.594s
training loss:	0.616432
validation loss:	0.641832
validation error:	0.375592
2017-01-19 03:19:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:20:13 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-126.pkl
2017-01-19 03:20:15 >>> epoch: 126 took 61.254s
training loss:	0.616378
validation loss:	0.639594
validation error:	0.373103
2017-01-19 03:20:15 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:21:14 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-127.pkl
2017-01-19 03:21:16 >>> epoch: 127 took 61.036s
training loss:	0.615393
validation loss:	0.642642
validation error:	0.378324
2017-01-19 03:21:16 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:22:15 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-128.pkl
2017-01-19 03:22:17 >>> epoch: 128 took 61.090s
training loss:	0.615139
validation loss:	0.642180
validation error:	0.374609
2017-01-19 03:22:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:23:19 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-129.pkl
2017-01-19 03:23:21 >>> epoch: 129 took 63.870s
training loss:	0.614565
validation loss:	0.639803
validation error:	0.375538
2017-01-19 03:23:21 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:24:21 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-130.pkl
2017-01-19 03:24:23 >>> epoch: 130 took 61.961s
training loss:	0.614037
validation loss:	0.640272
validation error:	0.375703
2017-01-19 03:24:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:25:23 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-131.pkl
2017-01-19 03:25:25 >>> epoch: 131 took 61.758s
training loss:	0.613690
validation loss:	0.639757
validation error:	0.374136
2017-01-19 03:25:25 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:26:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-132.pkl
2017-01-19 03:26:27 >>> epoch: 132 took 62.038s
training loss:	0.613509
validation loss:	0.642671
validation error:	0.376337
2017-01-19 03:26:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:27:27 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-133.pkl
2017-01-19 03:27:29 >>> epoch: 133 took 62.023s
training loss:	0.612831
validation loss:	0.646195
validation error:	0.376411
2017-01-19 03:27:29 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:28:29 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-134.pkl
2017-01-19 03:28:31 >>> epoch: 134 took 61.762s
training loss:	0.612056
validation loss:	0.642791
validation error:	0.376279
2017-01-19 03:28:31 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:29:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-135.pkl
2017-01-19 03:29:33 >>> epoch: 135 took 62.184s
training loss:	0.611316
validation loss:	0.641414
validation error:	0.378964
2017-01-19 03:29:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:30:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-136.pkl
2017-01-19 03:30:35 >>> epoch: 136 took 61.960s
training loss:	0.611705
validation loss:	0.642069
validation error:	0.374116
2017-01-19 03:30:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:31:35 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-137.pkl
2017-01-19 03:31:37 >>> epoch: 137 took 61.963s
training loss:	0.610278
validation loss:	0.643701
validation error:	0.376056
2017-01-19 03:31:37 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:32:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-138.pkl
2017-01-19 03:32:39 >>> epoch: 138 took 62.047s
training loss:	0.610673
validation loss:	0.642343
validation error:	0.376221
2017-01-19 03:32:39 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:33:39 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-139.pkl
2017-01-19 03:33:41 >>> epoch: 139 took 61.846s
training loss:	0.610049
validation loss:	0.640343
validation error:	0.374585
2017-01-19 03:33:41 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:34:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-140.pkl
2017-01-19 03:34:42 >>> epoch: 140 took 61.747s
training loss:	0.609298
validation loss:	0.645581
validation error:	0.376350
2017-01-19 03:34:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:35:42 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-141.pkl
2017-01-19 03:35:44 >>> epoch: 141 took 61.748s
training loss:	0.609037
validation loss:	0.646373
validation error:	0.377127
2017-01-19 03:35:44 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:36:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-142.pkl
2017-01-19 03:36:46 >>> epoch: 142 took 61.816s
training loss:	0.608472
validation loss:	0.645540
validation error:	0.377366
2017-01-19 03:36:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:37:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-143.pkl
2017-01-19 03:37:48 >>> epoch: 143 took 61.825s
training loss:	0.607737
validation loss:	0.645358
validation error:	0.378679
2017-01-19 03:37:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:38:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-144.pkl
2017-01-19 03:38:50 >>> epoch: 144 took 61.925s
training loss:	0.606919
validation loss:	0.655450
validation error:	0.381442
2017-01-19 03:38:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:39:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-145.pkl
2017-01-19 03:39:52 >>> epoch: 145 took 62.263s
training loss:	0.606853
validation loss:	0.647626
validation error:	0.376920
2017-01-19 03:39:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:40:52 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-146.pkl
2017-01-19 03:40:54 >>> epoch: 146 took 62.021s
training loss:	0.606336
validation loss:	0.645878
validation error:	0.376641
2017-01-19 03:40:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:41:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-147.pkl
2017-01-19 03:41:56 >>> epoch: 147 took 62.271s
training loss:	0.605468
validation loss:	0.646164
validation error:	0.377438
2017-01-19 03:41:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:42:57 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-148.pkl
2017-01-19 03:42:59 >>> epoch: 148 took 62.304s
training loss:	0.604986
validation loss:	0.646012
validation error:	0.377763
2017-01-19 03:42:59 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:43:59 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-149.pkl
2017-01-19 03:44:00 >>> epoch: 149 took 61.882s
training loss:	0.604488
validation loss:	0.646939
validation error:	0.377016
2017-01-19 03:44:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:45:01 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-150.pkl
2017-01-19 03:45:03 >>> epoch: 150 took 62.311s
training loss:	0.603770
validation loss:	0.646725
validation error:	0.378029
2017-01-19 03:45:03 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:46:03 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-151.pkl
2017-01-19 03:46:04 >>> epoch: 151 took 61.781s
training loss:	0.602870
validation loss:	0.648198
validation error:	0.381996
2017-01-19 03:46:05 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:47:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-152.pkl
2017-01-19 03:47:07 >>> epoch: 152 took 62.109s
training loss:	0.602831
validation loss:	0.651657
validation error:	0.380315
2017-01-19 03:47:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:48:07 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-153.pkl
2017-01-19 03:48:09 >>> epoch: 153 took 61.933s
training loss:	0.601785
validation loss:	0.652918
validation error:	0.378871
2017-01-19 03:48:09 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:49:09 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-154.pkl
2017-01-19 03:49:11 >>> epoch: 154 took 62.031s
training loss:	0.601188
validation loss:	0.647122
validation error:	0.378728
2017-01-19 03:49:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:50:11 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-155.pkl
2017-01-19 03:50:12 >>> epoch: 155 took 61.925s
training loss:	0.600745
validation loss:	0.653968
validation error:	0.379413
2017-01-19 03:50:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:51:13 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-156.pkl
2017-01-19 03:51:15 >>> epoch: 156 took 62.286s
training loss:	0.600578
validation loss:	0.655528
validation error:	0.381364
2017-01-19 03:51:15 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:52:15 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-157.pkl
2017-01-19 03:52:17 >>> epoch: 157 took 62.207s
training loss:	0.599957
validation loss:	0.651083
validation error:	0.381549
2017-01-19 03:52:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:53:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-158.pkl
2017-01-19 03:53:19 >>> epoch: 158 took 62.196s
training loss:	0.599066
validation loss:	0.652721
validation error:	0.382542
2017-01-19 03:53:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:54:19 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-159.pkl
2017-01-19 03:54:21 >>> epoch: 159 took 61.952s
training loss:	0.598684
validation loss:	0.657757
validation error:	0.381906
2017-01-19 03:54:21 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:55:21 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-160.pkl
2017-01-19 03:55:23 >>> epoch: 160 took 61.781s
training loss:	0.597413
validation loss:	0.651690
validation error:	0.380397
2017-01-19 03:55:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:56:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-161.pkl
2017-01-19 03:56:27 >>> epoch: 161 took 63.877s
training loss:	0.597964
validation loss:	0.657130
validation error:	0.380433
2017-01-19 03:56:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:57:27 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-162.pkl
2017-01-19 03:57:29 >>> epoch: 162 took 62.355s
training loss:	0.596377
validation loss:	0.653977
validation error:	0.378933
2017-01-19 03:57:29 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:58:29 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-163.pkl
2017-01-19 03:58:31 >>> epoch: 163 took 61.677s
training loss:	0.596566
validation loss:	0.654537
validation error:	0.383319
2017-01-19 03:58:31 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 03:59:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-164.pkl
2017-01-19 03:59:33 >>> epoch: 164 took 61.779s
training loss:	0.595295
validation loss:	0.656405
validation error:	0.380797
2017-01-19 03:59:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:00:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-165.pkl
2017-01-19 04:00:34 >>> epoch: 165 took 61.651s
training loss:	0.594893
validation loss:	0.658586
validation error:	0.382221
2017-01-19 04:00:34 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:01:34 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-166.pkl
2017-01-19 04:01:36 >>> epoch: 166 took 61.729s
training loss:	0.594669
validation loss:	0.659532
validation error:	0.381826
2017-01-19 04:01:36 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:02:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-167.pkl
2017-01-19 04:02:38 >>> epoch: 167 took 61.792s
training loss:	0.593756
validation loss:	0.655975
validation error:	0.381692
2017-01-19 04:02:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:03:38 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-168.pkl
2017-01-19 04:03:40 >>> epoch: 168 took 61.966s
training loss:	0.592966
validation loss:	0.656861
validation error:	0.383397
2017-01-19 04:03:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:04:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-169.pkl
2017-01-19 04:04:42 >>> epoch: 169 took 61.846s
training loss:	0.592255
validation loss:	0.664656
validation error:	0.384616
2017-01-19 04:04:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:05:42 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-170.pkl
2017-01-19 04:05:44 >>> epoch: 170 took 61.965s
training loss:	0.591893
validation loss:	0.661103
validation error:	0.382022
2017-01-19 04:05:44 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:06:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-171.pkl
2017-01-19 04:06:46 >>> epoch: 171 took 62.192s
training loss:	0.591216
validation loss:	0.662190
validation error:	0.383368
2017-01-19 04:06:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:07:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-172.pkl
2017-01-19 04:07:48 >>> epoch: 172 took 61.897s
training loss:	0.590785
validation loss:	0.662529
validation error:	0.382152
2017-01-19 04:07:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:08:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-173.pkl
2017-01-19 04:08:50 >>> epoch: 173 took 62.099s
training loss:	0.589540
validation loss:	0.665205
validation error:	0.386362
2017-01-19 04:08:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:09:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-174.pkl
2017-01-19 04:09:52 >>> epoch: 174 took 62.067s
training loss:	0.589750
validation loss:	0.671367
validation error:	0.389453
2017-01-19 04:09:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:10:52 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-175.pkl
2017-01-19 04:10:54 >>> epoch: 175 took 62.166s
training loss:	0.588647
validation loss:	0.663367
validation error:	0.383594
2017-01-19 04:10:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:11:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-176.pkl
2017-01-19 04:11:56 >>> epoch: 176 took 62.357s
training loss:	0.587750
validation loss:	0.665866
validation error:	0.385172
2017-01-19 04:11:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:12:57 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-177.pkl
2017-01-19 04:12:58 >>> epoch: 177 took 61.819s
training loss:	0.587533
validation loss:	0.663032
validation error:	0.385027
2017-01-19 04:12:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:13:58 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-178.pkl
2017-01-19 04:14:00 >>> epoch: 178 took 61.726s
training loss:	0.586171
validation loss:	0.664755
validation error:	0.385103
2017-01-19 04:14:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:15:00 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-179.pkl
2017-01-19 04:15:02 >>> epoch: 179 took 61.927s
training loss:	0.585610
validation loss:	0.665583
validation error:	0.383879
2017-01-19 04:15:02 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:16:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-180.pkl
2017-01-19 04:16:04 >>> epoch: 180 took 62.035s
training loss:	0.585261
validation loss:	0.675784
validation error:	0.388170
2017-01-19 04:16:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:17:04 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-181.pkl
2017-01-19 04:17:06 >>> epoch: 181 took 62.204s
training loss:	0.584201
validation loss:	0.667010
validation error:	0.386069
2017-01-19 04:17:06 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:18:06 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-182.pkl
2017-01-19 04:18:08 >>> epoch: 182 took 61.818s
training loss:	0.583824
validation loss:	0.687807
validation error:	0.393922
2017-01-19 04:18:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:19:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-183.pkl
2017-01-19 04:19:10 >>> epoch: 183 took 61.929s
training loss:	0.582742
validation loss:	0.674289
validation error:	0.387819
2017-01-19 04:19:10 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:20:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-184.pkl
2017-01-19 04:20:12 >>> epoch: 184 took 61.755s
training loss:	0.582800
validation loss:	0.672933
validation error:	0.385757
2017-01-19 04:20:12 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:21:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-185.pkl
2017-01-19 04:21:13 >>> epoch: 185 took 61.828s
training loss:	0.581317
validation loss:	0.671743
validation error:	0.386696
2017-01-19 04:21:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:22:14 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-186.pkl
2017-01-19 04:22:15 >>> epoch: 186 took 61.827s
training loss:	0.580760
validation loss:	0.669164
validation error:	0.387056
2017-01-19 04:22:15 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:23:16 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-187.pkl
2017-01-19 04:23:17 >>> epoch: 187 took 62.164s
training loss:	0.580358
validation loss:	0.669112
validation error:	0.387188
2017-01-19 04:23:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:24:18 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-188.pkl
2017-01-19 04:24:19 >>> epoch: 188 took 62.024s
training loss:	0.579766
validation loss:	0.677316
validation error:	0.387585
2017-01-19 04:24:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:25:21 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-189.pkl
2017-01-19 04:25:23 >>> epoch: 189 took 63.564s
training loss:	0.578477
validation loss:	0.669809
validation error:	0.385603
2017-01-19 04:25:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:26:24 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-190.pkl
2017-01-19 04:26:25 >>> epoch: 190 took 62.473s
training loss:	0.577353
validation loss:	0.678468
validation error:	0.388942
2017-01-19 04:26:25 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:27:27 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-191.pkl
2017-01-19 04:27:28 >>> epoch: 191 took 62.733s
training loss:	0.577178
validation loss:	0.681065
validation error:	0.389333
2017-01-19 04:27:28 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:28:29 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-192.pkl
2017-01-19 04:28:31 >>> epoch: 192 took 62.611s
training loss:	0.576393
validation loss:	0.680947
validation error:	0.389324
2017-01-19 04:28:31 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:29:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-193.pkl
2017-01-19 04:29:33 >>> epoch: 193 took 61.889s
training loss:	0.575644
validation loss:	0.679068
validation error:	0.389065
2017-01-19 04:29:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:30:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-194.pkl
2017-01-19 04:30:35 >>> epoch: 194 took 61.960s
training loss:	0.574568
validation loss:	0.678049
validation error:	0.388958
2017-01-19 04:30:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:31:35 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-195.pkl
2017-01-19 04:31:37 >>> epoch: 195 took 61.922s
training loss:	0.574300
validation loss:	0.676034
validation error:	0.388801
2017-01-19 04:31:37 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:32:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-196.pkl
2017-01-19 04:32:39 >>> epoch: 196 took 62.074s
training loss:	0.573218
validation loss:	0.681966
validation error:	0.389585
2017-01-19 04:32:39 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:33:39 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-197.pkl
2017-01-19 04:33:41 >>> epoch: 197 took 62.074s
training loss:	0.572886
validation loss:	0.686993
validation error:	0.389489
2017-01-19 04:33:41 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:34:42 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-198.pkl
2017-01-19 04:34:43 >>> epoch: 198 took 62.501s
training loss:	0.572592
validation loss:	0.687080
validation error:	0.387484
2017-01-19 04:34:43 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:35:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-199.pkl
2017-01-19 04:35:46 >>> epoch: 199 took 62.358s
training loss:	0.572330
validation loss:	0.683289
validation error:	0.389096
2017-01-19 04:35:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:36:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-200.pkl
2017-01-19 04:36:48 >>> epoch: 200 took 62.341s
training loss:	0.570322
validation loss:	0.694644
validation error:	0.393960
2017-01-19 04:36:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:37:49 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-201.pkl
2017-01-19 04:37:50 >>> epoch: 201 took 62.373s
training loss:	0.570129
validation loss:	0.695992
validation error:	0.392743
2017-01-19 04:37:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:38:51 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-202.pkl
2017-01-19 04:38:53 >>> epoch: 202 took 62.304s
training loss:	0.568993
validation loss:	0.690217
validation error:	0.390114
2017-01-19 04:38:53 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:39:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-203.pkl
2017-01-19 04:39:55 >>> epoch: 203 took 62.781s
training loss:	0.568139
validation loss:	0.688732
validation error:	0.389516
2017-01-19 04:39:55 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:40:56 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-204.pkl
2017-01-19 04:40:58 >>> epoch: 204 took 62.551s
training loss:	0.567572
validation loss:	0.690299
validation error:	0.393154
2017-01-19 04:40:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:42:00 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-205.pkl
2017-01-19 04:42:01 >>> epoch: 205 took 63.611s
training loss:	0.566389
validation loss:	0.699881
validation error:	0.391830
2017-01-19 04:42:02 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:43:03 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-206.pkl
2017-01-19 04:43:05 >>> epoch: 206 took 63.376s
training loss:	0.566948
validation loss:	0.690426
validation error:	0.390272
2017-01-19 04:43:05 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:44:06 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-207.pkl
2017-01-19 04:44:08 >>> epoch: 207 took 62.792s
training loss:	0.564476
validation loss:	0.693969
validation error:	0.391915
2017-01-19 04:44:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:45:09 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-208.pkl
2017-01-19 04:45:11 >>> epoch: 208 took 63.402s
training loss:	0.564988
validation loss:	0.692528
validation error:	0.390129
2017-01-19 04:45:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:46:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-209.pkl
2017-01-19 04:46:14 >>> epoch: 209 took 63.025s
training loss:	0.564047
validation loss:	0.698408
validation error:	0.391330
2017-01-19 04:46:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:47:15 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-210.pkl
2017-01-19 04:47:17 >>> epoch: 210 took 62.577s
training loss:	0.561684
validation loss:	0.691223
validation error:	0.389299
2017-01-19 04:47:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:48:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-211.pkl
2017-01-19 04:48:19 >>> epoch: 211 took 62.308s
training loss:	0.562440
validation loss:	0.700270
validation error:	0.393973
2017-01-19 04:48:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:49:19 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-212.pkl
2017-01-19 04:49:21 >>> epoch: 212 took 62.326s
training loss:	0.561785
validation loss:	0.697025
validation error:	0.391346
2017-01-19 04:49:21 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:50:22 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-213.pkl
2017-01-19 04:50:24 >>> epoch: 213 took 62.784s
training loss:	0.560077
validation loss:	0.700514
validation error:	0.393685
2017-01-19 04:50:24 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:51:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-214.pkl
2017-01-19 04:51:27 >>> epoch: 214 took 62.627s
training loss:	0.559914
validation loss:	0.698844
validation error:	0.391929
2017-01-19 04:51:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:52:28 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-215.pkl
2017-01-19 04:52:30 >>> epoch: 215 took 62.856s
training loss:	0.558560
validation loss:	0.704771
validation error:	0.394078
2017-01-19 04:52:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:53:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-216.pkl
2017-01-19 04:53:33 >>> epoch: 216 took 63.361s
training loss:	0.557201
validation loss:	0.710596
validation error:	0.394580
2017-01-19 04:53:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:54:34 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-217.pkl
2017-01-19 04:54:36 >>> epoch: 217 took 62.851s
training loss:	0.557372
validation loss:	0.719939
validation error:	0.396080
2017-01-19 04:54:36 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:55:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-218.pkl
2017-01-19 04:55:38 >>> epoch: 218 took 62.603s
training loss:	0.556519
validation loss:	0.707157
validation error:	0.397067
2017-01-19 04:55:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:56:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-219.pkl
2017-01-19 04:56:41 >>> epoch: 219 took 62.995s
training loss:	0.555180
validation loss:	0.704372
validation error:	0.392105
2017-01-19 04:56:41 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:57:43 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-220.pkl
2017-01-19 04:57:45 >>> epoch: 220 took 63.137s
training loss:	0.554679
validation loss:	0.708448
validation error:	0.395306
2017-01-19 04:57:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:58:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-221.pkl
2017-01-19 04:58:48 >>> epoch: 221 took 63.644s
training loss:	0.553936
validation loss:	0.708896
validation error:	0.396672
2017-01-19 04:58:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 04:59:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-222.pkl
2017-01-19 04:59:52 >>> epoch: 222 took 63.603s
training loss:	0.553549
validation loss:	0.704758
validation error:	0.394946
2017-01-19 04:59:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:00:53 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-223.pkl
2017-01-19 05:00:54 >>> epoch: 223 took 62.567s
training loss:	0.551871
validation loss:	0.710767
validation error:	0.394538
2017-01-19 05:00:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:01:56 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-224.pkl
2017-01-19 05:01:58 >>> epoch: 224 took 63.482s
training loss:	0.551589
validation loss:	0.711961
validation error:	0.394362
2017-01-19 05:01:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:02:59 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-225.pkl
2017-01-19 05:03:00 >>> epoch: 225 took 62.432s
training loss:	0.551092
validation loss:	0.707876
validation error:	0.392882
2017-01-19 05:03:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:04:01 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-226.pkl
2017-01-19 05:04:03 >>> epoch: 226 took 62.388s
training loss:	0.548997
validation loss:	0.710337
validation error:	0.393266
2017-01-19 05:04:03 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:05:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-227.pkl
2017-01-19 05:05:06 >>> epoch: 227 took 63.669s
training loss:	0.548858
validation loss:	0.712871
validation error:	0.394830
2017-01-19 05:05:06 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:06:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-228.pkl
2017-01-19 05:06:10 >>> epoch: 228 took 63.511s
training loss:	0.547590
validation loss:	0.723151
validation error:	0.399424
2017-01-19 05:06:10 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:07:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-229.pkl
2017-01-19 05:07:14 >>> epoch: 229 took 64.059s
training loss:	0.548225
validation loss:	0.717694
validation error:	0.396406
2017-01-19 05:07:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:08:16 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-230.pkl
2017-01-19 05:08:17 >>> epoch: 230 took 63.433s
training loss:	0.546692
validation loss:	0.704641
validation error:	0.393815
2017-01-19 05:08:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:09:19 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-231.pkl
2017-01-19 05:09:20 >>> epoch: 231 took 63.034s
training loss:	0.546042
validation loss:	0.717708
validation error:	0.395234
2017-01-19 05:09:20 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:10:21 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-232.pkl
2017-01-19 05:10:23 >>> epoch: 232 took 62.319s
training loss:	0.546624
validation loss:	0.716232
validation error:	0.395071
2017-01-19 05:10:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:11:23 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-233.pkl
2017-01-19 05:11:25 >>> epoch: 233 took 62.434s
training loss:	0.543949
validation loss:	0.730247
validation error:	0.399172
2017-01-19 05:11:25 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:12:26 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-234.pkl
2017-01-19 05:12:28 >>> epoch: 234 took 62.766s
training loss:	0.542870
validation loss:	0.726523
validation error:	0.396161
2017-01-19 05:12:28 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:13:28 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-235.pkl
2017-01-19 05:13:30 >>> epoch: 235 took 62.034s
training loss:	0.543379
validation loss:	0.717356
validation error:	0.395565
2017-01-19 05:13:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:14:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-236.pkl
2017-01-19 05:14:32 >>> epoch: 236 took 62.316s
training loss:	0.541630
validation loss:	0.721506
validation error:	0.395386
2017-01-19 05:14:32 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:15:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-237.pkl
2017-01-19 05:15:34 >>> epoch: 237 took 62.103s
training loss:	0.540609
validation loss:	0.724339
validation error:	0.393145
2017-01-19 05:15:34 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:16:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-238.pkl
2017-01-19 05:16:37 >>> epoch: 238 took 62.833s
training loss:	0.540406
validation loss:	0.736177
validation error:	0.400833
2017-01-19 05:16:37 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:17:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-239.pkl
2017-01-19 05:17:39 >>> epoch: 239 took 61.860s
training loss:	0.539484
validation loss:	0.736823
validation error:	0.399078
2017-01-19 05:17:39 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:18:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-240.pkl
2017-01-19 05:18:42 >>> epoch: 240 took 63.006s
training loss:	0.538186
validation loss:	0.738272
validation error:	0.398721
2017-01-19 05:18:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:19:42 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-241.pkl
2017-01-19 05:19:44 >>> epoch: 241 took 62.071s
training loss:	0.537671
validation loss:	0.737122
validation error:	0.398636
2017-01-19 05:19:44 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:20:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-242.pkl
2017-01-19 05:20:47 >>> epoch: 242 took 63.199s
training loss:	0.536244
validation loss:	0.735424
validation error:	0.398158
2017-01-19 05:20:47 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:21:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-243.pkl
2017-01-19 05:21:50 >>> epoch: 243 took 62.575s
training loss:	0.535509
validation loss:	0.739127
validation error:	0.396161
2017-01-19 05:21:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:22:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-244.pkl
2017-01-19 05:22:52 >>> epoch: 244 took 61.895s
training loss:	0.535962
validation loss:	0.740677
validation error:	0.397824
2017-01-19 05:22:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:23:53 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-245.pkl
2017-01-19 05:23:55 >>> epoch: 245 took 62.909s
training loss:	0.533446
validation loss:	0.737310
validation error:	0.400658
2017-01-19 05:23:55 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:24:56 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-246.pkl
2017-01-19 05:24:58 >>> epoch: 246 took 63.005s
training loss:	0.533896
validation loss:	0.736705
validation error:	0.396388
2017-01-19 05:24:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:25:59 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-247.pkl
2017-01-19 05:26:00 >>> epoch: 247 took 62.758s
training loss:	0.532922
validation loss:	0.756811
validation error:	0.399129
2017-01-19 05:26:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:27:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-248.pkl
2017-01-19 05:27:03 >>> epoch: 248 took 62.762s
training loss:	0.532103
validation loss:	0.748734
validation error:	0.399333
2017-01-19 05:27:03 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:28:04 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-249.pkl
2017-01-19 05:28:05 >>> epoch: 249 took 62.239s
training loss:	0.529785
validation loss:	0.780096
validation error:	0.401609
2017-01-19 05:28:05 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:29:07 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-250.pkl
2017-01-19 05:29:08 >>> epoch: 250 took 62.955s
training loss:	0.530401
validation loss:	0.734493
validation error:	0.397721
2017-01-19 05:29:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:30:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-251.pkl
2017-01-19 05:30:12 >>> epoch: 251 took 63.289s
training loss:	0.530189
validation loss:	0.766372
validation error:	0.404955
2017-01-19 05:30:12 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:31:13 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-252.pkl
2017-01-19 05:31:15 >>> epoch: 252 took 63.385s
training loss:	0.528832
validation loss:	0.742504
validation error:	0.397277
2017-01-19 05:31:15 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:32:16 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-253.pkl
2017-01-19 05:32:17 >>> epoch: 253 took 62.311s
training loss:	0.527377
validation loss:	0.757644
validation error:	0.398324
2017-01-19 05:32:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:33:18 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-254.pkl
2017-01-19 05:33:20 >>> epoch: 254 took 62.279s
training loss:	0.527287
validation loss:	0.733975
validation error:	0.396299
2017-01-19 05:33:20 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:34:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-255.pkl
2017-01-19 05:34:22 >>> epoch: 255 took 62.359s
training loss:	0.526630
validation loss:	0.753001
validation error:	0.400850
2017-01-19 05:34:22 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:35:23 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-256.pkl
2017-01-19 05:35:25 >>> epoch: 256 took 62.956s
training loss:	0.525834
validation loss:	0.750905
validation error:	0.399018
2017-01-19 05:35:25 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:36:26 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-257.pkl
2017-01-19 05:36:28 >>> epoch: 257 took 63.230s
training loss:	0.523925
validation loss:	0.763569
validation error:	0.401797
2017-01-19 05:36:28 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:37:29 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-258.pkl
2017-01-19 05:37:30 >>> epoch: 258 took 62.085s
training loss:	0.524189
validation loss:	0.754436
validation error:	0.398272
2017-01-19 05:37:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:38:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-259.pkl
2017-01-19 05:38:33 >>> epoch: 259 took 62.725s
training loss:	0.523493
validation loss:	0.751598
validation error:	0.400754
2017-01-19 05:38:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:39:34 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-260.pkl
2017-01-19 05:39:36 >>> epoch: 260 took 62.790s
training loss:	0.522014
validation loss:	0.756421
validation error:	0.401210
2017-01-19 05:39:36 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:40:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-261.pkl
2017-01-19 05:40:38 >>> epoch: 261 took 62.221s
training loss:	0.522854
validation loss:	0.760476
validation error:	0.402993
2017-01-19 05:40:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:41:39 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-262.pkl
2017-01-19 05:41:41 >>> epoch: 262 took 63.171s
training loss:	0.520584
validation loss:	0.782398
validation error:	0.402935
2017-01-19 05:41:41 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:42:42 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-263.pkl
2017-01-19 05:42:44 >>> epoch: 263 took 62.603s
training loss:	0.520312
validation loss:	0.779272
validation error:	0.402051
2017-01-19 05:42:44 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:43:45 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-264.pkl
2017-01-19 05:43:46 >>> epoch: 264 took 62.652s
training loss:	0.518842
validation loss:	0.764713
validation error:	0.402422
2017-01-19 05:43:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:44:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-265.pkl
2017-01-19 05:44:50 >>> epoch: 265 took 63.061s
training loss:	0.518329
validation loss:	0.784444
validation error:	0.407395
2017-01-19 05:44:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:45:51 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-266.pkl
2017-01-19 05:45:53 >>> epoch: 266 took 63.269s
training loss:	0.518016
validation loss:	0.789478
validation error:	0.402406
2017-01-19 05:45:53 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:46:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-267.pkl
2017-01-19 05:46:56 >>> epoch: 267 took 63.089s
training loss:	0.517658
validation loss:	0.787236
validation error:	0.403475
2017-01-19 05:46:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:47:57 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-268.pkl
2017-01-19 05:47:59 >>> epoch: 268 took 63.394s
training loss:	0.516147
validation loss:	0.768117
validation error:	0.399933
2017-01-19 05:47:59 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:49:00 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-269.pkl
2017-01-19 05:49:02 >>> epoch: 269 took 62.645s
training loss:	0.515615
validation loss:	0.778546
validation error:	0.403701
2017-01-19 05:49:02 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:50:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-270.pkl
2017-01-19 05:50:04 >>> epoch: 270 took 61.928s
training loss:	0.514163
validation loss:	0.764541
validation error:	0.398625
2017-01-19 05:50:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:51:04 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-271.pkl
2017-01-19 05:51:06 >>> epoch: 271 took 61.819s
training loss:	0.513846
validation loss:	0.776887
validation error:	0.400545
2017-01-19 05:51:06 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:52:06 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-272.pkl
2017-01-19 05:52:08 >>> epoch: 272 took 61.943s
training loss:	0.513760
validation loss:	0.786826
validation error:	0.402395
2017-01-19 05:52:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:53:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-273.pkl
2017-01-19 05:53:10 >>> epoch: 273 took 61.930s
training loss:	0.512938
validation loss:	0.774299
validation error:	0.401571
2017-01-19 05:53:10 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:54:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-274.pkl
2017-01-19 05:54:12 >>> epoch: 274 took 62.083s
training loss:	0.512159
validation loss:	0.790392
validation error:	0.403998
2017-01-19 05:54:12 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:55:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-275.pkl
2017-01-19 05:55:13 >>> epoch: 275 took 61.673s
training loss:	0.509960
validation loss:	0.780568
validation error:	0.405051
2017-01-19 05:55:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:56:13 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-276.pkl
2017-01-19 05:56:15 >>> epoch: 276 took 61.623s
training loss:	0.510119
validation loss:	0.778603
validation error:	0.401185
2017-01-19 05:56:15 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:57:15 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-277.pkl
2017-01-19 05:57:17 >>> epoch: 277 took 62.417s
training loss:	0.510251
validation loss:	0.774515
validation error:	0.401489
2017-01-19 05:57:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:58:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-278.pkl
2017-01-19 05:58:19 >>> epoch: 278 took 62.050s
training loss:	0.509752
validation loss:	0.784548
validation error:	0.404696
2017-01-19 05:58:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 05:59:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-279.pkl
2017-01-19 05:59:22 >>> epoch: 279 took 62.154s
training loss:	0.508691
validation loss:	0.788983
validation error:	0.400292
2017-01-19 05:59:22 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:00:22 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-280.pkl
2017-01-19 06:00:23 >>> epoch: 280 took 61.762s
training loss:	0.509089
validation loss:	0.788215
validation error:	0.399862
2017-01-19 06:00:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:01:24 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-281.pkl
2017-01-19 06:01:25 >>> epoch: 281 took 62.132s
training loss:	0.506851
validation loss:	0.781691
validation error:	0.398667
2017-01-19 06:01:25 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:02:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-282.pkl
2017-01-19 06:02:27 >>> epoch: 282 took 61.590s
training loss:	0.505756
validation loss:	0.809543
validation error:	0.406194
2017-01-19 06:02:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:03:27 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-283.pkl
2017-01-19 06:03:29 >>> epoch: 283 took 61.734s
training loss:	0.505801
validation loss:	0.800349
validation error:	0.402752
2017-01-19 06:03:29 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:04:30 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-284.pkl
2017-01-19 06:04:31 >>> epoch: 284 took 62.755s
training loss:	0.503845
validation loss:	0.788052
validation error:	0.401212
2017-01-19 06:04:32 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:05:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-285.pkl
2017-01-19 06:05:33 >>> epoch: 285 took 61.419s
training loss:	0.503571
validation loss:	0.804588
validation error:	0.401583
2017-01-19 06:05:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:06:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-286.pkl
2017-01-19 06:06:35 >>> epoch: 286 took 61.845s
training loss:	0.504195
validation loss:	0.780059
validation error:	0.402688
2017-01-19 06:06:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:07:35 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-287.pkl
2017-01-19 06:07:37 >>> epoch: 287 took 61.768s
training loss:	0.501689
validation loss:	0.818005
validation error:	0.403770
2017-01-19 06:07:37 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:08:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-288.pkl
2017-01-19 06:08:38 >>> epoch: 288 took 61.727s
training loss:	0.502712
validation loss:	0.797206
validation error:	0.400339
2017-01-19 06:08:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:09:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-289.pkl
2017-01-19 06:09:41 >>> epoch: 289 took 63.027s
training loss:	0.501662
validation loss:	0.793041
validation error:	0.405203
2017-01-19 06:09:41 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:10:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-290.pkl
2017-01-19 06:10:43 >>> epoch: 290 took 61.596s
training loss:	0.500556
validation loss:	0.810649
validation error:	0.403152
2017-01-19 06:10:43 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:11:43 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-291.pkl
2017-01-19 06:11:44 >>> epoch: 291 took 61.452s
training loss:	0.498917
validation loss:	0.804039
validation error:	0.403386
2017-01-19 06:11:44 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:12:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-292.pkl
2017-01-19 06:12:46 >>> epoch: 292 took 61.686s
training loss:	0.500207
validation loss:	0.835740
validation error:	0.402665
2017-01-19 06:12:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:13:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-293.pkl
2017-01-19 06:13:48 >>> epoch: 293 took 61.721s
training loss:	0.499119
validation loss:	0.794757
validation error:	0.402750
2017-01-19 06:13:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:14:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-294.pkl
2017-01-19 06:14:50 >>> epoch: 294 took 61.862s
training loss:	0.498462
validation loss:	0.793175
validation error:	0.401654
2017-01-19 06:14:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:15:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-295.pkl
2017-01-19 06:15:52 >>> epoch: 295 took 62.202s
training loss:	0.495498
validation loss:	0.800363
validation error:	0.402703
2017-01-19 06:15:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:16:52 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-296.pkl
2017-01-19 06:16:54 >>> epoch: 296 took 61.765s
training loss:	0.495234
validation loss:	0.810309
validation error:	0.403547
2017-01-19 06:16:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:17:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-297.pkl
2017-01-19 06:17:56 >>> epoch: 297 took 62.338s
training loss:	0.496184
validation loss:	0.820784
validation error:	0.405717
2017-01-19 06:17:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:18:56 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-298.pkl
2017-01-19 06:18:58 >>> epoch: 298 took 62.057s
training loss:	0.494234
validation loss:	0.814603
validation error:	0.404625
2017-01-19 06:18:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:19:58 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-299.pkl
2017-01-19 06:20:00 >>> epoch: 299 took 62.030s
training loss:	0.493742
validation loss:	0.798681
validation error:	0.403853
2017-01-19 06:20:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:21:00 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-300.pkl
2017-01-19 06:21:02 >>> epoch: 300 took 61.696s
training loss:	0.493119
validation loss:	0.815095
validation error:	0.404567
2017-01-19 06:21:02 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:22:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-301.pkl
2017-01-19 06:22:04 >>> epoch: 301 took 62.373s
training loss:	0.493205
validation loss:	0.806216
validation error:	0.400908
2017-01-19 06:22:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:23:04 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-302.pkl
2017-01-19 06:23:06 >>> epoch: 302 took 61.664s
training loss:	0.492036
validation loss:	0.809350
validation error:	0.404692
2017-01-19 06:23:06 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:24:06 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-303.pkl
2017-01-19 06:24:08 >>> epoch: 303 took 61.975s
training loss:	0.492972
validation loss:	0.827371
validation error:	0.404366
2017-01-19 06:24:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:25:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-304.pkl
2017-01-19 06:25:09 >>> epoch: 304 took 61.766s
training loss:	0.490526
validation loss:	0.829923
validation error:	0.403176
2017-01-19 06:25:09 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:26:09 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-305.pkl
2017-01-19 06:26:11 >>> epoch: 305 took 61.635s
training loss:	0.490998
validation loss:	0.822928
validation error:	0.403538
2017-01-19 06:26:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:27:11 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-306.pkl
2017-01-19 06:27:13 >>> epoch: 306 took 62.003s
training loss:	0.488477
validation loss:	0.821012
validation error:	0.404431
2017-01-19 06:27:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:28:14 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-307.pkl
2017-01-19 06:28:15 >>> epoch: 307 took 62.358s
training loss:	0.489614
validation loss:	0.842680
validation error:	0.405569
2017-01-19 06:28:15 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:29:16 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-308.pkl
2017-01-19 06:29:17 >>> epoch: 308 took 61.944s
training loss:	0.489477
validation loss:	0.822647
validation error:	0.404112
2017-01-19 06:29:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:30:18 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-309.pkl
2017-01-19 06:30:20 >>> epoch: 309 took 62.407s
training loss:	0.485777
validation loss:	0.824156
validation error:	0.401783
2017-01-19 06:30:20 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:31:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-310.pkl
2017-01-19 06:31:22 >>> epoch: 310 took 61.955s
training loss:	0.486469
validation loss:	0.818967
validation error:	0.402600
2017-01-19 06:31:22 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:32:22 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-311.pkl
2017-01-19 06:32:24 >>> epoch: 311 took 61.843s
training loss:	0.485281
validation loss:	0.826201
validation error:	0.404232
2017-01-19 06:32:24 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:33:23 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-312.pkl
2017-01-19 06:33:25 >>> epoch: 312 took 61.632s
training loss:	0.484696
validation loss:	0.846120
validation error:	0.407228
2017-01-19 06:33:25 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:34:26 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-313.pkl
2017-01-19 06:34:27 >>> epoch: 313 took 62.162s
training loss:	0.484474
validation loss:	0.836137
validation error:	0.406000
2017-01-19 06:34:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:35:27 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-314.pkl
2017-01-19 06:35:29 >>> epoch: 314 took 61.675s
training loss:	0.483348
validation loss:	0.824238
validation error:	0.401750
2017-01-19 06:35:29 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:36:29 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-315.pkl
2017-01-19 06:36:30 >>> epoch: 315 took 61.228s
training loss:	0.483310
validation loss:	0.816876
validation error:	0.400938
2017-01-19 06:36:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:37:30 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-316.pkl
2017-01-19 06:37:32 >>> epoch: 316 took 61.309s
training loss:	0.481769
validation loss:	0.846186
validation error:	0.405078
2017-01-19 06:37:32 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:38:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-317.pkl
2017-01-19 06:38:33 >>> epoch: 317 took 61.260s
training loss:	0.482326
validation loss:	0.852984
validation error:	0.406661
2017-01-19 06:38:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:39:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-318.pkl
2017-01-19 06:39:34 >>> epoch: 318 took 61.323s
training loss:	0.479792
validation loss:	0.837518
validation error:	0.404114
2017-01-19 06:39:34 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:40:34 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-319.pkl
2017-01-19 06:40:36 >>> epoch: 319 took 61.519s
training loss:	0.479603
validation loss:	0.855390
validation error:	0.408179
2017-01-19 06:40:36 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:41:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-320.pkl
2017-01-19 06:41:37 >>> epoch: 320 took 61.460s
training loss:	0.481107
validation loss:	0.836944
validation error:	0.404433
2017-01-19 06:41:37 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:42:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-321.pkl
2017-01-19 06:42:39 >>> epoch: 321 took 61.514s
training loss:	0.479860
validation loss:	0.852189
validation error:	0.409049
2017-01-19 06:42:39 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:43:38 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-322.pkl
2017-01-19 06:43:40 >>> epoch: 322 took 61.391s
training loss:	0.478238
validation loss:	0.873604
validation error:	0.408210
2017-01-19 06:43:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:44:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-323.pkl
2017-01-19 06:44:41 >>> epoch: 323 took 61.311s
training loss:	0.480202
validation loss:	0.836927
validation error:	0.404967
2017-01-19 06:44:41 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:45:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-324.pkl
2017-01-19 06:45:43 >>> epoch: 324 took 61.352s
training loss:	0.478302
validation loss:	0.833749
validation error:	0.401688
2017-01-19 06:45:43 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:46:43 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-325.pkl
2017-01-19 06:46:45 >>> epoch: 325 took 61.760s
training loss:	0.477417
validation loss:	0.849870
validation error:	0.407480
2017-01-19 06:46:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:47:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-326.pkl
2017-01-19 06:47:46 >>> epoch: 326 took 61.453s
training loss:	0.476979
validation loss:	0.855285
validation error:	0.402621
2017-01-19 06:47:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:48:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-327.pkl
2017-01-19 06:48:47 >>> epoch: 327 took 61.519s
training loss:	0.476193
validation loss:	0.836638
validation error:	0.402915
2017-01-19 06:48:47 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:49:47 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-328.pkl
2017-01-19 06:49:49 >>> epoch: 328 took 61.413s
training loss:	0.473638
validation loss:	0.851495
validation error:	0.405714
2017-01-19 06:49:49 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:50:49 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-329.pkl
2017-01-19 06:50:50 >>> epoch: 329 took 61.516s
training loss:	0.474277
validation loss:	0.849442
validation error:	0.408109
2017-01-19 06:50:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:51:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-330.pkl
2017-01-19 06:51:52 >>> epoch: 330 took 61.268s
training loss:	0.473923
validation loss:	0.846729
validation error:	0.404690
2017-01-19 06:51:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:52:51 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-331.pkl
2017-01-19 06:52:53 >>> epoch: 331 took 61.372s
training loss:	0.472781
validation loss:	0.839304
validation error:	0.405623
2017-01-19 06:52:53 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:53:53 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-332.pkl
2017-01-19 06:53:54 >>> epoch: 332 took 61.347s
training loss:	0.472238
validation loss:	0.857601
validation error:	0.405482
2017-01-19 06:53:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:54:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-333.pkl
2017-01-19 06:54:56 >>> epoch: 333 took 61.540s
training loss:	0.472981
validation loss:	0.865903
validation error:	0.408232
2017-01-19 06:54:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:55:56 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-334.pkl
2017-01-19 06:55:58 >>> epoch: 334 took 61.717s
training loss:	0.471097
validation loss:	0.874100
validation error:	0.401717
2017-01-19 06:55:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:56:59 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-335.pkl
2017-01-19 06:57:01 >>> epoch: 335 took 63.585s
training loss:	0.470348
validation loss:	0.866692
validation error:	0.405290
2017-01-19 06:57:01 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:58:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-336.pkl
2017-01-19 06:58:04 >>> epoch: 336 took 62.966s
training loss:	0.470079
validation loss:	0.876594
validation error:	0.406328
2017-01-19 06:58:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 06:59:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-337.pkl
2017-01-19 06:59:07 >>> epoch: 337 took 62.870s
training loss:	0.469678
validation loss:	0.855718
validation error:	0.408074
2017-01-19 06:59:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:00:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-338.pkl
2017-01-19 07:00:10 >>> epoch: 338 took 62.768s
training loss:	0.469850
validation loss:	0.857173
validation error:	0.404690
2017-01-19 07:00:10 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:01:11 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-339.pkl
2017-01-19 07:01:13 >>> epoch: 339 took 62.892s
training loss:	0.468239
validation loss:	0.885906
validation error:	0.410112
2017-01-19 07:01:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:02:14 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-340.pkl
2017-01-19 07:02:16 >>> epoch: 340 took 63.103s
training loss:	0.467000
validation loss:	0.908320
validation error:	0.412362
2017-01-19 07:02:16 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:03:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-341.pkl
2017-01-19 07:03:19 >>> epoch: 341 took 62.788s
training loss:	0.466521
validation loss:	0.872272
validation error:	0.408915
2017-01-19 07:03:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:04:19 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-342.pkl
2017-01-19 07:04:21 >>> epoch: 342 took 62.215s
training loss:	0.466325
validation loss:	0.884915
validation error:	0.407281
2017-01-19 07:04:21 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:05:22 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-343.pkl
2017-01-19 07:05:24 >>> epoch: 343 took 63.153s
training loss:	0.466885
validation loss:	0.881452
validation error:	0.407250
2017-01-19 07:05:24 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:06:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-344.pkl
2017-01-19 07:06:27 >>> epoch: 344 took 63.150s
training loss:	0.466211
validation loss:	0.908978
validation error:	0.407371
2017-01-19 07:06:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:07:28 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-345.pkl
2017-01-19 07:07:29 >>> epoch: 345 took 62.343s
training loss:	0.465621
validation loss:	0.890122
validation error:	0.409337
2017-01-19 07:07:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:08:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-346.pkl
2017-01-19 07:08:32 >>> epoch: 346 took 62.711s
training loss:	0.464183
validation loss:	0.888295
validation error:	0.407237
2017-01-19 07:08:32 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:09:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-347.pkl
2017-01-19 07:09:35 >>> epoch: 347 took 62.679s
training loss:	0.463756
validation loss:	0.884085
validation error:	0.407161
2017-01-19 07:09:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:10:35 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-348.pkl
2017-01-19 07:10:37 >>> epoch: 348 took 62.186s
training loss:	0.462269
validation loss:	0.911289
validation error:	0.410221
2017-01-19 07:10:37 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:11:38 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-349.pkl
2017-01-19 07:11:40 >>> epoch: 349 took 62.539s
training loss:	0.463675
validation loss:	0.859496
validation error:	0.407096
2017-01-19 07:11:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:12:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-350.pkl
2017-01-19 07:12:42 >>> epoch: 350 took 62.262s
training loss:	0.460397
validation loss:	0.901387
validation error:	0.407112
2017-01-19 07:12:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:13:43 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-351.pkl
2017-01-19 07:13:45 >>> epoch: 351 took 63.000s
training loss:	0.461841
validation loss:	0.893057
validation error:	0.408210
2017-01-19 07:13:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:14:45 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-352.pkl
2017-01-19 07:14:47 >>> epoch: 352 took 61.818s
training loss:	0.461703
validation loss:	0.889215
validation error:	0.406094
2017-01-19 07:14:47 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:15:47 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-353.pkl
2017-01-19 07:15:49 >>> epoch: 353 took 62.406s
training loss:	0.460847
validation loss:	0.890522
validation error:	0.406333
2017-01-19 07:15:49 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:16:49 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-354.pkl
2017-01-19 07:16:51 >>> epoch: 354 took 61.891s
training loss:	0.458453
validation loss:	0.910275
validation error:	0.410275
2017-01-19 07:16:51 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:17:51 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-355.pkl
2017-01-19 07:17:53 >>> epoch: 355 took 61.949s
training loss:	0.459821
validation loss:	0.886104
validation error:	0.410145
2017-01-19 07:17:53 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:18:53 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-356.pkl
2017-01-19 07:18:55 >>> epoch: 356 took 62.022s
training loss:	0.457647
validation loss:	0.917186
validation error:	0.410185
2017-01-19 07:18:55 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:19:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-357.pkl
2017-01-19 07:19:57 >>> epoch: 357 took 62.151s
training loss:	0.456891
validation loss:	0.899399
validation error:	0.412422
2017-01-19 07:19:57 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:20:58 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-358.pkl
2017-01-19 07:21:00 >>> epoch: 358 took 62.814s
training loss:	0.458247
validation loss:	0.891378
validation error:	0.406596
2017-01-19 07:21:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:22:01 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-359.pkl
2017-01-19 07:22:03 >>> epoch: 359 took 62.656s
training loss:	0.457507
validation loss:	0.894254
validation error:	0.408105
2017-01-19 07:22:03 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:23:03 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-360.pkl
2017-01-19 07:23:04 >>> epoch: 360 took 61.872s
training loss:	0.455444
validation loss:	0.911786
validation error:	0.410007
2017-01-19 07:23:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:24:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-361.pkl
2017-01-19 07:24:07 >>> epoch: 361 took 62.095s
training loss:	0.454107
validation loss:	0.913616
validation error:	0.410799
2017-01-19 07:24:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:25:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-362.pkl
2017-01-19 07:25:09 >>> epoch: 362 took 62.763s
training loss:	0.455552
validation loss:	0.907754
validation error:	0.409299
2017-01-19 07:25:09 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:26:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-363.pkl
2017-01-19 07:26:12 >>> epoch: 363 took 62.731s
training loss:	0.455048
validation loss:	0.926829
validation error:	0.409174
2017-01-19 07:26:12 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:27:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-364.pkl
2017-01-19 07:27:14 >>> epoch: 364 took 62.165s
training loss:	0.453179
validation loss:	0.922752
validation error:	0.410022
2017-01-19 07:27:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:28:15 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-365.pkl
2017-01-19 07:28:17 >>> epoch: 365 took 62.458s
training loss:	0.453902
validation loss:	0.869427
validation error:	0.407509
2017-01-19 07:28:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:29:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-366.pkl
2017-01-19 07:29:19 >>> epoch: 366 took 62.613s
training loss:	0.453456
validation loss:	0.902961
validation error:	0.411641
2017-01-19 07:29:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:30:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-367.pkl
2017-01-19 07:30:22 >>> epoch: 367 took 62.715s
training loss:	0.451989
validation loss:	0.906872
validation error:	0.409556
2017-01-19 07:30:22 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:31:23 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-368.pkl
2017-01-19 07:31:25 >>> epoch: 368 took 63.168s
training loss:	0.451507
validation loss:	0.912353
validation error:	0.409324
2017-01-19 07:31:25 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:32:26 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-369.pkl
2017-01-19 07:32:28 >>> epoch: 369 took 63.021s
training loss:	0.452004
validation loss:	0.924621
validation error:	0.410065
2017-01-19 07:32:28 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:33:30 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-370.pkl
2017-01-19 07:33:32 >>> epoch: 370 took 63.580s
training loss:	0.452049
validation loss:	0.928882
validation error:	0.414188
2017-01-19 07:33:32 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:34:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-371.pkl
2017-01-19 07:34:34 >>> epoch: 371 took 62.562s
training loss:	0.452371
validation loss:	0.922403
validation error:	0.407167
2017-01-19 07:34:34 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:35:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-372.pkl
2017-01-19 07:35:37 >>> epoch: 372 took 63.190s
training loss:	0.450226
validation loss:	0.890814
validation error:	0.407257
2017-01-19 07:35:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:36:39 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-373.pkl
2017-01-19 07:36:40 >>> epoch: 373 took 62.887s
training loss:	0.449692
validation loss:	0.907813
validation error:	0.410821
2017-01-19 07:36:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:37:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-374.pkl
2017-01-19 07:37:43 >>> epoch: 374 took 62.452s
training loss:	0.447282
validation loss:	0.948009
validation error:	0.413446
2017-01-19 07:37:43 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:38:43 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-375.pkl
2017-01-19 07:38:45 >>> epoch: 375 took 62.216s
training loss:	0.450012
validation loss:	0.919680
validation error:	0.410696
2017-01-19 07:38:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:39:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-376.pkl
2017-01-19 07:39:47 >>> epoch: 376 took 62.245s
training loss:	0.447784
validation loss:	0.915252
validation error:	0.407547
2017-01-19 07:39:47 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:40:47 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-377.pkl
2017-01-19 07:40:49 >>> epoch: 377 took 61.748s
training loss:	0.447341
validation loss:	0.915003
validation error:	0.408214
2017-01-19 07:40:49 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:41:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-378.pkl
2017-01-19 07:41:50 >>> epoch: 378 took 60.901s
training loss:	0.447662
validation loss:	0.932588
validation error:	0.411775
2017-01-19 07:41:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:42:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-379.pkl
2017-01-19 07:42:51 >>> epoch: 379 took 61.505s
training loss:	0.447140
validation loss:	0.932390
validation error:	0.409174
2017-01-19 07:42:51 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:43:51 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-380.pkl
2017-01-19 07:43:53 >>> epoch: 380 took 61.732s
training loss:	0.447144
validation loss:	0.946919
validation error:	0.410417
2017-01-19 07:43:53 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:44:53 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-381.pkl
2017-01-19 07:44:54 >>> epoch: 381 took 61.182s
training loss:	0.445658
validation loss:	0.940640
validation error:	0.412772
2017-01-19 07:44:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:45:56 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-382.pkl
2017-01-19 07:45:58 >>> epoch: 382 took 63.147s
training loss:	0.444385
validation loss:	0.961957
validation error:	0.410290
2017-01-19 07:45:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:46:57 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-383.pkl
2017-01-19 07:46:59 >>> epoch: 383 took 61.675s
training loss:	0.445112
validation loss:	0.936747
validation error:	0.412210
2017-01-19 07:46:59 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:47:59 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-384.pkl
2017-01-19 07:48:01 >>> epoch: 384 took 61.660s
training loss:	0.444316
validation loss:	0.939812
validation error:	0.409317
2017-01-19 07:48:01 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:49:01 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-385.pkl
2017-01-19 07:49:02 >>> epoch: 385 took 61.572s
training loss:	0.443986
validation loss:	0.938700
validation error:	0.407315
2017-01-19 07:49:02 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:50:03 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-386.pkl
2017-01-19 07:50:04 >>> epoch: 386 took 61.852s
training loss:	0.442892
validation loss:	0.919111
validation error:	0.410871
2017-01-19 07:50:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:51:04 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-387.pkl
2017-01-19 07:51:06 >>> epoch: 387 took 61.754s
training loss:	0.442217
validation loss:	0.943855
validation error:	0.409397
2017-01-19 07:51:06 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:52:06 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-388.pkl
2017-01-19 07:52:08 >>> epoch: 388 took 61.751s
training loss:	0.443098
validation loss:	0.937167
validation error:	0.411087
2017-01-19 07:52:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:53:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-389.pkl
2017-01-19 07:53:10 >>> epoch: 389 took 61.867s
training loss:	0.438995
validation loss:	0.948643
validation error:	0.406433
2017-01-19 07:53:10 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:54:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-390.pkl
2017-01-19 07:54:11 >>> epoch: 390 took 61.790s
training loss:	0.440827
validation loss:	0.932294
validation error:	0.413478
2017-01-19 07:54:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:55:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-391.pkl
2017-01-19 07:55:13 >>> epoch: 391 took 61.807s
training loss:	0.441801
validation loss:	0.964036
validation error:	0.409768
2017-01-19 07:55:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:56:13 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-392.pkl
2017-01-19 07:56:15 >>> epoch: 392 took 61.962s
training loss:	0.439452
validation loss:	0.954838
validation error:	0.411306
2017-01-19 07:56:15 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:57:15 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-393.pkl
2017-01-19 07:57:17 >>> epoch: 393 took 61.850s
training loss:	0.440471
validation loss:	0.960319
validation error:	0.412525
2017-01-19 07:57:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:58:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-394.pkl
2017-01-19 07:58:19 >>> epoch: 394 took 61.691s
training loss:	0.438858
validation loss:	0.986529
validation error:	0.415652
2017-01-19 07:58:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 07:59:19 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-395.pkl
2017-01-19 07:59:21 >>> epoch: 395 took 62.178s
training loss:	0.441503
validation loss:	0.952883
validation error:	0.409835
2017-01-19 07:59:21 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:00:21 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-396.pkl
2017-01-19 08:00:23 >>> epoch: 396 took 61.882s
training loss:	0.436509
validation loss:	0.936896
validation error:	0.410049
2017-01-19 08:00:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:01:23 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-397.pkl
2017-01-19 08:01:25 >>> epoch: 397 took 61.742s
training loss:	0.438550
validation loss:	0.974501
validation error:	0.413344
2017-01-19 08:01:25 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:02:26 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-398.pkl
2017-01-19 08:02:28 >>> epoch: 398 took 63.419s
training loss:	0.437886
validation loss:	0.974906
validation error:	0.411777
2017-01-19 08:02:28 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:03:28 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-399.pkl
2017-01-19 08:03:30 >>> epoch: 399 took 61.976s
training loss:	0.437667
validation loss:	0.969415
validation error:	0.413763
2017-01-19 08:03:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:04:30 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-400.pkl
2017-01-19 08:04:32 >>> epoch: 400 took 62.079s
training loss:	0.437347
validation loss:	0.971125
validation error:	0.410342
2017-01-19 08:04:32 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:05:32 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-401.pkl
2017-01-19 08:05:34 >>> epoch: 401 took 61.887s
training loss:	0.437487
validation loss:	0.941912
validation error:	0.413998
2017-01-19 08:05:34 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:06:34 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-402.pkl
2017-01-19 08:06:36 >>> epoch: 402 took 62.243s
training loss:	0.435368
validation loss:	0.980909
validation error:	0.414208
2017-01-19 08:06:36 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:07:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-403.pkl
2017-01-19 08:07:38 >>> epoch: 403 took 61.949s
training loss:	0.436782
validation loss:	0.953368
validation error:	0.409721
2017-01-19 08:07:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:08:38 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-404.pkl
2017-01-19 08:08:40 >>> epoch: 404 took 62.103s
training loss:	0.432919
validation loss:	0.969595
validation error:	0.409875
2017-01-19 08:08:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:09:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-405.pkl
2017-01-19 08:09:43 >>> epoch: 405 took 62.552s
training loss:	0.434653
validation loss:	0.948696
validation error:	0.414069
2017-01-19 08:09:43 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:10:43 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-406.pkl
2017-01-19 08:10:45 >>> epoch: 406 took 61.853s
training loss:	0.433212
validation loss:	0.954927
validation error:	0.411998
2017-01-19 08:10:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:11:45 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-407.pkl
2017-01-19 08:11:47 >>> epoch: 407 took 61.940s
training loss:	0.434226
validation loss:	0.977581
validation error:	0.413355
2017-01-19 08:11:47 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:12:47 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-408.pkl
2017-01-19 08:12:48 >>> epoch: 408 took 61.827s
training loss:	0.432665
validation loss:	0.978483
validation error:	0.411511
2017-01-19 08:12:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:13:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-409.pkl
2017-01-19 08:13:50 >>> epoch: 409 took 61.633s
training loss:	0.433016
validation loss:	0.982712
validation error:	0.416071
2017-01-19 08:13:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:14:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-410.pkl
2017-01-19 08:14:52 >>> epoch: 410 took 61.836s
training loss:	0.430355
validation loss:	0.981976
validation error:	0.411433
2017-01-19 08:14:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:15:52 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-411.pkl
2017-01-19 08:15:54 >>> epoch: 411 took 62.303s
training loss:	0.431779
validation loss:	0.975447
validation error:	0.412114
2017-01-19 08:15:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:16:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-412.pkl
2017-01-19 08:16:56 >>> epoch: 412 took 62.059s
training loss:	0.431820
validation loss:	0.979584
validation error:	0.411154
2017-01-19 08:16:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:17:56 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-413.pkl
2017-01-19 08:17:58 >>> epoch: 413 took 61.704s
training loss:	0.430709
validation loss:	0.987210
validation error:	0.414837
2017-01-19 08:17:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:18:58 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-414.pkl
2017-01-19 08:19:00 >>> epoch: 414 took 61.665s
training loss:	0.430558
validation loss:	0.965133
validation error:	0.413281
2017-01-19 08:19:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:20:00 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-415.pkl
2017-01-19 08:20:02 >>> epoch: 415 took 61.942s
training loss:	0.428981
validation loss:	1.000109
validation error:	0.415574
2017-01-19 08:20:02 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:21:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-416.pkl
2017-01-19 08:21:03 >>> epoch: 416 took 61.796s
training loss:	0.429408
validation loss:	0.944647
validation error:	0.413257
2017-01-19 08:21:03 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:22:03 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-417.pkl
2017-01-19 08:22:05 >>> epoch: 417 took 61.943s
training loss:	0.428341
validation loss:	0.987332
validation error:	0.410750
2017-01-19 08:22:05 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:23:06 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-418.pkl
2017-01-19 08:23:07 >>> epoch: 418 took 61.938s
training loss:	0.427687
validation loss:	0.977573
validation error:	0.409708
2017-01-19 08:23:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:24:07 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-419.pkl
2017-01-19 08:24:09 >>> epoch: 419 took 61.854s
training loss:	0.426094
validation loss:	0.967805
validation error:	0.414134
2017-01-19 08:24:09 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:25:09 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-420.pkl
2017-01-19 08:25:11 >>> epoch: 420 took 61.595s
training loss:	0.428516
validation loss:	1.002686
validation error:	0.407241
2017-01-19 08:25:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:26:11 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-421.pkl
2017-01-19 08:26:12 >>> epoch: 421 took 61.601s
training loss:	0.428478
validation loss:	1.010396
validation error:	0.410938
2017-01-19 08:26:12 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:27:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-422.pkl
2017-01-19 08:27:14 >>> epoch: 422 took 61.988s
training loss:	0.429730
validation loss:	0.989575
validation error:	0.413540
2017-01-19 08:27:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:28:14 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-423.pkl
2017-01-19 08:28:16 >>> epoch: 423 took 62.024s
training loss:	0.426941
validation loss:	0.992081
validation error:	0.415980
2017-01-19 08:28:16 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:29:16 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-424.pkl
2017-01-19 08:29:18 >>> epoch: 424 took 62.076s
training loss:	0.425235
validation loss:	1.019990
validation error:	0.413310
2017-01-19 08:29:18 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:30:18 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-425.pkl
2017-01-19 08:30:20 >>> epoch: 425 took 61.955s
training loss:	0.425726
validation loss:	1.000635
validation error:	0.411638
2017-01-19 08:30:20 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:31:21 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-426.pkl
2017-01-19 08:31:23 >>> epoch: 426 took 62.236s
training loss:	0.426070
validation loss:	0.981271
validation error:	0.411170
2017-01-19 08:31:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:32:23 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-427.pkl
2017-01-19 08:32:25 >>> epoch: 427 took 62.092s
training loss:	0.426220
validation loss:	1.006719
validation error:	0.412650
2017-01-19 08:32:25 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:33:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-428.pkl
2017-01-19 08:33:27 >>> epoch: 428 took 62.389s
training loss:	0.426188
validation loss:	1.013569
validation error:	0.412674
2017-01-19 08:33:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:34:27 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-429.pkl
2017-01-19 08:34:29 >>> epoch: 429 took 61.986s
training loss:	0.422325
validation loss:	1.038850
validation error:	0.413085
2017-01-19 08:34:29 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:35:29 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-430.pkl
2017-01-19 08:35:31 >>> epoch: 430 took 61.954s
training loss:	0.424058
validation loss:	1.025816
validation error:	0.411533
2017-01-19 08:35:31 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:36:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-431.pkl
2017-01-19 08:36:33 >>> epoch: 431 took 61.837s
training loss:	0.424413
validation loss:	1.005181
validation error:	0.412172
2017-01-19 08:36:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:37:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-432.pkl
2017-01-19 08:37:34 >>> epoch: 432 took 61.650s
training loss:	0.424954
validation loss:	1.018041
validation error:	0.415549
2017-01-19 08:37:34 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:38:34 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-433.pkl
2017-01-19 08:38:36 >>> epoch: 433 took 61.837s
training loss:	0.421941
validation loss:	1.028629
validation error:	0.415031
2017-01-19 08:38:36 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:39:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-434.pkl
2017-01-19 08:39:38 >>> epoch: 434 took 62.194s
training loss:	0.422867
validation loss:	1.020232
validation error:	0.417723
2017-01-19 08:39:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:40:39 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-435.pkl
2017-01-19 08:40:40 >>> epoch: 435 took 61.919s
training loss:	0.422234
validation loss:	0.993518
validation error:	0.412165
2017-01-19 08:40:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:41:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-436.pkl
2017-01-19 08:41:42 >>> epoch: 436 took 62.032s
training loss:	0.422340
validation loss:	1.024012
validation error:	0.409799
2017-01-19 08:41:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:42:43 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-437.pkl
2017-01-19 08:42:44 >>> epoch: 437 took 62.079s
training loss:	0.421706
validation loss:	1.045502
validation error:	0.410748
2017-01-19 08:42:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:43:45 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-438.pkl
2017-01-19 08:43:47 >>> epoch: 438 took 62.033s
training loss:	0.420280
validation loss:	1.008969
validation error:	0.411357
2017-01-19 08:43:47 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:44:47 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-439.pkl
2017-01-19 08:44:48 >>> epoch: 439 took 61.917s
training loss:	0.421800
validation loss:	1.039144
validation error:	0.414692
2017-01-19 08:44:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:45:49 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-440.pkl
2017-01-19 08:45:51 >>> epoch: 440 took 62.098s
training loss:	0.417291
validation loss:	1.039154
validation error:	0.414047
2017-01-19 08:45:51 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:46:51 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-441.pkl
2017-01-19 08:46:53 >>> epoch: 441 took 62.350s
training loss:	0.420013
validation loss:	1.027964
validation error:	0.413504
2017-01-19 08:46:53 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:47:53 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-442.pkl
2017-01-19 08:47:55 >>> epoch: 442 took 62.419s
training loss:	0.418870
validation loss:	1.024174
validation error:	0.413609
2017-01-19 08:47:55 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:48:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-443.pkl
2017-01-19 08:48:57 >>> epoch: 443 took 61.854s
training loss:	0.419073
validation loss:	1.025318
validation error:	0.415826
2017-01-19 08:48:57 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:49:57 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-444.pkl
2017-01-19 08:49:59 >>> epoch: 444 took 61.973s
training loss:	0.418513
validation loss:	1.035185
validation error:	0.413993
2017-01-19 08:49:59 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:50:59 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-445.pkl
2017-01-19 08:51:01 >>> epoch: 445 took 61.860s
training loss:	0.419474
validation loss:	1.018080
validation error:	0.410746
2017-01-19 08:51:01 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:52:01 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-446.pkl
2017-01-19 08:52:03 >>> epoch: 446 took 61.734s
training loss:	0.418160
validation loss:	1.013082
validation error:	0.414232
2017-01-19 08:52:03 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:53:03 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-447.pkl
2017-01-19 08:53:05 >>> epoch: 447 took 61.995s
training loss:	0.418377
validation loss:	1.012771
validation error:	0.413817
2017-01-19 08:53:05 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:54:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-448.pkl
2017-01-19 08:54:07 >>> epoch: 448 took 62.414s
training loss:	0.415650
validation loss:	1.002904
validation error:	0.414167
2017-01-19 08:54:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:55:07 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-449.pkl
2017-01-19 08:55:09 >>> epoch: 449 took 61.979s
training loss:	0.417382
validation loss:	1.019169
validation error:	0.415000
2017-01-19 08:55:09 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:56:09 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-450.pkl
2017-01-19 08:56:11 >>> epoch: 450 took 61.864s
training loss:	0.415854
validation loss:	1.001160
validation error:	0.412864
2017-01-19 08:56:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:57:11 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-451.pkl
2017-01-19 08:57:13 >>> epoch: 451 took 61.673s
training loss:	0.417541
validation loss:	1.038864
validation error:	0.416188
2017-01-19 08:57:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:58:13 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-452.pkl
2017-01-19 08:58:14 >>> epoch: 452 took 61.550s
training loss:	0.414763
validation loss:	1.027783
validation error:	0.412426
2017-01-19 08:58:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 08:59:14 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-453.pkl
2017-01-19 08:59:16 >>> epoch: 453 took 61.572s
training loss:	0.413439
validation loss:	1.040634
validation error:	0.413290
2017-01-19 08:59:16 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:00:18 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-454.pkl
2017-01-19 09:00:20 >>> epoch: 454 took 63.772s
training loss:	0.416801
validation loss:	1.077479
validation error:	0.412616
2017-01-19 09:00:20 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:01:21 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-455.pkl
2017-01-19 09:01:23 >>> epoch: 455 took 63.310s
training loss:	0.415590
validation loss:	1.010579
validation error:	0.412938
2017-01-19 09:01:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:02:24 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-456.pkl
2017-01-19 09:02:26 >>> epoch: 456 took 62.777s
training loss:	0.412640
validation loss:	1.028195
validation error:	0.414969
2017-01-19 09:02:26 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:03:26 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-457.pkl
2017-01-19 09:03:28 >>> epoch: 457 took 62.110s
training loss:	0.415312
validation loss:	1.011323
validation error:	0.413813
2017-01-19 09:03:28 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:04:29 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-458.pkl
2017-01-19 09:04:30 >>> epoch: 458 took 62.650s
training loss:	0.412069
validation loss:	1.050809
validation error:	0.417618
2017-01-19 09:04:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:05:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-459.pkl
2017-01-19 09:05:33 >>> epoch: 459 took 62.765s
training loss:	0.414986
validation loss:	1.018492
validation error:	0.412730
2017-01-19 09:05:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:06:34 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-460.pkl
2017-01-19 09:06:36 >>> epoch: 460 took 62.509s
training loss:	0.412475
validation loss:	1.043399
validation error:	0.414038
2017-01-19 09:06:36 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:07:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-461.pkl
2017-01-19 09:07:39 >>> epoch: 461 took 62.889s
training loss:	0.412644
validation loss:	1.037832
validation error:	0.413400
2017-01-19 09:07:39 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:08:39 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-462.pkl
2017-01-19 09:08:41 >>> epoch: 462 took 62.083s
training loss:	0.413447
validation loss:	1.024127
validation error:	0.415797
2017-01-19 09:08:41 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:09:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-463.pkl
2017-01-19 09:09:43 >>> epoch: 463 took 62.419s
training loss:	0.412393
validation loss:	1.041479
validation error:	0.412531
2017-01-19 09:09:43 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:10:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-464.pkl
2017-01-19 09:10:46 >>> epoch: 464 took 63.129s
training loss:	0.413056
validation loss:	1.069087
validation error:	0.415545
2017-01-19 09:10:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:11:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-465.pkl
2017-01-19 09:11:50 >>> epoch: 465 took 63.681s
training loss:	0.410316
validation loss:	1.063150
validation error:	0.418364
2017-01-19 09:11:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:12:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-466.pkl
2017-01-19 09:12:52 >>> epoch: 466 took 61.773s
training loss:	0.411174
validation loss:	1.075075
validation error:	0.415768
2017-01-19 09:12:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:13:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-467.pkl
2017-01-19 09:13:55 >>> epoch: 467 took 63.793s
training loss:	0.411442
validation loss:	1.056857
validation error:	0.416304
2017-01-19 09:13:55 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:14:56 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-468.pkl
2017-01-19 09:14:58 >>> epoch: 468 took 62.407s
training loss:	0.410403
validation loss:	1.078699
validation error:	0.410449
2017-01-19 09:14:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:15:58 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-469.pkl
2017-01-19 09:16:00 >>> epoch: 469 took 62.368s
training loss:	0.409725
validation loss:	1.055260
validation error:	0.413179
2017-01-19 09:16:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:17:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-470.pkl
2017-01-19 09:17:04 >>> epoch: 470 took 63.460s
training loss:	0.409819
validation loss:	1.049197
validation error:	0.416920
2017-01-19 09:17:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:18:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-471.pkl
2017-01-19 09:18:06 >>> epoch: 471 took 62.768s
training loss:	0.407890
validation loss:	1.084111
validation error:	0.416761
2017-01-19 09:18:06 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:19:07 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-472.pkl
2017-01-19 09:19:09 >>> epoch: 472 took 62.387s
training loss:	0.409081
validation loss:	1.050942
validation error:	0.415580
2017-01-19 09:19:09 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:20:11 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-473.pkl
2017-01-19 09:20:13 >>> epoch: 473 took 63.867s
training loss:	0.407529
validation loss:	1.051301
validation error:	0.415210
2017-01-19 09:20:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:21:13 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-474.pkl
2017-01-19 09:21:15 >>> epoch: 474 took 62.317s
training loss:	0.409667
validation loss:	1.043534
validation error:	0.416612
2017-01-19 09:21:15 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:22:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-475.pkl
2017-01-19 09:22:18 >>> epoch: 475 took 63.430s
training loss:	0.407181
validation loss:	1.077805
validation error:	0.415674
2017-01-19 09:22:18 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:23:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-476.pkl
2017-01-19 09:23:22 >>> epoch: 476 took 63.120s
training loss:	0.406459
validation loss:	1.105223
validation error:	0.416496
2017-01-19 09:23:22 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:24:22 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-477.pkl
2017-01-19 09:24:24 >>> epoch: 477 took 62.246s
training loss:	0.408471
validation loss:	1.069035
validation error:	0.413933
2017-01-19 09:24:24 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:25:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-478.pkl
2017-01-19 09:25:27 >>> epoch: 478 took 63.144s
training loss:	0.408585
validation loss:	1.055607
validation error:	0.414121
2017-01-19 09:25:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:26:28 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-479.pkl
2017-01-19 09:26:30 >>> epoch: 479 took 63.267s
training loss:	0.406741
validation loss:	1.022921
validation error:	0.413817
2017-01-19 09:26:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:27:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-480.pkl
2017-01-19 09:27:33 >>> epoch: 480 took 62.604s
training loss:	0.407659
validation loss:	1.049916
validation error:	0.411547
2017-01-19 09:27:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:28:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-481.pkl
2017-01-19 09:28:35 >>> epoch: 481 took 62.450s
training loss:	0.408563
validation loss:	1.075839
validation error:	0.414815
2017-01-19 09:28:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:29:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-482.pkl
2017-01-19 09:29:38 >>> epoch: 482 took 62.678s
training loss:	0.406786
validation loss:	1.030926
validation error:	0.413038
2017-01-19 09:29:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:30:39 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-483.pkl
2017-01-19 09:30:41 >>> epoch: 483 took 62.702s
training loss:	0.407381
validation loss:	1.041845
validation error:	0.416801
2017-01-19 09:30:41 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:31:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-484.pkl
2017-01-19 09:31:43 >>> epoch: 484 took 62.547s
training loss:	0.405397
validation loss:	1.066957
validation error:	0.412402
2017-01-19 09:31:43 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:32:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-485.pkl
2017-01-19 09:32:46 >>> epoch: 485 took 62.976s
training loss:	0.404211
validation loss:	1.041010
validation error:	0.416277
2017-01-19 09:32:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:33:47 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-486.pkl
2017-01-19 09:33:49 >>> epoch: 486 took 62.612s
training loss:	0.405348
validation loss:	1.064696
validation error:	0.415739
2017-01-19 09:33:49 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:34:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-487.pkl
2017-01-19 09:34:52 >>> epoch: 487 took 62.826s
training loss:	0.405599
validation loss:	1.045787
validation error:	0.416335
2017-01-19 09:34:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:35:52 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-488.pkl
2017-01-19 09:35:54 >>> epoch: 488 took 62.403s
training loss:	0.404857
validation loss:	1.038235
validation error:	0.413779
2017-01-19 09:35:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:36:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-489.pkl
2017-01-19 09:36:56 >>> epoch: 489 took 61.977s
training loss:	0.403664
validation loss:	1.051935
validation error:	0.412375
2017-01-19 09:36:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:37:57 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-490.pkl
2017-01-19 09:37:59 >>> epoch: 490 took 62.715s
training loss:	0.403375
validation loss:	1.079037
validation error:	0.416150
2017-01-19 09:37:59 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:39:00 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-491.pkl
2017-01-19 09:39:01 >>> epoch: 491 took 62.659s
training loss:	0.404743
validation loss:	1.077209
validation error:	0.418455
2017-01-19 09:39:01 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:40:03 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-492.pkl
2017-01-19 09:40:04 >>> epoch: 492 took 62.971s
training loss:	0.403841
validation loss:	1.078132
validation error:	0.416978
2017-01-19 09:40:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:41:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-493.pkl
2017-01-19 09:41:07 >>> epoch: 493 took 62.550s
training loss:	0.403943
validation loss:	1.082649
validation error:	0.417547
2017-01-19 09:41:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:42:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-494.pkl
2017-01-19 09:42:10 >>> epoch: 494 took 63.348s
training loss:	0.402275
validation loss:	1.056723
validation error:	0.414672
2017-01-19 09:42:10 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:43:11 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-495.pkl
2017-01-19 09:43:12 >>> epoch: 495 took 62.081s
training loss:	0.401347
validation loss:	1.104879
validation error:	0.414754
2017-01-19 09:43:12 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:44:13 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-496.pkl
2017-01-19 09:44:15 >>> epoch: 496 took 62.677s
training loss:	0.403711
validation loss:	1.039975
validation error:	0.412049
2017-01-19 09:44:15 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:45:16 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-497.pkl
2017-01-19 09:45:18 >>> epoch: 497 took 63.121s
training loss:	0.403631
validation loss:	1.098615
validation error:	0.415188
2017-01-19 09:45:18 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:46:19 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-498.pkl
2017-01-19 09:46:20 >>> epoch: 498 took 62.231s
training loss:	0.401391
validation loss:	1.114852
validation error:	0.418875
2017-01-19 09:46:20 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:47:22 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-499.pkl
2017-01-19 09:47:23 >>> epoch: 499 took 63.017s
training loss:	0.401445
validation loss:	1.091958
validation error:	0.418098
2017-01-19 09:47:23 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:48:24 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-500.pkl
2017-01-19 09:48:26 >>> epoch: 500 took 62.598s
training loss:	0.399953
validation loss:	1.089187
validation error:	0.419313
2017-01-19 09:48:26 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:49:27 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-501.pkl
2017-01-19 09:49:29 >>> epoch: 501 took 62.657s
training loss:	0.401383
validation loss:	1.096293
validation error:	0.418210
2017-01-19 09:49:29 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:50:30 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-502.pkl
2017-01-19 09:50:31 >>> epoch: 502 took 62.763s
training loss:	0.400678
validation loss:	1.083190
validation error:	0.417761
2017-01-19 09:50:31 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:51:32 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-503.pkl
2017-01-19 09:51:34 >>> epoch: 503 took 62.766s
training loss:	0.399199
validation loss:	1.091753
validation error:	0.415051
2017-01-19 09:51:34 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:52:35 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-504.pkl
2017-01-19 09:52:37 >>> epoch: 504 took 62.578s
training loss:	0.400279
validation loss:	1.105742
validation error:	0.415975
2017-01-19 09:52:37 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:53:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-505.pkl
2017-01-19 09:53:39 >>> epoch: 505 took 62.424s
training loss:	0.399430
validation loss:	1.099874
validation error:	0.413741
2017-01-19 09:53:39 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:54:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-506.pkl
2017-01-19 09:54:42 >>> epoch: 506 took 62.576s
training loss:	0.398819
validation loss:	1.101124
validation error:	0.416775
2017-01-19 09:54:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:55:42 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-507.pkl
2017-01-19 09:55:44 >>> epoch: 507 took 62.292s
training loss:	0.399621
validation loss:	1.097499
validation error:	0.414788
2017-01-19 09:55:44 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:56:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-508.pkl
2017-01-19 09:56:46 >>> epoch: 508 took 62.335s
training loss:	0.399541
validation loss:	1.114215
validation error:	0.419627
2017-01-19 09:56:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:57:47 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-509.pkl
2017-01-19 09:57:49 >>> epoch: 509 took 62.685s
training loss:	0.398953
validation loss:	1.074984
validation error:	0.418051
2017-01-19 09:57:49 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:58:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-510.pkl
2017-01-19 09:58:52 >>> epoch: 510 took 62.586s
training loss:	0.399334
validation loss:	1.090620
validation error:	0.414257
2017-01-19 09:58:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 09:59:52 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-511.pkl
2017-01-19 09:59:54 >>> epoch: 511 took 62.217s
training loss:	0.395718
validation loss:	1.110639
validation error:	0.415491
2017-01-19 09:59:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:00:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-512.pkl
2017-01-19 10:00:57 >>> epoch: 512 took 62.820s
training loss:	0.395631
validation loss:	1.151408
validation error:	0.417199
2017-01-19 10:00:57 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:01:57 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-513.pkl
2017-01-19 10:01:59 >>> epoch: 513 took 62.222s
training loss:	0.401220
validation loss:	1.111647
validation error:	0.417629
2017-01-19 10:01:59 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:02:59 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-514.pkl
2017-01-19 10:03:01 >>> epoch: 514 took 62.230s
training loss:	0.396534
validation loss:	1.114657
validation error:	0.418040
2017-01-19 10:03:01 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:04:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-515.pkl
2017-01-19 10:04:04 >>> epoch: 515 took 63.005s
training loss:	0.398553
validation loss:	1.071666
validation error:	0.416603
2017-01-19 10:04:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:05:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-516.pkl
2017-01-19 10:05:07 >>> epoch: 516 took 63.156s
training loss:	0.396159
validation loss:	1.098437
validation error:	0.415277
2017-01-19 10:05:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:06:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-517.pkl
2017-01-19 10:06:10 >>> epoch: 517 took 62.244s
training loss:	0.395899
validation loss:	1.104304
validation error:	0.415203
2017-01-19 10:06:10 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:07:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-518.pkl
2017-01-19 10:07:11 >>> epoch: 518 took 61.917s
training loss:	0.395299
validation loss:	1.095241
validation error:	0.413710
2017-01-19 10:07:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:08:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-519.pkl
2017-01-19 10:08:14 >>> epoch: 519 took 62.083s
training loss:	0.395896
validation loss:	1.113220
validation error:	0.417672
2017-01-19 10:08:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:09:15 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-520.pkl
2017-01-19 10:09:17 >>> epoch: 520 took 63.176s
training loss:	0.397570
validation loss:	1.122603
validation error:	0.418335
2017-01-19 10:09:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:10:18 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-521.pkl
2017-01-19 10:10:20 >>> epoch: 521 took 63.417s
training loss:	0.395503
validation loss:	1.145877
validation error:	0.417286
2017-01-19 10:10:20 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:11:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-522.pkl
2017-01-19 10:11:22 >>> epoch: 522 took 62.141s
training loss:	0.393991
validation loss:	1.113623
validation error:	0.417542
2017-01-19 10:11:22 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:12:23 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-523.pkl
2017-01-19 10:12:24 >>> epoch: 523 took 62.175s
training loss:	0.395083
validation loss:	1.145644
validation error:	0.420614
2017-01-19 10:12:24 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:13:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-524.pkl
2017-01-19 10:13:27 >>> epoch: 524 took 62.108s
training loss:	0.397044
validation loss:	1.089559
validation error:	0.417545
2017-01-19 10:13:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:14:29 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-525.pkl
2017-01-19 10:14:30 >>> epoch: 525 took 63.818s
training loss:	0.392870
validation loss:	1.098008
validation error:	0.415435
2017-01-19 10:14:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:15:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-526.pkl
2017-01-19 10:15:33 >>> epoch: 526 took 62.806s
training loss:	0.393693
validation loss:	1.106196
validation error:	0.417875
2017-01-19 10:15:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:16:34 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-527.pkl
2017-01-19 10:16:36 >>> epoch: 527 took 62.756s
training loss:	0.394949
validation loss:	1.138932
validation error:	0.418484
2017-01-19 10:16:36 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:17:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-528.pkl
2017-01-19 10:17:38 >>> epoch: 528 took 62.203s
training loss:	0.394362
validation loss:	1.121993
validation error:	0.417462
2017-01-19 10:17:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:18:38 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-529.pkl
2017-01-19 10:18:40 >>> epoch: 529 took 62.146s
training loss:	0.392541
validation loss:	1.104679
validation error:	0.415482
2017-01-19 10:18:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:19:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-530.pkl
2017-01-19 10:19:42 >>> epoch: 530 took 62.081s
training loss:	0.394004
validation loss:	1.122351
validation error:	0.417194
2017-01-19 10:19:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:20:43 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-531.pkl
2017-01-19 10:20:44 >>> epoch: 531 took 61.993s
training loss:	0.393116
validation loss:	1.099889
validation error:	0.416254
2017-01-19 10:20:44 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:21:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-532.pkl
2017-01-19 10:21:46 >>> epoch: 532 took 61.817s
training loss:	0.390664
validation loss:	1.107386
validation error:	0.417174
2017-01-19 10:21:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:22:47 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-533.pkl
2017-01-19 10:22:48 >>> epoch: 533 took 62.200s
training loss:	0.394170
validation loss:	1.116599
validation error:	0.414703
2017-01-19 10:22:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:23:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-534.pkl
2017-01-19 10:23:50 >>> epoch: 534 took 61.832s
training loss:	0.393460
validation loss:	1.130504
validation error:	0.416982
2017-01-19 10:23:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:24:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-535.pkl
2017-01-19 10:24:52 >>> epoch: 535 took 61.948s
training loss:	0.390062
validation loss:	1.128052
validation error:	0.416353
2017-01-19 10:24:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:25:52 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-536.pkl
2017-01-19 10:25:54 >>> epoch: 536 took 61.745s
training loss:	0.392812
validation loss:	1.076188
validation error:	0.415984
2017-01-19 10:25:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:26:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-537.pkl
2017-01-19 10:26:56 >>> epoch: 537 took 62.586s
training loss:	0.389941
validation loss:	1.142081
validation error:	0.414993
2017-01-19 10:26:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:27:58 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-538.pkl
2017-01-19 10:27:59 >>> epoch: 538 took 62.926s
training loss:	0.389339
validation loss:	1.154316
validation error:	0.415844
2017-01-19 10:27:59 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:29:00 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-539.pkl
2017-01-19 10:29:02 >>> epoch: 539 took 62.367s
training loss:	0.394015
validation loss:	1.126026
validation error:	0.418337
2017-01-19 10:29:02 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:30:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-540.pkl
2017-01-19 10:30:04 >>> epoch: 540 took 62.409s
training loss:	0.389964
validation loss:	1.121804
validation error:	0.416188
2017-01-19 10:30:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:31:06 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-541.pkl
2017-01-19 10:31:08 >>> epoch: 541 took 63.579s
training loss:	0.389912
validation loss:	1.135787
validation error:	0.418033
2017-01-19 10:31:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:32:09 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-542.pkl
2017-01-19 10:32:10 >>> epoch: 542 took 62.729s
training loss:	0.390123
validation loss:	1.079854
validation error:	0.416692
2017-01-19 10:32:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:33:11 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-543.pkl
2017-01-19 10:33:13 >>> epoch: 543 took 62.728s
training loss:	0.390299
validation loss:	1.113028
validation error:	0.414458
2017-01-19 10:33:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:34:14 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-544.pkl
2017-01-19 10:34:15 >>> epoch: 544 took 62.236s
training loss:	0.391962
validation loss:	1.123197
validation error:	0.415741
2017-01-19 10:34:15 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:35:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-545.pkl
2017-01-19 10:35:18 >>> epoch: 545 took 63.004s
training loss:	0.389104
validation loss:	1.175948
validation error:	0.421223
2017-01-19 10:35:18 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:36:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-546.pkl
2017-01-19 10:36:21 >>> epoch: 546 took 62.928s
training loss:	0.389639
validation loss:	1.125120
validation error:	0.416174
2017-01-19 10:36:21 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:37:22 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-547.pkl
2017-01-19 10:37:23 >>> epoch: 547 took 62.121s
training loss:	0.388419
validation loss:	1.135364
validation error:	0.415116
2017-01-19 10:37:24 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:38:24 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-548.pkl
2017-01-19 10:38:26 >>> epoch: 548 took 62.219s
training loss:	0.391129
validation loss:	1.072879
validation error:	0.414196
2017-01-19 10:38:26 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:39:27 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-549.pkl
2017-01-19 10:39:28 >>> epoch: 549 took 62.699s
training loss:	0.386215
validation loss:	1.134934
validation error:	0.414842
2017-01-19 10:39:28 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:40:30 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-550.pkl
2017-01-19 10:40:31 >>> epoch: 550 took 63.047s
training loss:	0.389904
validation loss:	1.121344
validation error:	0.416612
2017-01-19 10:40:31 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:41:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-551.pkl
2017-01-19 10:41:35 >>> epoch: 551 took 63.420s
training loss:	0.385367
validation loss:	1.128349
validation error:	0.416404
2017-01-19 10:41:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:42:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-552.pkl
2017-01-19 10:42:38 >>> epoch: 552 took 63.436s
training loss:	0.388313
validation loss:	1.144963
validation error:	0.418011
2017-01-19 10:42:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:43:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-553.pkl
2017-01-19 10:43:42 >>> epoch: 553 took 63.979s
training loss:	0.388043
validation loss:	1.124902
validation error:	0.417252
2017-01-19 10:43:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:44:43 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-554.pkl
2017-01-19 10:44:45 >>> epoch: 554 took 62.767s
training loss:	0.388183
validation loss:	1.112538
validation error:	0.416096
2017-01-19 10:44:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:45:47 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-555.pkl
2017-01-19 10:45:48 >>> epoch: 555 took 63.335s
training loss:	0.387120
validation loss:	1.153615
validation error:	0.416266
2017-01-19 10:45:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:46:49 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-556.pkl
2017-01-19 10:46:51 >>> epoch: 556 took 62.677s
training loss:	0.385484
validation loss:	1.108112
validation error:	0.417105
2017-01-19 10:46:51 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:47:53 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-557.pkl
2017-01-19 10:47:54 >>> epoch: 557 took 63.097s
training loss:	0.383926
validation loss:	1.150716
validation error:	0.418683
2017-01-19 10:47:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:48:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-558.pkl
2017-01-19 10:48:57 >>> epoch: 558 took 62.962s
training loss:	0.388157
validation loss:	1.135023
validation error:	0.416183
2017-01-19 10:48:57 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:49:58 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-559.pkl
2017-01-19 10:50:00 >>> epoch: 559 took 62.953s
training loss:	0.387221
validation loss:	1.124633
validation error:	0.417489
2017-01-19 10:50:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:51:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-560.pkl
2017-01-19 10:51:04 >>> epoch: 560 took 63.712s
training loss:	0.383492
validation loss:	1.157982
validation error:	0.416087
2017-01-19 10:51:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:52:06 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-561.pkl
2017-01-19 10:52:08 >>> epoch: 561 took 64.059s
training loss:	0.386525
validation loss:	1.151952
validation error:	0.416922
2017-01-19 10:52:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:53:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-562.pkl
2017-01-19 10:53:11 >>> epoch: 562 took 63.450s
training loss:	0.385853
validation loss:	1.149910
validation error:	0.418538
2017-01-19 10:53:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:54:14 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-563.pkl
2017-01-19 10:54:16 >>> epoch: 563 took 64.644s
training loss:	0.384118
validation loss:	1.143864
validation error:	0.418924
2017-01-19 10:54:16 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:55:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-564.pkl
2017-01-19 10:55:19 >>> epoch: 564 took 62.916s
training loss:	0.385708
validation loss:	1.120361
validation error:	0.415413
2017-01-19 10:55:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:56:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-565.pkl
2017-01-19 10:56:22 >>> epoch: 565 took 62.675s
training loss:	0.384852
validation loss:	1.146999
validation error:	0.415136
2017-01-19 10:56:22 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:57:22 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-566.pkl
2017-01-19 10:57:24 >>> epoch: 566 took 62.513s
training loss:	0.385946
validation loss:	1.143992
validation error:	0.423067
2017-01-19 10:57:24 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:58:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-567.pkl
2017-01-19 10:58:27 >>> epoch: 567 took 63.048s
training loss:	0.385777
validation loss:	1.165675
validation error:	0.418478
2017-01-19 10:58:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 10:59:28 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-568.pkl
2017-01-19 10:59:30 >>> epoch: 568 took 62.887s
training loss:	0.383070
validation loss:	1.134321
validation error:	0.418795
2017-01-19 10:59:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:00:32 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-569.pkl
2017-01-19 11:00:34 >>> epoch: 569 took 63.946s
training loss:	0.383606
validation loss:	1.135044
validation error:	0.418342
2017-01-19 11:00:34 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:01:35 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-570.pkl
2017-01-19 11:01:37 >>> epoch: 570 took 62.777s
training loss:	0.383876
validation loss:	1.131957
validation error:	0.420658
2017-01-19 11:01:37 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:02:38 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-571.pkl
2017-01-19 11:02:40 >>> epoch: 571 took 62.928s
training loss:	0.382072
validation loss:	1.163004
validation error:	0.418375
2017-01-19 11:02:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:03:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-572.pkl
2017-01-19 11:03:42 >>> epoch: 572 took 62.754s
training loss:	0.382209
validation loss:	1.172566
validation error:	0.421583
2017-01-19 11:03:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:04:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-573.pkl
2017-01-19 11:04:45 >>> epoch: 573 took 63.058s
training loss:	0.382039
validation loss:	1.167629
validation error:	0.418411
2017-01-19 11:04:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:05:47 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-574.pkl
2017-01-19 11:05:48 >>> epoch: 574 took 62.972s
training loss:	0.382479
validation loss:	1.159056
validation error:	0.423728
2017-01-19 11:05:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:06:49 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-575.pkl
2017-01-19 11:06:51 >>> epoch: 575 took 62.827s
training loss:	0.382078
validation loss:	1.195003
validation error:	0.419511
2017-01-19 11:06:51 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:07:52 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-576.pkl
2017-01-19 11:07:54 >>> epoch: 576 took 62.617s
training loss:	0.385000
validation loss:	1.176910
validation error:	0.419589
2017-01-19 11:07:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:08:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-577.pkl
2017-01-19 11:08:57 >>> epoch: 577 took 62.831s
training loss:	0.383759
validation loss:	1.148914
validation error:	0.416310
2017-01-19 11:08:57 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:09:58 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-578.pkl
2017-01-19 11:10:00 >>> epoch: 578 took 62.811s
training loss:	0.382853
validation loss:	1.177085
validation error:	0.417996
2017-01-19 11:10:00 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:11:01 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-579.pkl
2017-01-19 11:11:02 >>> epoch: 579 took 62.894s
training loss:	0.382457
validation loss:	1.160456
validation error:	0.416690
2017-01-19 11:11:02 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:12:03 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-580.pkl
2017-01-19 11:12:05 >>> epoch: 580 took 62.677s
training loss:	0.382589
validation loss:	1.160775
validation error:	0.415790
2017-01-19 11:12:05 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:13:06 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-581.pkl
2017-01-19 11:13:08 >>> epoch: 581 took 62.831s
training loss:	0.380467
validation loss:	1.160637
validation error:	0.419319
2017-01-19 11:13:08 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:14:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-582.pkl
2017-01-19 11:14:12 >>> epoch: 582 took 63.685s
training loss:	0.379890
validation loss:	1.162281
validation error:	0.422940
2017-01-19 11:14:12 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:15:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-583.pkl
2017-01-19 11:15:14 >>> epoch: 583 took 62.569s
training loss:	0.383142
validation loss:	1.142833
validation error:	0.420342
2017-01-19 11:15:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:16:15 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-584.pkl
2017-01-19 11:16:17 >>> epoch: 584 took 62.737s
training loss:	0.382206
validation loss:	1.153309
validation error:	0.420746
2017-01-19 11:16:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:17:18 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-585.pkl
2017-01-19 11:17:20 >>> epoch: 585 took 62.649s
training loss:	0.380050
validation loss:	1.164783
validation error:	0.420248
2017-01-19 11:17:20 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:18:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-586.pkl
2017-01-19 11:18:22 >>> epoch: 586 took 62.845s
training loss:	0.379701
validation loss:	1.186899
validation error:	0.421846
2017-01-19 11:18:22 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:19:24 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-587.pkl
2017-01-19 11:19:26 >>> epoch: 587 took 63.112s
training loss:	0.379870
validation loss:	1.190328
validation error:	0.422152
2017-01-19 11:19:26 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:20:27 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-588.pkl
2017-01-19 11:20:28 >>> epoch: 588 took 62.970s
training loss:	0.378792
validation loss:	1.190110
validation error:	0.421536
2017-01-19 11:20:29 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:21:30 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-589.pkl
2017-01-19 11:21:32 >>> epoch: 589 took 63.139s
training loss:	0.381601
validation loss:	1.147267
validation error:	0.415435
2017-01-19 11:21:32 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:22:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-590.pkl
2017-01-19 11:22:35 >>> epoch: 590 took 63.558s
training loss:	0.379896
validation loss:	1.166273
validation error:	0.418732
2017-01-19 11:22:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:23:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-591.pkl
2017-01-19 11:23:38 >>> epoch: 591 took 62.605s
training loss:	0.378828
validation loss:	1.160050
validation error:	0.416152
2017-01-19 11:23:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:24:38 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-592.pkl
2017-01-19 11:24:40 >>> epoch: 592 took 62.543s
training loss:	0.380545
validation loss:	1.162080
validation error:	0.421033
2017-01-19 11:24:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:25:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-593.pkl
2017-01-19 11:25:43 >>> epoch: 593 took 62.700s
training loss:	0.378522
validation loss:	1.164483
validation error:	0.420393
2017-01-19 11:25:43 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:26:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-594.pkl
2017-01-19 11:26:45 >>> epoch: 594 took 62.346s
training loss:	0.377707
validation loss:	1.185486
validation error:	0.419625
2017-01-19 11:26:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:27:46 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-595.pkl
2017-01-19 11:27:48 >>> epoch: 595 took 62.780s
training loss:	0.378280
validation loss:	1.196186
validation error:	0.423350
2017-01-19 11:27:48 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:28:49 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-596.pkl
2017-01-19 11:28:51 >>> epoch: 596 took 62.590s
training loss:	0.380808
validation loss:	1.198407
validation error:	0.421339
2017-01-19 11:28:51 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:29:51 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-597.pkl
2017-01-19 11:29:53 >>> epoch: 597 took 62.387s
training loss:	0.377511
validation loss:	1.211234
validation error:	0.419763
2017-01-19 11:29:53 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:30:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-598.pkl
2017-01-19 11:30:55 >>> epoch: 598 took 62.373s
training loss:	0.379772
validation loss:	1.209787
validation error:	0.423540
2017-01-19 11:30:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:31:57 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-599.pkl
2017-01-19 11:31:59 >>> epoch: 599 took 63.145s
training loss:	0.376915
validation loss:	1.204170
validation error:	0.418692
2017-01-19 11:31:59 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:32:59 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-600.pkl
2017-01-19 11:33:01 >>> epoch: 600 took 62.477s
training loss:	0.376181
validation loss:	1.189048
validation error:	0.417344
2017-01-19 11:33:01 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:34:02 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-601.pkl
2017-01-19 11:34:04 >>> epoch: 601 took 62.452s
training loss:	0.379523
validation loss:	1.199562
validation error:	0.421379
2017-01-19 11:34:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:35:04 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-602.pkl
2017-01-19 11:35:06 >>> epoch: 602 took 62.557s
training loss:	0.376657
validation loss:	1.213522
validation error:	0.415964
2017-01-19 11:35:06 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:36:07 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-603.pkl
2017-01-19 11:36:09 >>> epoch: 603 took 62.531s
training loss:	0.380510
validation loss:	1.169867
validation error:	0.422853
2017-01-19 11:36:09 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:37:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-604.pkl
2017-01-19 11:37:11 >>> epoch: 604 took 62.766s
training loss:	0.377974
validation loss:	1.192903
validation error:	0.420042
2017-01-19 11:37:11 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:38:12 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-605.pkl
2017-01-19 11:38:14 >>> epoch: 605 took 62.754s
training loss:	0.375321
validation loss:	1.193122
validation error:	0.417775
2017-01-19 11:38:14 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:39:15 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-606.pkl
2017-01-19 11:39:17 >>> epoch: 606 took 62.488s
training loss:	0.377280
validation loss:	1.137857
validation error:	0.418031
2017-01-19 11:39:17 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:40:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-607.pkl
2017-01-19 11:40:19 >>> epoch: 607 took 62.612s
training loss:	0.374361
validation loss:	1.210599
validation error:	0.420125
2017-01-19 11:40:19 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:41:20 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-608.pkl
2017-01-19 11:41:22 >>> epoch: 608 took 62.313s
training loss:	0.377294
validation loss:	1.169571
validation error:	0.415145
2017-01-19 11:41:22 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:42:22 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-609.pkl
2017-01-19 11:42:24 >>> epoch: 609 took 62.460s
training loss:	0.378016
validation loss:	1.155858
validation error:	0.419315
2017-01-19 11:42:24 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:43:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-610.pkl
2017-01-19 11:43:27 >>> epoch: 610 took 62.654s
training loss:	0.373835
validation loss:	1.224599
validation error:	0.417625
2017-01-19 11:43:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:44:28 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-611.pkl
2017-01-19 11:44:30 >>> epoch: 611 took 63.214s
training loss:	0.376654
validation loss:	1.150608
validation error:	0.420406
2017-01-19 11:44:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:45:31 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-612.pkl
2017-01-19 11:45:33 >>> epoch: 612 took 62.582s
training loss:	0.375537
validation loss:	1.187373
validation error:	0.419362
2017-01-19 11:45:33 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:46:33 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-613.pkl
2017-01-19 11:46:35 >>> epoch: 613 took 62.515s
training loss:	0.374119
validation loss:	1.183219
validation error:	0.418105
2017-01-19 11:46:35 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:47:36 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-614.pkl
2017-01-19 11:47:37 >>> epoch: 614 took 62.451s
training loss:	0.374491
validation loss:	1.188295
validation error:	0.420288
2017-01-19 11:47:38 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:48:38 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-615.pkl
2017-01-19 11:48:40 >>> epoch: 615 took 62.443s
training loss:	0.372742
validation loss:	1.190324
validation error:	0.418641
2017-01-19 11:48:40 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:49:41 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-616.pkl
2017-01-19 11:49:42 >>> epoch: 616 took 62.520s
training loss:	0.373647
validation loss:	1.211614
validation error:	0.417263
2017-01-19 11:49:42 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:50:43 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-617.pkl
2017-01-19 11:50:45 >>> epoch: 617 took 62.373s
training loss:	0.374046
validation loss:	1.208162
validation error:	0.417739
2017-01-19 11:50:45 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:51:45 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-618.pkl
2017-01-19 11:51:47 >>> epoch: 618 took 62.363s
training loss:	0.371814
validation loss:	1.223785
validation error:	0.420688
2017-01-19 11:51:47 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:52:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-619.pkl
2017-01-19 11:52:50 >>> epoch: 619 took 62.617s
training loss:	0.375436
validation loss:	1.159983
validation error:	0.417127
2017-01-19 11:52:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:53:50 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-620.pkl
2017-01-19 11:53:52 >>> epoch: 620 took 62.220s
training loss:	0.373375
validation loss:	1.172723
validation error:	0.419268
2017-01-19 11:53:52 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:54:53 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-621.pkl
2017-01-19 11:54:54 >>> epoch: 621 took 62.304s
training loss:	0.373968
validation loss:	1.194878
validation error:	0.419676
2017-01-19 11:54:54 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:55:55 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-622.pkl
2017-01-19 11:55:57 >>> epoch: 622 took 62.564s
training loss:	0.373215
validation loss:	1.212324
validation error:	0.419446
2017-01-19 11:55:57 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:56:58 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-623.pkl
2017-01-19 11:56:59 >>> epoch: 623 took 62.576s
training loss:	0.372599
validation loss:	1.213776
validation error:	0.416625
2017-01-19 11:56:59 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:58:00 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-624.pkl
2017-01-19 11:58:02 >>> epoch: 624 took 62.583s
training loss:	0.375944
validation loss:	1.199235
validation error:	0.418924
2017-01-19 11:58:02 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 11:59:03 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-625.pkl
2017-01-19 11:59:04 >>> epoch: 625 took 62.395s
training loss:	0.371888
validation loss:	1.168750
validation error:	0.416045
2017-01-19 11:59:04 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:00:05 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-626.pkl
2017-01-19 12:00:07 >>> epoch: 626 took 62.638s
training loss:	0.374343
validation loss:	1.183885
validation error:	0.420482
2017-01-19 12:00:07 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:01:08 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-627.pkl
2017-01-19 12:01:09 >>> epoch: 627 took 62.412s
training loss:	0.373466
validation loss:	1.227537
validation error:	0.420330
2017-01-19 12:01:10 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:02:10 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-628.pkl
2017-01-19 12:02:13 >>> epoch: 628 took 63.319s
training loss:	0.374096
validation loss:	1.152953
validation error:	0.416980
2017-01-19 12:02:13 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:03:14 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-629.pkl
2017-01-19 12:03:16 >>> epoch: 629 took 63.158s
training loss:	0.371436
validation loss:	1.182000
validation error:	0.416319
2017-01-19 12:03:16 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:04:17 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-630.pkl
2017-01-19 12:04:18 >>> epoch: 630 took 62.471s
training loss:	0.371490
validation loss:	1.188921
validation error:	0.415469
2017-01-19 12:04:18 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:05:19 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-631.pkl
2017-01-19 12:05:21 >>> epoch: 631 took 62.567s
training loss:	0.371355
validation loss:	1.191170
validation error:	0.420531
2017-01-19 12:05:21 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:06:22 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-632.pkl
2017-01-19 12:06:24 >>> epoch: 632 took 63.027s
training loss:	0.372916
validation loss:	1.198697
validation error:	0.420460
2017-01-19 12:06:24 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:07:25 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-633.pkl
2017-01-19 12:07:27 >>> epoch: 633 took 62.960s
training loss:	0.371199
validation loss:	1.202062
validation error:	0.419422
2017-01-19 12:07:27 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:08:28 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-634.pkl
2017-01-19 12:08:30 >>> epoch: 634 took 63.286s
training loss:	0.371857
validation loss:	1.209169
validation error:	0.421917
2017-01-19 12:08:30 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:09:32 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-635.pkl
2017-01-19 12:09:34 >>> epoch: 635 took 63.634s
training loss:	0.371011
validation loss:	1.212555
validation error:	0.417991
2017-01-19 12:09:34 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:10:34 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-636.pkl
2017-01-19 12:10:36 >>> epoch: 636 took 62.088s
training loss:	0.371635
validation loss:	1.174568
validation error:	0.418859
2017-01-19 12:10:36 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:11:37 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-637.pkl
2017-01-19 12:11:39 >>> epoch: 637 took 62.703s
training loss:	0.371307
validation loss:	1.201896
validation error:	0.416658
2017-01-19 12:11:39 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:12:40 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-638.pkl
2017-01-19 12:12:41 >>> epoch: 638 took 62.474s
training loss:	0.371900
validation loss:	1.195590
validation error:	0.418893
2017-01-19 12:12:41 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:13:42 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-639.pkl
2017-01-19 12:13:44 >>> epoch: 639 took 62.458s
training loss:	0.370522
validation loss:	1.243771
validation error:	0.418795
2017-01-19 12:13:44 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:14:44 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-640.pkl
2017-01-19 12:14:46 >>> epoch: 640 took 62.402s
training loss:	0.369909
validation loss:	1.209228
validation error:	0.420277
2017-01-19 12:14:46 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:15:48 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-641.pkl
2017-01-19 12:15:50 >>> epoch: 641 took 64.003s
training loss:	0.370474
validation loss:	1.192889
validation error:	0.419799
2017-01-19 12:15:50 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:16:51 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-642.pkl
2017-01-19 12:16:53 >>> epoch: 642 took 63.076s
training loss:	0.373138
validation loss:	1.214616
validation error:	0.415710
2017-01-19 12:16:53 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:17:54 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-643.pkl
2017-01-19 12:17:56 >>> epoch: 643 took 62.982s
training loss:	0.368657
validation loss:	1.210952
validation error:	0.415333
2017-01-19 12:17:56 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
2017-01-19 12:18:57 >>> Saving network params to trainNN/out/v041-lstm-continous-adadelta/epoch-644.pkl
2017-01-19 12:18:58 >>> epoch: 644 took 62.257s
training loss:	0.370609
validation loss:	1.220562
validation error:	0.416402
2017-01-19 12:18:58 >>> Wrote output to trainNN/out/v041-lstm-continous-adadelta/config.json
