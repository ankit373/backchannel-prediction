2016-11-25 19:54:42 >>> version=v10-glorot
2016-11-25 19:54:42 >>> loading config file extract_pfiles_python/out/v09-without-frame-limiting-context40/config.json
2016-11-25 19:54:42 >>> loading numpy file extract_pfiles_python/out/v09-without-frame-limiting-context40/train.npz
2016-11-25 19:54:45 >>> loading numpy file extract_pfiles_python/out/v09-without-frame-limiting-context40/validate.npz
2016-11-25 19:54:45 >>> Using fuzzy_newbob as schedulung method.
2016-11-25 19:54:45 >>> Training network with 21452 trainable out of 21452 total params.
2016-11-25 19:54:45 >>> Using adadelta with learning_rate=1.000000
2016-11-25 19:54:45 >>> Compiling theano functions...
2016-11-25 19:54:46 >>> Starting training...
2016-11-25 19:55:04 >>>   training loss:	0.656409
2016-11-25 19:55:04 >>> Saving network params to trainNN/out/v10-glorot/epoch-000.pkl
2016-11-25 19:55:05 >>> epoch: 0 validation error:		0.385296
2016-11-25 19:55:23 >>>   training loss:	0.645342
2016-11-25 19:55:23 >>> Saving network params to trainNN/out/v10-glorot/epoch-001.pkl
2016-11-25 19:55:24 >>> epoch: 1 validation error:		0.374242
2016-11-25 19:55:41 >>>   training loss:	0.639672
2016-11-25 19:55:41 >>> Saving network params to trainNN/out/v10-glorot/epoch-002.pkl
2016-11-25 19:55:43 >>> epoch: 2 validation error:		0.370275
2016-11-25 19:56:03 >>>   training loss:	0.636271
2016-11-25 19:56:03 >>> Saving network params to trainNN/out/v10-glorot/epoch-003.pkl
2016-11-25 19:56:04 >>> epoch: 3 validation error:		0.365462
2016-11-25 19:56:23 >>>   training loss:	0.633808
2016-11-25 19:56:23 >>> Saving network params to trainNN/out/v10-glorot/epoch-004.pkl
2016-11-25 19:56:24 >>> epoch: 4 validation error:		0.363142
2016-11-25 19:56:43 >>>   training loss:	0.631891
2016-11-25 19:56:43 >>> Saving network params to trainNN/out/v10-glorot/epoch-005.pkl
2016-11-25 19:56:44 >>> epoch: 5 validation error:		0.360087
2016-11-25 19:57:05 >>>   training loss:	0.630220
2016-11-25 19:57:05 >>> Saving network params to trainNN/out/v10-glorot/epoch-006.pkl
2016-11-25 19:57:07 >>> epoch: 6 validation error:		0.358016
2016-11-25 19:57:27 >>>   training loss:	0.628832
2016-11-25 19:57:27 >>> Saving network params to trainNN/out/v10-glorot/epoch-007.pkl
2016-11-25 19:57:28 >>> epoch: 7 validation error:		0.356799
2016-11-25 19:57:45 >>>   training loss:	0.627600
2016-11-25 19:57:45 >>> Saving network params to trainNN/out/v10-glorot/epoch-008.pkl
2016-11-25 19:57:47 >>> epoch: 8 validation error:		0.360728
2016-11-25 19:57:47 >>> Loading old params from trainNN/out/v10-glorot/epoch-007.pkl
2016-11-25 19:57:47 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.500000
2016-11-25 19:57:47 >>> Re-compiling train function...
2016-11-25 19:58:06 >>>   training loss:	0.626225
2016-11-25 19:58:06 >>> Saving network params to trainNN/out/v10-glorot/epoch-009.pkl
2016-11-25 19:58:07 >>> epoch: 9 validation error:		0.355477
2016-11-25 19:58:26 >>>   training loss:	0.625532
2016-11-25 19:58:26 >>> Saving network params to trainNN/out/v10-glorot/epoch-010.pkl
2016-11-25 19:58:27 >>> epoch: 10 validation error:		0.357481
2016-11-25 19:58:27 >>> Loading old params from trainNN/out/v10-glorot/epoch-009.pkl
2016-11-25 19:58:27 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.250000
2016-11-25 19:58:27 >>> Re-compiling train function...
2016-11-25 19:58:44 >>>   training loss:	0.624761
2016-11-25 19:58:44 >>> Saving network params to trainNN/out/v10-glorot/epoch-011.pkl
2016-11-25 19:58:45 >>> epoch: 11 validation error:		0.354719
2016-11-25 19:59:04 >>>   training loss:	0.624350
2016-11-25 19:59:04 >>> Saving network params to trainNN/out/v10-glorot/epoch-012.pkl
2016-11-25 19:59:05 >>> epoch: 12 validation error:		0.353413
2016-11-25 19:59:22 >>>   training loss:	0.623994
2016-11-25 19:59:22 >>> Saving network params to trainNN/out/v10-glorot/epoch-013.pkl
2016-11-25 19:59:23 >>> epoch: 13 validation error:		0.353598
2016-11-25 19:59:23 >>> Loading old params from trainNN/out/v10-glorot/epoch-012.pkl
2016-11-25 19:59:23 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.125000
2016-11-25 19:59:23 >>> Re-compiling train function...
2016-11-25 19:59:40 >>>   training loss:	0.623545
2016-11-25 19:59:40 >>> Saving network params to trainNN/out/v10-glorot/epoch-014.pkl
2016-11-25 19:59:41 >>> epoch: 14 validation error:		0.353262
2016-11-25 19:59:58 >>>   training loss:	0.623316
2016-11-25 19:59:58 >>> Saving network params to trainNN/out/v10-glorot/epoch-015.pkl
2016-11-25 20:00:00 >>> epoch: 15 validation error:		0.353619
2016-11-25 20:00:00 >>> Loading old params from trainNN/out/v10-glorot/epoch-014.pkl
2016-11-25 20:00:00 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.062500
2016-11-25 20:00:00 >>> Re-compiling train function...
2016-11-25 20:00:17 >>>   training loss:	0.623094
2016-11-25 20:00:17 >>> Saving network params to trainNN/out/v10-glorot/epoch-016.pkl
2016-11-25 20:00:18 >>> epoch: 16 validation error:		0.352803
2016-11-25 20:00:35 >>>   training loss:	0.622972
2016-11-25 20:00:35 >>> Saving network params to trainNN/out/v10-glorot/epoch-017.pkl
2016-11-25 20:00:37 >>> epoch: 17 validation error:		0.352672
2016-11-25 20:00:55 >>>   training loss:	0.622870
2016-11-25 20:00:55 >>> Saving network params to trainNN/out/v10-glorot/epoch-018.pkl
2016-11-25 20:00:57 >>> epoch: 18 validation error:		0.352692
2016-11-25 20:00:57 >>> Loading old params from trainNN/out/v10-glorot/epoch-017.pkl
2016-11-25 20:00:57 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.031250
2016-11-25 20:00:57 >>> Re-compiling train function...
2016-11-25 20:01:19 >>>   training loss:	0.622744
2016-11-25 20:01:19 >>> Saving network params to trainNN/out/v10-glorot/epoch-019.pkl
2016-11-25 20:01:20 >>> epoch: 19 validation error:		0.352678
2016-11-25 20:01:20 >>> Loading old params from trainNN/out/v10-glorot/epoch-017.pkl
2016-11-25 20:01:20 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.015625
2016-11-25 20:01:20 >>> Re-compiling train function...
2016-11-25 20:01:38 >>>   training loss:	0.622685
2016-11-25 20:01:38 >>> Saving network params to trainNN/out/v10-glorot/epoch-020.pkl
2016-11-25 20:01:39 >>> epoch: 20 validation error:		0.352634
2016-11-25 20:01:56 >>>   training loss:	0.622643
2016-11-25 20:01:56 >>> Saving network params to trainNN/out/v10-glorot/epoch-021.pkl
2016-11-25 20:01:58 >>> epoch: 21 validation error:		0.353023
2016-11-25 20:01:58 >>> Loading old params from trainNN/out/v10-glorot/epoch-020.pkl
2016-11-25 20:01:58 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.007812
2016-11-25 20:01:58 >>> Re-compiling train function...
2016-11-25 20:02:16 >>>   training loss:	0.622610
2016-11-25 20:02:16 >>> Saving network params to trainNN/out/v10-glorot/epoch-022.pkl
2016-11-25 20:02:18 >>> epoch: 22 validation error:		0.352904
2016-11-25 20:02:18 >>> Loading old params from trainNN/out/v10-glorot/epoch-020.pkl
2016-11-25 20:02:18 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.003906
2016-11-25 20:02:18 >>> Re-compiling train function...
2016-11-25 20:02:35 >>>   training loss:	0.622596
2016-11-25 20:02:35 >>> Saving network params to trainNN/out/v10-glorot/epoch-023.pkl
2016-11-25 20:02:37 >>> epoch: 23 validation error:		0.352811
2016-11-25 20:02:37 >>> Loading old params from trainNN/out/v10-glorot/epoch-020.pkl
2016-11-25 20:02:37 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.001953
2016-11-25 20:02:37 >>> Re-compiling train function...
2016-11-25 20:02:54 >>>   training loss:	0.622589
2016-11-25 20:02:54 >>> Saving network params to trainNN/out/v10-glorot/epoch-024.pkl
2016-11-25 20:02:55 >>> epoch: 24 validation error:		0.352707
2016-11-25 20:02:55 >>> Loading old params from trainNN/out/v10-glorot/epoch-020.pkl
2016-11-25 20:02:55 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000977
2016-11-25 20:02:55 >>> Re-compiling train function...
2016-11-25 20:03:16 >>>   training loss:	0.622588
2016-11-25 20:03:16 >>> Saving network params to trainNN/out/v10-glorot/epoch-025.pkl
2016-11-25 20:03:17 >>> epoch: 25 validation error:		0.352660
2016-11-25 20:03:17 >>> Loading old params from trainNN/out/v10-glorot/epoch-020.pkl
2016-11-25 20:03:18 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000488
2016-11-25 20:03:18 >>> Re-compiling train function...
2016-11-25 20:03:35 >>>   training loss:	0.622595
2016-11-25 20:03:35 >>> Saving network params to trainNN/out/v10-glorot/epoch-026.pkl
2016-11-25 20:03:37 >>> epoch: 26 validation error:		0.352582
2016-11-25 20:03:56 >>>   training loss:	0.622579
2016-11-25 20:03:56 >>> Saving network params to trainNN/out/v10-glorot/epoch-027.pkl
2016-11-25 20:03:58 >>> epoch: 27 validation error:		0.352681
2016-11-25 20:03:58 >>> Loading old params from trainNN/out/v10-glorot/epoch-026.pkl
2016-11-25 20:03:58 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000244
2016-11-25 20:03:58 >>> Re-compiling train function...
2016-11-25 20:04:16 >>>   training loss:	0.622582
2016-11-25 20:04:16 >>> Saving network params to trainNN/out/v10-glorot/epoch-028.pkl
2016-11-25 20:04:18 >>> epoch: 28 validation error:		0.352646
2016-11-25 20:04:18 >>> Loading old params from trainNN/out/v10-glorot/epoch-026.pkl
2016-11-25 20:04:18 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000122
2016-11-25 20:04:18 >>> Re-compiling train function...
2016-11-25 20:04:35 >>>   training loss:	0.622581
2016-11-25 20:04:35 >>> Saving network params to trainNN/out/v10-glorot/epoch-029.pkl
2016-11-25 20:04:37 >>> epoch: 29 validation error:		0.352564
2016-11-25 20:04:55 >>>   training loss:	0.622579
2016-11-25 20:04:55 >>> Saving network params to trainNN/out/v10-glorot/epoch-030.pkl
2016-11-25 20:04:56 >>> epoch: 30 validation error:		0.352611
2016-11-25 20:04:56 >>> Loading old params from trainNN/out/v10-glorot/epoch-029.pkl
2016-11-25 20:04:56 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000061
2016-11-25 20:04:56 >>> Re-compiling train function...
2016-11-25 20:05:16 >>>   training loss:	0.622578
2016-11-25 20:05:16 >>> Saving network params to trainNN/out/v10-glorot/epoch-031.pkl
2016-11-25 20:05:18 >>> epoch: 31 validation error:		0.352608
2016-11-25 20:05:18 >>> Loading old params from trainNN/out/v10-glorot/epoch-029.pkl
2016-11-25 20:05:18 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000031
2016-11-25 20:05:18 >>> Re-compiling train function...
2016-11-25 20:05:36 >>>   training loss:	0.622579
2016-11-25 20:05:36 >>> Saving network params to trainNN/out/v10-glorot/epoch-032.pkl
2016-11-25 20:05:38 >>> epoch: 32 validation error:		0.352614
2016-11-25 20:05:38 >>> Loading old params from trainNN/out/v10-glorot/epoch-029.pkl
2016-11-25 20:05:38 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000015
2016-11-25 20:05:38 >>> Re-compiling train function...
2016-11-25 20:05:57 >>>   training loss:	0.622576
2016-11-25 20:05:57 >>> Saving network params to trainNN/out/v10-glorot/epoch-033.pkl
2016-11-25 20:05:58 >>> epoch: 33 validation error:		0.352564
2016-11-25 20:06:18 >>>   training loss:	0.622580
2016-11-25 20:06:18 >>> Saving network params to trainNN/out/v10-glorot/epoch-034.pkl
2016-11-25 20:06:19 >>> epoch: 34 validation error:		0.352564
2016-11-25 20:06:36 >>>   training loss:	0.622577
2016-11-25 20:06:36 >>> Saving network params to trainNN/out/v10-glorot/epoch-035.pkl
2016-11-25 20:06:37 >>> epoch: 35 validation error:		0.352576
2016-11-25 20:06:37 >>> Loading old params from trainNN/out/v10-glorot/epoch-029.pkl
2016-11-25 20:06:37 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000008
2016-11-25 20:06:37 >>> Re-compiling train function...
2016-11-25 20:06:54 >>>   training loss:	0.622576
2016-11-25 20:06:54 >>> Saving network params to trainNN/out/v10-glorot/epoch-036.pkl
2016-11-25 20:06:55 >>> epoch: 36 validation error:		0.352579
2016-11-25 20:06:55 >>> Loading old params from trainNN/out/v10-glorot/epoch-029.pkl
2016-11-25 20:06:55 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000004
2016-11-25 20:06:55 >>> Re-compiling train function...
2016-11-25 20:07:17 >>>   training loss:	0.622581
2016-11-25 20:07:17 >>> Saving network params to trainNN/out/v10-glorot/epoch-037.pkl
2016-11-25 20:07:18 >>> epoch: 37 validation error:		0.352576
2016-11-25 20:07:18 >>> Loading old params from trainNN/out/v10-glorot/epoch-029.pkl
2016-11-25 20:07:18 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000002
2016-11-25 20:07:18 >>> Re-compiling train function...
2016-11-25 20:07:35 >>>   training loss:	0.622578
2016-11-25 20:07:35 >>> Saving network params to trainNN/out/v10-glorot/epoch-038.pkl
2016-11-25 20:07:36 >>> epoch: 38 validation error:		0.352570
2016-11-25 20:07:36 >>> Loading old params from trainNN/out/v10-glorot/epoch-029.pkl
2016-11-25 20:07:36 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000001
2016-11-25 20:07:36 >>> Re-compiling train function...
2016-11-25 20:07:54 >>>   training loss:	0.622577
2016-11-25 20:07:54 >>> Saving network params to trainNN/out/v10-glorot/epoch-039.pkl
2016-11-25 20:07:55 >>> epoch: 39 validation error:		0.352553
2016-11-25 20:08:12 >>>   training loss:	0.622580
2016-11-25 20:08:12 >>> Saving network params to trainNN/out/v10-glorot/epoch-040.pkl
2016-11-25 20:08:13 >>> epoch: 40 validation error:		0.352533
2016-11-25 20:08:32 >>>   training loss:	0.622577
2016-11-25 20:08:32 >>> Saving network params to trainNN/out/v10-glorot/epoch-041.pkl
2016-11-25 20:08:33 >>> epoch: 41 validation error:		0.352576
2016-11-25 20:08:33 >>> Loading old params from trainNN/out/v10-glorot/epoch-040.pkl
2016-11-25 20:08:33 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 20:08:33 >>> Re-compiling train function...
2016-11-25 20:08:53 >>>   training loss:	0.622579
2016-11-25 20:08:53 >>> Saving network params to trainNN/out/v10-glorot/epoch-042.pkl
2016-11-25 20:08:55 >>> epoch: 42 validation error:		0.352564
2016-11-25 20:08:55 >>> Loading old params from trainNN/out/v10-glorot/epoch-040.pkl
2016-11-25 20:08:55 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 20:08:55 >>> Re-compiling train function...
2016-11-25 20:09:14 >>>   training loss:	0.622578
2016-11-25 20:09:14 >>> Saving network params to trainNN/out/v10-glorot/epoch-043.pkl
2016-11-25 20:09:15 >>> epoch: 43 validation error:		0.352567
2016-11-25 20:09:15 >>> Loading old params from trainNN/out/v10-glorot/epoch-040.pkl
2016-11-25 20:09:15 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 20:09:15 >>> Re-compiling train function...
2016-11-25 20:09:36 >>>   training loss:	0.622578
2016-11-25 20:09:36 >>> Saving network params to trainNN/out/v10-glorot/epoch-044.pkl
2016-11-25 20:09:37 >>> epoch: 44 validation error:		0.352559
2016-11-25 20:09:37 >>> Loading old params from trainNN/out/v10-glorot/epoch-040.pkl
2016-11-25 20:09:37 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 20:09:37 >>> Re-compiling train function...
2016-11-25 20:09:53 >>>   training loss:	0.622576
2016-11-25 20:09:53 >>> Saving network params to trainNN/out/v10-glorot/epoch-045.pkl
2016-11-25 20:09:55 >>> epoch: 45 validation error:		0.352550
2016-11-25 20:09:55 >>> Loading old params from trainNN/out/v10-glorot/epoch-040.pkl
2016-11-25 20:09:55 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 20:09:55 >>> Re-compiling train function...
2016-11-25 20:10:12 >>>   training loss:	0.622579
2016-11-25 20:10:12 >>> Saving network params to trainNN/out/v10-glorot/epoch-046.pkl
2016-11-25 20:10:13 >>> epoch: 46 validation error:		0.352582
2016-11-25 20:10:13 >>> Loading old params from trainNN/out/v10-glorot/epoch-040.pkl
2016-11-25 20:10:13 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 20:10:13 >>> Re-compiling train function...
2016-11-25 20:10:31 >>>   training loss:	0.622578
2016-11-25 20:10:31 >>> Saving network params to trainNN/out/v10-glorot/epoch-047.pkl
2016-11-25 20:10:32 >>> epoch: 47 validation error:		0.352564
2016-11-25 20:10:32 >>> learning rate below 1e-08, ending
2016-11-25 20:10:32 >>> Wrote output to trainNN/out/v10-glorot/config.json
