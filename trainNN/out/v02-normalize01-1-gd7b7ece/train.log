2016-11-22 01:44:02 >>> loading config from ../extract_pfiles_python/out/v2-normalize01-context40/train-config.json
2016-11-22 01:44:02 >>> loading numpy file ../extract_pfiles_python/out/v2-normalize01-context40/train.npz
2016-11-22 01:44:05 >>> loading numpy file ../extract_pfiles_python/out/v2-normalize01-context40/validate.npz
2016-11-22 01:44:06 >>> Using fuzzy_newbob as schedulung method.
2016-11-22 01:44:06 >>> Training network with 21452 trainable out of 21452 total params.
2016-11-22 01:44:06 >>> Using adadelta with learning_rate=1.000000
2016-11-22 01:44:06 >>> Compiling theano functions...
2016-11-22 01:44:06 >>> Starting training...
2016-11-22 01:44:23 >>>   training loss:	0.118361
2016-11-22 01:44:23 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-0.pkl
2016-11-22 01:44:24 >>> epoch: 0 validation error:		0.045959
2016-11-22 01:44:41 >>>   training loss:	0.105717
2016-11-22 01:44:41 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-1.pkl
2016-11-22 01:44:42 >>> epoch: 1 validation error:		0.042954
2016-11-22 01:44:58 >>>   training loss:	0.104560
2016-11-22 01:44:58 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-2.pkl
2016-11-22 01:44:59 >>> epoch: 2 validation error:		0.042517
2016-11-22 01:45:16 >>>   training loss:	0.103681
2016-11-22 01:45:16 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-3.pkl
2016-11-22 01:45:17 >>> epoch: 3 validation error:		0.043689
2016-11-22 01:45:17 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-2.pkl
2016-11-22 01:45:17 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.500000
2016-11-22 01:45:17 >>> Re-compiling train function...
2016-11-22 01:45:34 >>>   training loss:	0.102607
2016-11-22 01:45:34 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-4.pkl
2016-11-22 01:45:35 >>> epoch: 4 validation error:		0.042408
2016-11-22 01:45:51 >>>   training loss:	0.102344
2016-11-22 01:45:51 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-5.pkl
2016-11-22 01:45:52 >>> epoch: 5 validation error:		0.042773
2016-11-22 01:45:52 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-4.pkl
2016-11-22 01:45:52 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.250000
2016-11-22 01:45:52 >>> Re-compiling train function...
2016-11-22 01:46:09 >>>   training loss:	0.101920
2016-11-22 01:46:09 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-6.pkl
2016-11-22 01:46:10 >>> epoch: 6 validation error:		0.042610
2016-11-22 01:46:10 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-4.pkl
2016-11-22 01:46:10 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.125000
2016-11-22 01:46:10 >>> Re-compiling train function...
2016-11-22 01:46:28 >>>   training loss:	0.101747
2016-11-22 01:46:28 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-7.pkl
2016-11-22 01:46:29 >>> epoch: 7 validation error:		0.042508
2016-11-22 01:46:29 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-4.pkl
2016-11-22 01:46:29 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.062500
2016-11-22 01:46:29 >>> Re-compiling train function...
2016-11-22 01:46:46 >>>   training loss:	0.101679
2016-11-22 01:46:46 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:46:47 >>> epoch: 8 validation error:		0.042369
2016-11-22 01:47:04 >>>   training loss:	0.101638
2016-11-22 01:47:04 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-9.pkl
2016-11-22 01:47:05 >>> epoch: 9 validation error:		0.042459
2016-11-22 01:47:05 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:47:05 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.031250
2016-11-22 01:47:05 >>> Re-compiling train function...
2016-11-22 01:47:22 >>>   training loss:	0.101601
2016-11-22 01:47:22 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-10.pkl
2016-11-22 01:47:23 >>> epoch: 10 validation error:		0.042571
2016-11-22 01:47:23 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:47:23 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.015625
2016-11-22 01:47:23 >>> Re-compiling train function...
2016-11-22 01:47:40 >>>   training loss:	0.101585
2016-11-22 01:47:40 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-11.pkl
2016-11-22 01:47:41 >>> epoch: 11 validation error:		0.042568
2016-11-22 01:47:41 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:47:41 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.007812
2016-11-22 01:47:41 >>> Re-compiling train function...
2016-11-22 01:47:58 >>>   training loss:	0.101576
2016-11-22 01:47:58 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-12.pkl
2016-11-22 01:47:59 >>> epoch: 12 validation error:		0.042535
2016-11-22 01:47:59 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:47:59 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.003906
2016-11-22 01:47:59 >>> Re-compiling train function...
2016-11-22 01:48:16 >>>   training loss:	0.101576
2016-11-22 01:48:16 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-13.pkl
2016-11-22 01:48:17 >>> epoch: 13 validation error:		0.042517
2016-11-22 01:48:17 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:48:17 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.001953
2016-11-22 01:48:17 >>> Re-compiling train function...
2016-11-22 01:48:35 >>>   training loss:	0.101578
2016-11-22 01:48:35 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-14.pkl
2016-11-22 01:48:36 >>> epoch: 14 validation error:		0.042526
2016-11-22 01:48:36 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:48:36 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000977
2016-11-22 01:48:36 >>> Re-compiling train function...
2016-11-22 01:48:53 >>>   training loss:	0.101580
2016-11-22 01:48:53 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-15.pkl
2016-11-22 01:48:54 >>> epoch: 15 validation error:		0.042541
2016-11-22 01:48:54 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:48:54 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000488
2016-11-22 01:48:54 >>> Re-compiling train function...
2016-11-22 01:49:11 >>>   training loss:	0.101586
2016-11-22 01:49:11 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-16.pkl
2016-11-22 01:49:12 >>> epoch: 16 validation error:		0.042526
2016-11-22 01:49:12 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:49:12 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000244
2016-11-22 01:49:12 >>> Re-compiling train function...
2016-11-22 01:49:29 >>>   training loss:	0.101589
2016-11-22 01:49:29 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-17.pkl
2016-11-22 01:49:30 >>> epoch: 17 validation error:		0.042529
2016-11-22 01:49:30 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:49:30 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000122
2016-11-22 01:49:30 >>> Re-compiling train function...
2016-11-22 01:49:47 >>>   training loss:	0.101603
2016-11-22 01:49:47 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-18.pkl
2016-11-22 01:49:48 >>> epoch: 18 validation error:		0.042535
2016-11-22 01:49:48 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:49:48 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000061
2016-11-22 01:49:48 >>> Re-compiling train function...
2016-11-22 01:50:06 >>>   training loss:	0.101623
2016-11-22 01:50:06 >>> Saving network params to out/v2-normalize01-1-gd7b7ece/epoch-19.pkl
2016-11-22 01:50:07 >>> epoch: 19 validation error:		0.042486
2016-11-22 01:50:07 >>> Loading old params from out/v2-normalize01-1-gd7b7ece/epoch-8.pkl
2016-11-22 01:50:07 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000031
2016-11-22 01:50:07 >>> Re-compiling train function...
