2016-11-22 18:53:51 >>> loading config file ../extract_pfiles_python/out/v05-nopitchnormalization-context40/train-config.json
2016-11-22 18:53:51 >>> loading numpy file ../extract_pfiles_python/out/v05-nopitchnormalization-context40/train.npz
2016-11-22 18:53:54 >>> loading numpy file ../extract_pfiles_python/out/v05-nopitchnormalization-context40/validate.npz
2016-11-22 18:53:54 >>> Using fuzzy_newbob as schedulung method.
2016-11-22 18:53:54 >>> Training network with 21452 trainable out of 21452 total params.
2016-11-22 18:53:54 >>> Using adadelta with learning_rate=1.000000
2016-11-22 18:53:54 >>> Compiling theano functions...
2016-11-22 18:53:55 >>> Starting training...
2016-11-22 18:54:12 >>>   training loss:	0.671780
2016-11-22 18:54:12 >>> Saving network params to out/v06-fuzzynewbob/epoch-000.pkl
2016-11-22 18:54:13 >>> epoch: 0 validation error:		0.383952
2016-11-22 18:54:29 >>>   training loss:	0.593315
2016-11-22 18:54:29 >>> Saving network params to out/v06-fuzzynewbob/epoch-001.pkl
2016-11-22 18:54:30 >>> epoch: 1 validation error:		0.429094
2016-11-22 18:54:30 >>> Loading old params from out/v06-fuzzynewbob/epoch-000.pkl
2016-11-22 18:54:30 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.500000
2016-11-22 18:54:30 >>> Re-compiling train function...
2016-11-22 18:54:47 >>>   training loss:	0.516201
2016-11-22 18:54:47 >>> Saving network params to out/v06-fuzzynewbob/epoch-002.pkl
2016-11-22 18:54:48 >>> epoch: 2 validation error:		0.131067
2016-11-22 18:55:04 >>>   training loss:	0.414854
2016-11-22 18:55:04 >>> Saving network params to out/v06-fuzzynewbob/epoch-003.pkl
2016-11-22 18:55:05 >>> epoch: 3 validation error:		0.087698
2016-11-22 18:55:22 >>>   training loss:	0.346121
2016-11-22 18:55:22 >>> Saving network params to out/v06-fuzzynewbob/epoch-004.pkl
2016-11-22 18:55:23 >>> epoch: 4 validation error:		0.122085
2016-11-22 18:55:23 >>> Loading old params from out/v06-fuzzynewbob/epoch-003.pkl
2016-11-22 18:55:23 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.250000
2016-11-22 18:55:23 >>> Re-compiling train function...
2016-11-22 18:55:40 >>>   training loss:	0.245827
2016-11-22 18:55:40 >>> Saving network params to out/v06-fuzzynewbob/epoch-005.pkl
2016-11-22 18:55:41 >>> epoch: 5 validation error:		0.135293
2016-11-22 18:55:41 >>> Loading old params from out/v06-fuzzynewbob/epoch-003.pkl
2016-11-22 18:55:41 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.125000
2016-11-22 18:55:41 >>> Re-compiling train function...
2016-11-22 18:55:58 >>>   training loss:	0.224702
2016-11-22 18:55:58 >>> Saving network params to out/v06-fuzzynewbob/epoch-006.pkl
2016-11-22 18:55:59 >>> epoch: 6 validation error:		0.079086
2016-11-22 18:56:15 >>>   training loss:	0.213082
2016-11-22 18:56:15 >>> Saving network params to out/v06-fuzzynewbob/epoch-007.pkl
2016-11-22 18:56:16 >>> epoch: 7 validation error:		0.088228
2016-11-22 18:56:16 >>> Loading old params from out/v06-fuzzynewbob/epoch-006.pkl
2016-11-22 18:56:16 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.062500
2016-11-22 18:56:16 >>> Re-compiling train function...
2016-11-22 18:56:33 >>>   training loss:	0.208904
2016-11-22 18:56:33 >>> Saving network params to out/v06-fuzzynewbob/epoch-008.pkl
2016-11-22 18:56:34 >>> epoch: 8 validation error:		0.073815
2016-11-22 18:56:50 >>>   training loss:	0.203189
2016-11-22 18:56:50 >>> Saving network params to out/v06-fuzzynewbob/epoch-009.pkl
2016-11-22 18:56:51 >>> epoch: 9 validation error:		0.071801
2016-11-22 18:57:08 >>>   training loss:	0.198138
2016-11-22 18:57:08 >>> Saving network params to out/v06-fuzzynewbob/epoch-010.pkl
2016-11-22 18:57:09 >>> epoch: 10 validation error:		0.072540
2016-11-22 18:57:09 >>> Loading old params from out/v06-fuzzynewbob/epoch-009.pkl
2016-11-22 18:57:09 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.031250
2016-11-22 18:57:09 >>> Re-compiling train function...
2016-11-22 18:57:27 >>>   training loss:	0.197276
2016-11-22 18:57:27 >>> Saving network params to out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 18:57:28 >>> epoch: 11 validation error:		0.069854
2016-11-22 18:57:46 >>>   training loss:	0.194645
2016-11-22 18:57:46 >>> Saving network params to out/v06-fuzzynewbob/epoch-012.pkl
2016-11-22 18:57:46 >>> epoch: 12 validation error:		0.071162
2016-11-22 18:57:46 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 18:57:47 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.015625
2016-11-22 18:57:47 >>> Re-compiling train function...
2016-11-22 18:58:04 >>>   training loss:	0.194499
2016-11-22 18:58:04 >>> Saving network params to out/v06-fuzzynewbob/epoch-013.pkl
2016-11-22 18:58:05 >>> epoch: 13 validation error:		0.069909
2016-11-22 18:58:05 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 18:58:05 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.007812
2016-11-22 18:58:05 >>> Re-compiling train function...
2016-11-22 18:58:21 >>>   training loss:	0.194503
2016-11-22 18:58:21 >>> Saving network params to out/v06-fuzzynewbob/epoch-014.pkl
2016-11-22 18:58:22 >>> epoch: 14 validation error:		0.070035
2016-11-22 18:58:22 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 18:58:22 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.003906
2016-11-22 18:58:22 >>> Re-compiling train function...
2016-11-22 18:58:40 >>>   training loss:	0.194501
2016-11-22 18:58:40 >>> Saving network params to out/v06-fuzzynewbob/epoch-015.pkl
2016-11-22 18:58:41 >>> epoch: 15 validation error:		0.070912
2016-11-22 18:58:41 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 18:58:41 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.001953
2016-11-22 18:58:41 >>> Re-compiling train function...
2016-11-22 18:58:58 >>>   training loss:	0.194511
2016-11-22 18:58:58 >>> Saving network params to out/v06-fuzzynewbob/epoch-016.pkl
2016-11-22 18:58:59 >>> epoch: 16 validation error:		0.070297
2016-11-22 18:58:59 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 18:58:59 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000977
2016-11-22 18:58:59 >>> Re-compiling train function...
2016-11-22 18:59:16 >>>   training loss:	0.194516
2016-11-22 18:59:16 >>> Saving network params to out/v06-fuzzynewbob/epoch-017.pkl
2016-11-22 18:59:17 >>> epoch: 17 validation error:		0.070716
2016-11-22 18:59:17 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 18:59:17 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000488
2016-11-22 18:59:17 >>> Re-compiling train function...
2016-11-22 18:59:36 >>>   training loss:	0.194529
2016-11-22 18:59:36 >>> Saving network params to out/v06-fuzzynewbob/epoch-018.pkl
2016-11-22 18:59:37 >>> epoch: 18 validation error:		0.070536
2016-11-22 18:59:37 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 18:59:37 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000244
2016-11-22 18:59:37 >>> Re-compiling train function...
2016-11-22 18:59:56 >>>   training loss:	0.194555
2016-11-22 18:59:56 >>> Saving network params to out/v06-fuzzynewbob/epoch-019.pkl
2016-11-22 18:59:57 >>> epoch: 19 validation error:		0.070924
2016-11-22 18:59:57 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 18:59:57 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000122
2016-11-22 18:59:57 >>> Re-compiling train function...
2016-11-22 19:00:17 >>>   training loss:	0.194581
2016-11-22 19:00:17 >>> Saving network params to out/v06-fuzzynewbob/epoch-020.pkl
2016-11-22 19:00:18 >>> epoch: 20 validation error:		0.070870
2016-11-22 19:00:18 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:00:18 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000061
2016-11-22 19:00:18 >>> Re-compiling train function...
2016-11-22 19:00:35 >>>   training loss:	0.194617
2016-11-22 19:00:35 >>> Saving network params to out/v06-fuzzynewbob/epoch-021.pkl
2016-11-22 19:00:36 >>> epoch: 21 validation error:		0.070822
2016-11-22 19:00:36 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:00:37 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000031
2016-11-22 19:00:37 >>> Re-compiling train function...
2016-11-22 19:00:55 >>>   training loss:	0.194649
2016-11-22 19:00:55 >>> Saving network params to out/v06-fuzzynewbob/epoch-022.pkl
2016-11-22 19:00:56 >>> epoch: 22 validation error:		0.070840
2016-11-22 19:00:56 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:00:56 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000015
2016-11-22 19:00:56 >>> Re-compiling train function...
2016-11-22 19:01:15 >>>   training loss:	0.194716
2016-11-22 19:01:15 >>> Saving network params to out/v06-fuzzynewbob/epoch-023.pkl
2016-11-22 19:01:16 >>> epoch: 23 validation error:		0.070831
2016-11-22 19:01:16 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:01:16 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000008
2016-11-22 19:01:16 >>> Re-compiling train function...
2016-11-22 19:01:34 >>>   training loss:	0.194834
2016-11-22 19:01:34 >>> Saving network params to out/v06-fuzzynewbob/epoch-024.pkl
2016-11-22 19:01:35 >>> epoch: 24 validation error:		0.070692
2016-11-22 19:01:35 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:01:35 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000004
2016-11-22 19:01:35 >>> Re-compiling train function...
2016-11-22 19:01:52 >>>   training loss:	0.194999
2016-11-22 19:01:52 >>> Saving network params to out/v06-fuzzynewbob/epoch-025.pkl
2016-11-22 19:01:53 >>> epoch: 25 validation error:		0.070382
2016-11-22 19:01:53 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:01:53 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000002
2016-11-22 19:01:53 >>> Re-compiling train function...
2016-11-22 19:02:10 >>>   training loss:	0.195198
2016-11-22 19:02:10 >>> Saving network params to out/v06-fuzzynewbob/epoch-026.pkl
2016-11-22 19:02:11 >>> epoch: 26 validation error:		0.070234
2016-11-22 19:02:11 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:02:11 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000001
2016-11-22 19:02:11 >>> Re-compiling train function...
2016-11-22 19:02:28 >>>   training loss:	0.195349
2016-11-22 19:02:28 >>> Saving network params to out/v06-fuzzynewbob/epoch-027.pkl
2016-11-22 19:02:29 >>> epoch: 27 validation error:		0.070002
2016-11-22 19:02:29 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:02:29 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-22 19:02:29 >>> Re-compiling train function...
2016-11-22 19:02:46 >>>   training loss:	0.195431
2016-11-22 19:02:46 >>> Saving network params to out/v06-fuzzynewbob/epoch-028.pkl
2016-11-22 19:02:47 >>> epoch: 28 validation error:		0.069912
2016-11-22 19:02:47 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:02:47 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-22 19:02:47 >>> Re-compiling train function...
2016-11-22 19:03:04 >>>   training loss:	0.195484
2016-11-22 19:03:04 >>> Saving network params to out/v06-fuzzynewbob/epoch-029.pkl
2016-11-22 19:03:05 >>> epoch: 29 validation error:		0.069906
2016-11-22 19:03:05 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:03:05 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-22 19:03:05 >>> Re-compiling train function...
2016-11-22 19:03:23 >>>   training loss:	0.195515
2016-11-22 19:03:23 >>> Saving network params to out/v06-fuzzynewbob/epoch-030.pkl
2016-11-22 19:03:24 >>> epoch: 30 validation error:		0.069860
2016-11-22 19:03:24 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:03:24 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-22 19:03:24 >>> Re-compiling train function...
2016-11-22 19:03:41 >>>   training loss:	0.195530
2016-11-22 19:03:41 >>> Saving network params to out/v06-fuzzynewbob/epoch-031.pkl
2016-11-22 19:03:42 >>> epoch: 31 validation error:		0.069857
2016-11-22 19:03:42 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:03:42 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-22 19:03:42 >>> Re-compiling train function...
2016-11-22 19:04:00 >>>   training loss:	0.195532
2016-11-22 19:04:00 >>> Saving network params to out/v06-fuzzynewbob/epoch-032.pkl
2016-11-22 19:04:01 >>> epoch: 32 validation error:		0.069857
2016-11-22 19:04:01 >>> Loading old params from out/v06-fuzzynewbob/epoch-011.pkl
2016-11-22 19:04:01 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-22 19:04:01 >>> Re-compiling train function...
2016-11-22 19:04:18 >>>   training loss:	0.195538
2016-11-22 19:04:18 >>> Saving network params to out/v06-fuzzynewbob/epoch-033.pkl
2016-11-22 19:04:19 >>> epoch: 33 validation error:		0.069851
2016-11-22 19:04:36 >>>   training loss:	0.195533
2016-11-22 19:04:36 >>> Saving network params to out/v06-fuzzynewbob/epoch-034.pkl
2016-11-22 19:04:37 >>> epoch: 34 validation error:		0.069857
2016-11-22 19:04:37 >>> learning rate below 1e-08, ending
