2016-11-25 22:41:09 >>> version=v14-largebatches
2016-11-25 22:41:09 >>> loading config file extract_pfiles_python/out/v09-without-frame-limiting-context40/config.json
2016-11-25 22:41:09 >>> loading numpy file extract_pfiles_python/out/v09-without-frame-limiting-context40/train.npz
2016-11-25 22:41:13 >>> loading numpy file extract_pfiles_python/out/v09-without-frame-limiting-context40/validate.npz
2016-11-25 22:41:13 >>> Using fuzzy_newbob as schedulung method.
2016-11-25 22:41:13 >>> Training network with 21452 trainable out of 21452 total params.
2016-11-25 22:41:13 >>> Using adadelta with learning_rate=1.000000
2016-11-25 22:41:13 >>> Compiling theano functions...
2016-11-25 22:41:14 >>> Starting training...
2016-11-25 22:41:35 >>>   training loss:	0.665430
2016-11-25 22:41:35 >>> Saving network params to trainNN/out/v14-largebatches/epoch-000.pkl
2016-11-25 22:41:35 >>> epoch: 0 validation error:		0.399309
2016-11-25 22:41:56 >>>   training loss:	0.655505
2016-11-25 22:41:56 >>> Saving network params to trainNN/out/v14-largebatches/epoch-001.pkl
2016-11-25 22:41:57 >>> epoch: 1 validation error:		0.387550
2016-11-25 22:42:18 >>>   training loss:	0.649759
2016-11-25 22:42:18 >>> Saving network params to trainNN/out/v14-largebatches/epoch-002.pkl
2016-11-25 22:42:19 >>> epoch: 2 validation error:		0.381990
2016-11-25 22:42:40 >>>   training loss:	0.646515
2016-11-25 22:42:40 >>> Saving network params to trainNN/out/v14-largebatches/epoch-003.pkl
2016-11-25 22:42:41 >>> epoch: 3 validation error:		0.378389
2016-11-25 22:43:01 >>>   training loss:	0.644211
2016-11-25 22:43:01 >>> Saving network params to trainNN/out/v14-largebatches/epoch-004.pkl
2016-11-25 22:43:02 >>> epoch: 4 validation error:		0.376378
2016-11-25 22:43:23 >>>   training loss:	0.642296
2016-11-25 22:43:23 >>> Saving network params to trainNN/out/v14-largebatches/epoch-005.pkl
2016-11-25 22:43:24 >>> epoch: 5 validation error:		0.372001
2016-11-25 22:43:44 >>>   training loss:	0.639971
2016-11-25 22:43:44 >>> Saving network params to trainNN/out/v14-largebatches/epoch-006.pkl
2016-11-25 22:43:45 >>> epoch: 6 validation error:		0.368501
2016-11-25 22:44:06 >>>   training loss:	0.637977
2016-11-25 22:44:06 >>> Saving network params to trainNN/out/v14-largebatches/epoch-007.pkl
2016-11-25 22:44:06 >>> epoch: 7 validation error:		0.368815
2016-11-25 22:44:06 >>> Loading old params from trainNN/out/v14-largebatches/epoch-006.pkl
2016-11-25 22:44:06 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.500000
2016-11-25 22:44:06 >>> Re-compiling train function...
2016-11-25 22:44:28 >>>   training loss:	0.637024
2016-11-25 22:44:28 >>> Saving network params to trainNN/out/v14-largebatches/epoch-008.pkl
2016-11-25 22:44:29 >>> epoch: 8 validation error:		0.366400
2016-11-25 22:44:50 >>>   training loss:	0.636039
2016-11-25 22:44:50 >>> Saving network params to trainNN/out/v14-largebatches/epoch-009.pkl
2016-11-25 22:44:51 >>> epoch: 9 validation error:		0.365013
2016-11-25 22:45:10 >>>   training loss:	0.635288
2016-11-25 22:45:10 >>> Saving network params to trainNN/out/v14-largebatches/epoch-010.pkl
2016-11-25 22:45:11 >>> epoch: 10 validation error:		0.363674
2016-11-25 22:45:32 >>>   training loss:	0.634617
2016-11-25 22:45:32 >>> Saving network params to trainNN/out/v14-largebatches/epoch-011.pkl
2016-11-25 22:45:33 >>> epoch: 11 validation error:		0.364900
2016-11-25 22:45:33 >>> Loading old params from trainNN/out/v14-largebatches/epoch-010.pkl
2016-11-25 22:45:33 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.250000
2016-11-25 22:45:33 >>> Re-compiling train function...
2016-11-25 22:45:55 >>>   training loss:	0.634075
2016-11-25 22:45:55 >>> Saving network params to trainNN/out/v14-largebatches/epoch-012.pkl
2016-11-25 22:45:56 >>> epoch: 12 validation error:		0.363662
2016-11-25 22:46:17 >>>   training loss:	0.633689
2016-11-25 22:46:17 >>> Saving network params to trainNN/out/v14-largebatches/epoch-013.pkl
2016-11-25 22:46:18 >>> epoch: 13 validation error:		0.364223
2016-11-25 22:46:18 >>> Loading old params from trainNN/out/v14-largebatches/epoch-012.pkl
2016-11-25 22:46:18 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.125000
2016-11-25 22:46:18 >>> Re-compiling train function...
2016-11-25 22:46:39 >>>   training loss:	0.633469
2016-11-25 22:46:39 >>> Saving network params to trainNN/out/v14-largebatches/epoch-014.pkl
2016-11-25 22:46:40 >>> epoch: 14 validation error:		0.362023
2016-11-25 22:47:01 >>>   training loss:	0.633271
2016-11-25 22:47:01 >>> Saving network params to trainNN/out/v14-largebatches/epoch-015.pkl
2016-11-25 22:47:02 >>> epoch: 15 validation error:		0.361866
2016-11-25 22:47:23 >>>   training loss:	0.633078
2016-11-25 22:47:23 >>> Saving network params to trainNN/out/v14-largebatches/epoch-016.pkl
2016-11-25 22:47:24 >>> epoch: 16 validation error:		0.362151
2016-11-25 22:47:24 >>> Loading old params from trainNN/out/v14-largebatches/epoch-015.pkl
2016-11-25 22:47:24 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.062500
2016-11-25 22:47:24 >>> Re-compiling train function...
2016-11-25 22:47:46 >>>   training loss:	0.632971
2016-11-25 22:47:46 >>> Saving network params to trainNN/out/v14-largebatches/epoch-017.pkl
2016-11-25 22:47:47 >>> epoch: 17 validation error:		0.362488
2016-11-25 22:47:47 >>> Loading old params from trainNN/out/v14-largebatches/epoch-015.pkl
2016-11-25 22:47:47 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.031250
2016-11-25 22:47:47 >>> Re-compiling train function...
2016-11-25 22:48:19 >>>   training loss:	0.632915
2016-11-25 22:48:19 >>> Saving network params to trainNN/out/v14-largebatches/epoch-018.pkl
2016-11-25 22:48:20 >>> epoch: 18 validation error:		0.362311
2016-11-25 22:48:20 >>> Loading old params from trainNN/out/v14-largebatches/epoch-015.pkl
2016-11-25 22:48:20 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.015625
2016-11-25 22:48:20 >>> Re-compiling train function...
2016-11-25 22:48:52 >>>   training loss:	0.632904
2016-11-25 22:48:52 >>> Saving network params to trainNN/out/v14-largebatches/epoch-019.pkl
2016-11-25 22:48:53 >>> epoch: 19 validation error:		0.361877
2016-11-25 22:48:53 >>> Loading old params from trainNN/out/v14-largebatches/epoch-015.pkl
2016-11-25 22:48:53 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.007812
2016-11-25 22:48:53 >>> Re-compiling train function...
2016-11-25 22:49:20 >>>   training loss:	0.632893
2016-11-25 22:49:20 >>> Saving network params to trainNN/out/v14-largebatches/epoch-020.pkl
2016-11-25 22:49:21 >>> epoch: 20 validation error:		0.361930
2016-11-25 22:49:21 >>> Loading old params from trainNN/out/v14-largebatches/epoch-015.pkl
2016-11-25 22:49:21 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.003906
2016-11-25 22:49:21 >>> Re-compiling train function...
2016-11-25 22:49:42 >>>   training loss:	0.632899
2016-11-25 22:49:42 >>> Saving network params to trainNN/out/v14-largebatches/epoch-021.pkl
2016-11-25 22:49:43 >>> epoch: 21 validation error:		0.362017
2016-11-25 22:49:43 >>> Loading old params from trainNN/out/v14-largebatches/epoch-015.pkl
2016-11-25 22:49:43 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.001953
2016-11-25 22:49:43 >>> Re-compiling train function...
2016-11-25 22:50:05 >>>   training loss:	0.632914
2016-11-25 22:50:05 >>> Saving network params to trainNN/out/v14-largebatches/epoch-022.pkl
2016-11-25 22:50:06 >>> epoch: 22 validation error:		0.361947
2016-11-25 22:50:06 >>> Loading old params from trainNN/out/v14-largebatches/epoch-015.pkl
2016-11-25 22:50:06 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000977
2016-11-25 22:50:06 >>> Re-compiling train function...
2016-11-25 22:50:28 >>>   training loss:	0.632930
2016-11-25 22:50:28 >>> Saving network params to trainNN/out/v14-largebatches/epoch-023.pkl
2016-11-25 22:50:29 >>> epoch: 23 validation error:		0.361764
2016-11-25 22:50:50 >>>   training loss:	0.632891
2016-11-25 22:50:50 >>> Saving network params to trainNN/out/v14-largebatches/epoch-024.pkl
2016-11-25 22:50:51 >>> epoch: 24 validation error:		0.361889
2016-11-25 22:50:51 >>> Loading old params from trainNN/out/v14-largebatches/epoch-023.pkl
2016-11-25 22:50:51 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000488
2016-11-25 22:50:51 >>> Re-compiling train function...
2016-11-25 22:51:12 >>>   training loss:	0.632892
2016-11-25 22:51:12 >>> Saving network params to trainNN/out/v14-largebatches/epoch-025.pkl
2016-11-25 22:51:13 >>> epoch: 25 validation error:		0.361895
2016-11-25 22:51:13 >>> Loading old params from trainNN/out/v14-largebatches/epoch-023.pkl
2016-11-25 22:51:13 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000244
2016-11-25 22:51:13 >>> Re-compiling train function...
2016-11-25 22:51:35 >>>   training loss:	0.632895
2016-11-25 22:51:35 >>> Saving network params to trainNN/out/v14-largebatches/epoch-026.pkl
2016-11-25 22:51:36 >>> epoch: 26 validation error:		0.361892
2016-11-25 22:51:36 >>> Loading old params from trainNN/out/v14-largebatches/epoch-023.pkl
2016-11-25 22:51:36 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000122
2016-11-25 22:51:36 >>> Re-compiling train function...
2016-11-25 22:51:57 >>>   training loss:	0.632897
2016-11-25 22:51:57 >>> Saving network params to trainNN/out/v14-largebatches/epoch-027.pkl
2016-11-25 22:51:57 >>> epoch: 27 validation error:		0.361811
2016-11-25 22:51:57 >>> Loading old params from trainNN/out/v14-largebatches/epoch-023.pkl
2016-11-25 22:51:58 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000061
2016-11-25 22:51:58 >>> Re-compiling train function...
2016-11-25 22:52:19 >>>   training loss:	0.632899
2016-11-25 22:52:19 >>> Saving network params to trainNN/out/v14-largebatches/epoch-028.pkl
2016-11-25 22:52:20 >>> epoch: 28 validation error:		0.361840
2016-11-25 22:52:20 >>> Loading old params from trainNN/out/v14-largebatches/epoch-023.pkl
2016-11-25 22:52:20 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000031
2016-11-25 22:52:20 >>> Re-compiling train function...
2016-11-25 22:52:41 >>>   training loss:	0.632899
2016-11-25 22:52:41 >>> Saving network params to trainNN/out/v14-largebatches/epoch-029.pkl
2016-11-25 22:52:42 >>> epoch: 29 validation error:		0.361793
2016-11-25 22:52:42 >>> Loading old params from trainNN/out/v14-largebatches/epoch-023.pkl
2016-11-25 22:52:42 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000015
2016-11-25 22:52:42 >>> Re-compiling train function...
2016-11-25 22:53:11 >>>   training loss:	0.632898
2016-11-25 22:53:11 >>> Saving network params to trainNN/out/v14-largebatches/epoch-030.pkl
2016-11-25 22:53:12 >>> epoch: 30 validation error:		0.361750
2016-11-25 22:53:42 >>>   training loss:	0.632897
2016-11-25 22:53:42 >>> Saving network params to trainNN/out/v14-largebatches/epoch-031.pkl
2016-11-25 22:53:43 >>> epoch: 31 validation error:		0.361805
2016-11-25 22:53:43 >>> Loading old params from trainNN/out/v14-largebatches/epoch-030.pkl
2016-11-25 22:53:43 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000008
2016-11-25 22:53:43 >>> Re-compiling train function...
2016-11-25 22:54:13 >>>   training loss:	0.632899
2016-11-25 22:54:13 >>> Saving network params to trainNN/out/v14-largebatches/epoch-032.pkl
2016-11-25 22:54:14 >>> epoch: 32 validation error:		0.361779
2016-11-25 22:54:14 >>> Loading old params from trainNN/out/v14-largebatches/epoch-030.pkl
2016-11-25 22:54:14 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000004
2016-11-25 22:54:14 >>> Re-compiling train function...
2016-11-25 22:54:44 >>>   training loss:	0.632892
2016-11-25 22:54:44 >>> Saving network params to trainNN/out/v14-largebatches/epoch-033.pkl
2016-11-25 22:54:45 >>> epoch: 33 validation error:		0.361782
2016-11-25 22:54:45 >>> Loading old params from trainNN/out/v14-largebatches/epoch-030.pkl
2016-11-25 22:54:45 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000002
2016-11-25 22:54:45 >>> Re-compiling train function...
2016-11-25 22:55:15 >>>   training loss:	0.632899
2016-11-25 22:55:15 >>> Saving network params to trainNN/out/v14-largebatches/epoch-034.pkl
2016-11-25 22:55:16 >>> epoch: 34 validation error:		0.361790
2016-11-25 22:55:16 >>> learning rate below 1e-06, ending
2016-11-25 22:55:16 >>> Wrote output to trainNN/out/v14-largebatches/config.json
