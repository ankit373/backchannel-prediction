2017-02-05 21:59:18,814 DEBUG    version=v049-finunified-4-g199b048:lstm-best-layers-70-50-35
2017-02-05 21:59:18,816 INFO     Open DBase(data/db/all240302-spk.dat, data/db/all240302-spk.idx, mode='r')
2017-02-05 21:59:30,704 DEBUG    loading cached extracted data from data/cache/extract-7a00b23850576d60137fa78432d8dfda09ea0f343092bd09848f21113b1acfd0.pickle
2017-02-05 21:59:32,095 DEBUG    set input dim to 9
2017-02-05 21:59:32,097 DEBUG    input dim = 9
2017-02-05 21:59:32,116 DEBUG    shuffling batches
2017-02-05 21:59:37,479 DEBUG    shuffling done
2017-02-05 21:59:37,479 DEBUG    loading data into ram
2017-02-05 21:59:37,620 DEBUG    loading data took 0.141s (cpu: 0.140s)
2017-02-05 21:59:37,964 DEBUG    set input dim to 9
2017-02-05 21:59:37,964 DEBUG    input dim = 9
2017-02-05 21:59:37,997 DEBUG    shuffling batches
2017-02-05 21:59:38,520 DEBUG    shuffling done
2017-02-05 21:59:38,520 DEBUG    loading data into ram
2017-02-05 21:59:38,535 DEBUG    loading data took 0.015s (cpu: 0.015s)
2017-02-05 21:59:38,728 DEBUG    Applying L2 regularization with 0.000100
2017-02-05 21:59:38,930 INFO     Training network with 59177 trainable out of 59487 total params.
2017-02-05 21:59:40,727 DEBUG    Using adam with learning_rate=0.001000
2017-02-05 22:00:44,618 DEBUG    Starting training...
2017-02-05 22:03:15,323 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-000.pkl
2017-02-05 22:03:18,580 INFO     epoch: 0 took 153.962s (0.001s in batching)
training loss:	0.643889
validation loss:	0.606528
validation error:	0.329300
2017-02-05 22:03:18,583 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:05:51,746 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-001.pkl
2017-02-05 22:05:54,918 INFO     epoch: 1 took 156.338s (0.002s in batching)
training loss:	0.600313
validation loss:	0.580064
validation error:	0.303500
2017-02-05 22:05:54,921 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:08:27,739 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-002.pkl
2017-02-05 22:08:31,019 INFO     epoch: 2 took 156.101s (0.001s in batching)
training loss:	0.590537
validation loss:	0.570016
validation error:	0.296500
2017-02-05 22:08:31,021 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:11:03,580 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-003.pkl
2017-02-05 22:11:06,833 INFO     epoch: 3 took 155.814s (0.001s in batching)
training loss:	0.583798
validation loss:	0.562299
validation error:	0.289400
2017-02-05 22:11:06,835 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:13:40,286 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-004.pkl
2017-02-05 22:13:43,510 INFO     epoch: 4 took 156.677s (0.001s in batching)
training loss:	0.579764
validation loss:	0.561748
validation error:	0.286600
2017-02-05 22:13:43,513 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:16:16,931 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-005.pkl
2017-02-05 22:16:20,225 INFO     epoch: 5 took 156.715s (0.001s in batching)
training loss:	0.576838
validation loss:	0.561660
validation error:	0.285800
2017-02-05 22:16:20,228 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:18:53,120 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-006.pkl
2017-02-05 22:18:56,383 INFO     epoch: 6 took 156.159s (0.001s in batching)
training loss:	0.572982
validation loss:	0.560021
validation error:	0.281800
2017-02-05 22:18:56,386 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:21:29,397 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-007.pkl
2017-02-05 22:21:32,575 INFO     epoch: 7 took 156.191s (0.001s in batching)
training loss:	0.569318
validation loss:	0.555545
validation error:	0.280500
2017-02-05 22:21:32,578 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:24:05,703 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-008.pkl
2017-02-05 22:24:08,937 INFO     epoch: 8 took 156.362s (0.001s in batching)
training loss:	0.564998
validation loss:	0.550040
validation error:	0.273600
2017-02-05 22:24:08,939 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:26:41,645 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-009.pkl
2017-02-05 22:26:44,929 INFO     epoch: 9 took 155.992s (0.001s in batching)
training loss:	0.562314
validation loss:	0.551566
validation error:	0.277400
2017-02-05 22:26:44,932 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:29:17,636 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-010.pkl
2017-02-05 22:29:20,845 INFO     epoch: 10 took 155.916s (0.002s in batching)
training loss:	0.559720
validation loss:	0.547544
validation error:	0.275700
2017-02-05 22:29:20,848 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:31:54,068 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-011.pkl
2017-02-05 22:31:57,319 INFO     epoch: 11 took 156.475s (0.001s in batching)
training loss:	0.558242
validation loss:	0.547172
validation error:	0.269500
2017-02-05 22:31:57,323 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:34:30,386 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-012.pkl
2017-02-05 22:34:33,670 INFO     epoch: 12 took 156.351s (0.002s in batching)
training loss:	0.556971
validation loss:	0.545984
validation error:	0.271900
2017-02-05 22:34:33,673 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:37:03,424 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-013.pkl
2017-02-05 22:37:06,200 INFO     epoch: 13 took 152.530s (0.002s in batching)
training loss:	0.555547
validation loss:	0.543536
validation error:	0.271400
2017-02-05 22:37:06,203 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:39:34,008 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-014.pkl
2017-02-05 22:39:37,217 INFO     epoch: 14 took 151.017s (0.001s in batching)
training loss:	0.554523
validation loss:	0.543345
validation error:	0.270900
2017-02-05 22:39:37,220 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:42:09,164 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-015.pkl
2017-02-05 22:42:12,369 INFO     epoch: 15 took 155.152s (0.001s in batching)
training loss:	0.553139
validation loss:	0.539604
validation error:	0.270200
2017-02-05 22:42:12,372 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:44:44,918 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-016.pkl
2017-02-05 22:44:48,120 INFO     epoch: 16 took 155.751s (0.002s in batching)
training loss:	0.551674
validation loss:	0.550418
validation error:	0.279100
2017-02-05 22:44:48,123 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:47:20,211 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-017.pkl
2017-02-05 22:47:23,440 INFO     epoch: 17 took 155.320s (0.001s in batching)
training loss:	0.552427
validation loss:	0.541850
validation error:	0.268300
2017-02-05 22:47:23,444 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:49:54,431 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-018.pkl
2017-02-05 22:49:57,662 INFO     epoch: 18 took 154.222s (0.001s in batching)
training loss:	0.550600
validation loss:	0.536046
validation error:	0.265500
2017-02-05 22:49:57,666 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:52:30,328 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-019.pkl
2017-02-05 22:52:33,565 INFO     epoch: 19 took 155.903s (0.001s in batching)
training loss:	0.549316
validation loss:	0.540922
validation error:	0.267900
2017-02-05 22:52:33,569 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:55:06,282 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-020.pkl
2017-02-05 22:55:09,572 INFO     epoch: 20 took 156.007s (0.001s in batching)
training loss:	0.549285
validation loss:	0.535715
validation error:	0.266800
2017-02-05 22:55:09,576 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 22:57:42,083 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-021.pkl
2017-02-05 22:57:45,332 INFO     epoch: 21 took 155.760s (0.001s in batching)
training loss:	0.547516
validation loss:	0.539328
validation error:	0.269300
2017-02-05 22:57:45,336 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:00:17,430 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-022.pkl
2017-02-05 23:00:20,655 INFO     epoch: 22 took 155.323s (0.002s in batching)
training loss:	0.547042
validation loss:	0.531717
validation error:	0.263100
2017-02-05 23:00:20,659 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:02:53,046 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-023.pkl
2017-02-05 23:02:56,237 INFO     epoch: 23 took 155.582s (0.001s in batching)
training loss:	0.546390
validation loss:	0.535950
validation error:	0.265600
2017-02-05 23:02:56,240 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:05:29,110 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-024.pkl
2017-02-05 23:05:32,350 INFO     epoch: 24 took 156.113s (0.001s in batching)
training loss:	0.545491
validation loss:	0.532030
validation error:	0.261200
2017-02-05 23:05:32,354 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:08:05,328 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-025.pkl
2017-02-05 23:08:08,607 INFO     epoch: 25 took 156.257s (0.001s in batching)
training loss:	0.544492
validation loss:	0.535759
validation error:	0.265800
2017-02-05 23:08:08,612 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:10:41,591 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-026.pkl
2017-02-05 23:10:44,843 INFO     epoch: 26 took 156.236s (0.001s in batching)
training loss:	0.543326
validation loss:	0.535051
validation error:	0.264700
2017-02-05 23:10:44,848 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:13:17,368 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-027.pkl
2017-02-05 23:13:20,632 INFO     epoch: 27 took 155.788s (0.001s in batching)
training loss:	0.542546
validation loss:	0.529455
validation error:	0.257900
2017-02-05 23:13:20,635 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:15:53,331 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-028.pkl
2017-02-05 23:15:56,563 INFO     epoch: 28 took 155.932s (0.001s in batching)
training loss:	0.542205
validation loss:	0.528616
validation error:	0.262400
2017-02-05 23:15:56,567 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:18:29,015 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-029.pkl
2017-02-05 23:18:32,279 INFO     epoch: 29 took 155.716s (0.001s in batching)
training loss:	0.541848
validation loss:	0.540194
validation error:	0.268300
2017-02-05 23:18:32,284 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:21:04,857 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-030.pkl
2017-02-05 23:21:08,108 INFO     epoch: 30 took 155.829s (0.002s in batching)
training loss:	0.540678
validation loss:	0.530659
validation error:	0.263400
2017-02-05 23:21:08,112 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:23:41,179 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-031.pkl
2017-02-05 23:23:44,401 INFO     epoch: 31 took 156.293s (0.001s in batching)
training loss:	0.539344
validation loss:	0.528775
validation error:	0.262100
2017-02-05 23:23:44,405 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:26:16,405 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-032.pkl
2017-02-05 23:26:19,648 INFO     epoch: 32 took 155.247s (0.001s in batching)
training loss:	0.538723
validation loss:	0.527732
validation error:	0.262600
2017-02-05 23:26:19,652 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:28:52,247 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-033.pkl
2017-02-05 23:28:55,491 INFO     epoch: 33 took 155.844s (0.001s in batching)
training loss:	0.538429
validation loss:	0.527628
validation error:	0.261600
2017-02-05 23:28:55,495 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:31:27,965 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-034.pkl
2017-02-05 23:31:31,204 INFO     epoch: 34 took 155.713s (0.001s in batching)
training loss:	0.537437
validation loss:	0.524478
validation error:	0.259600
2017-02-05 23:31:31,208 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:34:03,720 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-035.pkl
2017-02-05 23:34:06,958 INFO     epoch: 35 took 155.754s (0.001s in batching)
training loss:	0.536870
validation loss:	0.524466
validation error:	0.257500
2017-02-05 23:34:06,962 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:36:39,875 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-036.pkl
2017-02-05 23:36:43,098 INFO     epoch: 36 took 156.140s (0.001s in batching)
training loss:	0.535972
validation loss:	0.534325
validation error:	0.264300
2017-02-05 23:36:43,103 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:39:15,499 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-037.pkl
2017-02-05 23:39:18,765 INFO     epoch: 37 took 155.667s (0.001s in batching)
training loss:	0.536416
validation loss:	0.524848
validation error:	0.261200
2017-02-05 23:39:18,769 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:41:51,163 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-038.pkl
2017-02-05 23:41:54,413 INFO     epoch: 38 took 155.648s (0.001s in batching)
training loss:	0.535265
validation loss:	0.529213
validation error:	0.262400
2017-02-05 23:41:54,417 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:44:26,813 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-039.pkl
2017-02-05 23:44:30,062 INFO     epoch: 39 took 155.650s (0.001s in batching)
training loss:	0.534587
validation loss:	0.528044
validation error:	0.264400
2017-02-05 23:44:30,067 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:47:02,740 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-040.pkl
2017-02-05 23:47:05,977 INFO     epoch: 40 took 155.915s (0.002s in batching)
training loss:	0.533984
validation loss:	0.526107
validation error:	0.258900
2017-02-05 23:47:05,982 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:49:39,178 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-041.pkl
2017-02-05 23:49:42,426 INFO     epoch: 41 took 156.449s (0.001s in batching)
training loss:	0.533475
validation loss:	0.525634
validation error:	0.262500
2017-02-05 23:49:42,431 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:52:14,805 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-042.pkl
2017-02-05 23:52:18,063 INFO     epoch: 42 took 155.637s (0.001s in batching)
training loss:	0.533381
validation loss:	0.524081
validation error:	0.261300
2017-02-05 23:52:18,067 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:54:34,068 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-043.pkl
2017-02-05 23:54:37,241 INFO     epoch: 43 took 139.178s (0.001s in batching)
training loss:	0.532542
validation loss:	0.520903
validation error:	0.256300
2017-02-05 23:54:37,246 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:57:08,262 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-044.pkl
2017-02-05 23:57:11,518 INFO     epoch: 44 took 154.277s (0.001s in batching)
training loss:	0.531845
validation loss:	0.522028
validation error:	0.260300
2017-02-05 23:57:11,523 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-05 23:59:42,197 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-045.pkl
2017-02-05 23:59:45,436 INFO     epoch: 45 took 153.917s (0.001s in batching)
training loss:	0.531480
validation loss:	0.519584
validation error:	0.255500
2017-02-05 23:59:45,440 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:02:16,312 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-046.pkl
2017-02-06 00:02:19,539 INFO     epoch: 46 took 154.103s (0.001s in batching)
training loss:	0.531425
validation loss:	0.518834
validation error:	0.257400
2017-02-06 00:02:19,544 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:04:50,869 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-047.pkl
2017-02-06 00:04:54,092 INFO     epoch: 47 took 154.554s (0.001s in batching)
training loss:	0.530316
validation loss:	0.522336
validation error:	0.258200
2017-02-06 00:04:54,097 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:07:25,219 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-048.pkl
2017-02-06 00:07:28,446 INFO     epoch: 48 took 154.354s (0.001s in batching)
training loss:	0.529946
validation loss:	0.522190
validation error:	0.256200
2017-02-06 00:07:28,451 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:09:59,881 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-049.pkl
2017-02-06 00:10:03,061 INFO     epoch: 49 took 154.615s (0.001s in batching)
training loss:	0.529632
validation loss:	0.518179
validation error:	0.255800
2017-02-06 00:10:03,066 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:12:34,459 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-050.pkl
2017-02-06 00:12:37,692 INFO     epoch: 50 took 154.631s (0.001s in batching)
training loss:	0.528892
validation loss:	0.520942
validation error:	0.260500
2017-02-06 00:12:37,698 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:15:08,864 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-051.pkl
2017-02-06 00:15:12,070 INFO     epoch: 51 took 154.379s (0.002s in batching)
training loss:	0.529080
validation loss:	0.520737
validation error:	0.260200
2017-02-06 00:15:12,075 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:17:43,393 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-052.pkl
2017-02-06 00:17:46,634 INFO     epoch: 52 took 154.563s (0.002s in batching)
training loss:	0.528073
validation loss:	0.520362
validation error:	0.258300
2017-02-06 00:17:46,639 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:20:18,165 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-053.pkl
2017-02-06 00:20:21,422 INFO     epoch: 53 took 154.788s (0.001s in batching)
training loss:	0.527652
validation loss:	0.516698
validation error:	0.257100
2017-02-06 00:20:21,427 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:22:53,019 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-054.pkl
2017-02-06 00:22:56,266 INFO     epoch: 54 took 154.844s (0.001s in batching)
training loss:	0.526397
validation loss:	0.513858
validation error:	0.256100
2017-02-06 00:22:56,271 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:25:14,527 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-055.pkl
2017-02-06 00:25:17,263 INFO     epoch: 55 took 140.998s (0.001s in batching)
training loss:	0.526343
validation loss:	0.514995
validation error:	0.253200
2017-02-06 00:25:17,268 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:27:47,994 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-056.pkl
2017-02-06 00:27:51,267 INFO     epoch: 56 took 154.004s (0.001s in batching)
training loss:	0.526310
validation loss:	0.518853
validation error:	0.259400
2017-02-06 00:27:51,272 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:30:25,480 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-057.pkl
2017-02-06 00:30:29,036 INFO     epoch: 57 took 157.769s (0.001s in batching)
training loss:	0.525516
validation loss:	0.518471
validation error:	0.256500
2017-02-06 00:30:29,041 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:33:08,247 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-058.pkl
2017-02-06 00:33:11,805 INFO     epoch: 58 took 162.769s (0.002s in batching)
training loss:	0.525042
validation loss:	0.517119
validation error:	0.255700
2017-02-06 00:33:11,810 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:35:50,274 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-059.pkl
2017-02-06 00:35:53,791 INFO     epoch: 59 took 161.986s (0.001s in batching)
training loss:	0.525526
validation loss:	0.514623
validation error:	0.258800
2017-02-06 00:35:53,796 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:38:33,670 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-060.pkl
2017-02-06 00:38:37,180 INFO     epoch: 60 took 163.389s (0.001s in batching)
training loss:	0.525505
validation loss:	0.516908
validation error:	0.254800
2017-02-06 00:38:37,186 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:41:16,984 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-061.pkl
2017-02-06 00:41:20,179 INFO     epoch: 61 took 162.999s (0.002s in batching)
training loss:	0.523956
validation loss:	0.518399
validation error:	0.258900
2017-02-06 00:41:20,185 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:44:00,322 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-062.pkl
2017-02-06 00:44:03,379 INFO     epoch: 62 took 163.199s (0.002s in batching)
training loss:	0.523468
validation loss:	0.515887
validation error:	0.256600
2017-02-06 00:44:03,385 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:46:43,161 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-063.pkl
2017-02-06 00:46:46,760 INFO     epoch: 63 took 163.382s (0.002s in batching)
training loss:	0.523398
validation loss:	0.518950
validation error:	0.258400
2017-02-06 00:46:46,765 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:49:25,830 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-064.pkl
2017-02-06 00:49:29,360 INFO     epoch: 64 took 162.599s (0.001s in batching)
training loss:	0.523524
validation loss:	0.517240
validation error:	0.254800
2017-02-06 00:49:29,367 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:52:09,100 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-065.pkl
2017-02-06 00:52:12,219 INFO     epoch: 65 took 162.860s (0.002s in batching)
training loss:	0.523328
validation loss:	0.513889
validation error:	0.255900
2017-02-06 00:52:12,225 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:54:51,667 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-066.pkl
2017-02-06 00:54:55,017 INFO     epoch: 66 took 162.797s (0.001s in batching)
training loss:	0.521715
validation loss:	0.514245
validation error:	0.252400
2017-02-06 00:54:55,022 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 00:57:34,191 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-067.pkl
2017-02-06 00:57:37,732 INFO     epoch: 67 took 162.715s (0.002s in batching)
training loss:	0.522676
validation loss:	0.516611
validation error:	0.256400
2017-02-06 00:57:37,737 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:00:17,163 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-068.pkl
2017-02-06 01:00:20,693 INFO     epoch: 68 took 162.961s (0.001s in batching)
training loss:	0.522141
validation loss:	0.517652
validation error:	0.255700
2017-02-06 01:00:20,698 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:02:59,884 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-069.pkl
2017-02-06 01:03:03,128 INFO     epoch: 69 took 162.435s (0.001s in batching)
training loss:	0.521531
validation loss:	0.516549
validation error:	0.257500
2017-02-06 01:03:03,135 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:05:42,553 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-070.pkl
2017-02-06 01:05:46,064 INFO     epoch: 70 took 162.936s (0.002s in batching)
training loss:	0.521632
validation loss:	0.513599
validation error:	0.251500
2017-02-06 01:05:46,070 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:08:25,286 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-071.pkl
2017-02-06 01:08:28,769 INFO     epoch: 71 took 162.705s (0.001s in batching)
training loss:	0.521975
validation loss:	0.517544
validation error:	0.257700
2017-02-06 01:08:28,774 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:11:07,955 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-072.pkl
2017-02-06 01:11:11,497 INFO     epoch: 72 took 162.728s (0.001s in batching)
training loss:	0.521856
validation loss:	0.516016
validation error:	0.257400
2017-02-06 01:11:11,503 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:13:50,919 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-073.pkl
2017-02-06 01:13:54,405 INFO     epoch: 73 took 162.908s (0.001s in batching)
training loss:	0.521050
validation loss:	0.517357
validation error:	0.258200
2017-02-06 01:13:54,411 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:16:34,318 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-074.pkl
2017-02-06 01:16:37,852 INFO     epoch: 74 took 163.447s (0.001s in batching)
training loss:	0.519629
validation loss:	0.518160
validation error:	0.256300
2017-02-06 01:16:37,859 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:19:17,693 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-075.pkl
2017-02-06 01:19:21,234 INFO     epoch: 75 took 163.382s (0.002s in batching)
training loss:	0.520768
validation loss:	0.513605
validation error:	0.255100
2017-02-06 01:19:21,242 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:22:02,218 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-076.pkl
2017-02-06 01:22:05,836 INFO     epoch: 76 took 164.602s (0.002s in batching)
training loss:	0.520281
validation loss:	0.513582
validation error:	0.252800
2017-02-06 01:22:05,842 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:24:51,319 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-077.pkl
2017-02-06 01:24:54,295 INFO     epoch: 77 took 168.459s (0.001s in batching)
training loss:	0.519656
validation loss:	0.515689
validation error:	0.258800
2017-02-06 01:24:54,301 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:27:39,081 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-078.pkl
2017-02-06 01:27:42,315 INFO     epoch: 78 took 168.020s (0.002s in batching)
training loss:	0.519386
validation loss:	0.515798
validation error:	0.254300
2017-02-06 01:27:42,321 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:30:26,692 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-079.pkl
2017-02-06 01:30:30,514 INFO     epoch: 79 took 168.199s (0.001s in batching)
training loss:	0.519137
validation loss:	0.513271
validation error:	0.251900
2017-02-06 01:30:30,520 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:33:14,387 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-080.pkl
2017-02-06 01:33:17,831 INFO     epoch: 80 took 167.317s (0.002s in batching)
training loss:	0.518955
validation loss:	0.514116
validation error:	0.253200
2017-02-06 01:33:17,837 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:36:02,161 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-081.pkl
2017-02-06 01:36:05,550 INFO     epoch: 81 took 167.719s (0.001s in batching)
training loss:	0.519376
validation loss:	0.512453
validation error:	0.252000
2017-02-06 01:36:05,556 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:38:50,569 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-082.pkl
2017-02-06 01:38:53,799 INFO     epoch: 82 took 168.249s (0.002s in batching)
training loss:	0.518523
validation loss:	0.515551
validation error:	0.254600
2017-02-06 01:38:53,805 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:41:37,292 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-083.pkl
2017-02-06 01:41:41,273 INFO     epoch: 83 took 167.474s (0.001s in batching)
training loss:	0.518526
validation loss:	0.511589
validation error:	0.250600
2017-02-06 01:41:41,377 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:44:29,866 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-084.pkl
2017-02-06 01:44:33,543 INFO     epoch: 84 took 172.270s (0.002s in batching)
training loss:	0.518476
validation loss:	0.512157
validation error:	0.249900
2017-02-06 01:44:33,551 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:47:22,683 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-085.pkl
2017-02-06 01:47:26,318 INFO     epoch: 85 took 172.775s (0.002s in batching)
training loss:	0.518638
validation loss:	0.513206
validation error:	0.253300
2017-02-06 01:47:26,324 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:50:14,851 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-086.pkl
2017-02-06 01:50:18,052 INFO     epoch: 86 took 171.734s (0.001s in batching)
training loss:	0.517875
validation loss:	0.513581
validation error:	0.249400
2017-02-06 01:50:18,057 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:53:07,727 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-087.pkl
2017-02-06 01:53:11,195 INFO     epoch: 87 took 173.143s (0.002s in batching)
training loss:	0.517574
validation loss:	0.514489
validation error:	0.254500
2017-02-06 01:53:11,202 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:56:01,304 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-088.pkl
2017-02-06 01:56:05,282 INFO     epoch: 88 took 174.087s (0.002s in batching)
training loss:	0.518269
validation loss:	0.514017
validation error:	0.252000
2017-02-06 01:56:05,288 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 01:58:55,019 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-089.pkl
2017-02-06 01:58:58,252 INFO     epoch: 89 took 172.970s (0.002s in batching)
training loss:	0.517032
validation loss:	0.514746
validation error:	0.252600
2017-02-06 01:58:58,258 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 02:01:47,686 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-090.pkl
2017-02-06 02:01:51,136 INFO     epoch: 90 took 172.884s (0.002s in batching)
training loss:	0.517503
validation loss:	0.514476
validation error:	0.253400
2017-02-06 02:01:51,142 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 02:04:40,605 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-091.pkl
2017-02-06 02:04:44,220 INFO     epoch: 91 took 173.084s (0.001s in batching)
training loss:	0.516417
validation loss:	0.516512
validation error:	0.254600
2017-02-06 02:04:44,226 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 02:07:32,550 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-092.pkl
2017-02-06 02:07:35,306 INFO     epoch: 92 took 171.087s (0.002s in batching)
training loss:	0.516940
validation loss:	0.514027
validation error:	0.251800
2017-02-06 02:07:35,312 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 02:10:33,975 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-093.pkl
2017-02-06 02:10:38,284 INFO     epoch: 93 took 182.978s (0.002s in batching)
training loss:	0.516629
validation loss:	0.512972
validation error:	0.250900
2017-02-06 02:10:38,292 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 02:13:37,345 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-094.pkl
2017-02-06 02:13:41,168 INFO     epoch: 94 took 182.885s (0.002s in batching)
training loss:	0.516340
validation loss:	0.513742
validation error:	0.253900
2017-02-06 02:13:41,175 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 02:16:41,456 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-095.pkl
2017-02-06 02:16:45,482 INFO     epoch: 95 took 184.314s (0.002s in batching)
training loss:	0.516423
validation loss:	0.522406
validation error:	0.258400
2017-02-06 02:16:45,491 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 02:19:44,890 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-096.pkl
2017-02-06 02:19:48,482 INFO     epoch: 96 took 183.000s (0.002s in batching)
training loss:	0.516104
validation loss:	0.517502
validation error:	0.255500
2017-02-06 02:19:48,489 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 02:22:48,743 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-097.pkl
2017-02-06 02:22:52,793 INFO     epoch: 97 took 184.311s (0.002s in batching)
training loss:	0.516102
validation loss:	0.513689
validation error:	0.253300
2017-02-06 02:22:52,804 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 02:25:52,674 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-098.pkl
2017-02-06 02:25:56,244 INFO     epoch: 98 took 183.450s (0.002s in batching)
training loss:	0.515602
validation loss:	0.518662
validation error:	0.255500
2017-02-06 02:25:56,250 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
2017-02-06 02:28:56,983 INFO     Saving network params to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/epoch-099.pkl
2017-02-06 02:29:00,862 INFO     epoch: 99 took 184.619s (0.002s in batching)
training loss:	0.515443
validation loss:	0.513027
validation error:	0.251700
2017-02-06 02:29:00,870 INFO     Wrote output to trainNN/out/v049-finunified-4-g199b048:lstm-best-layers-70-50-35/config.json
