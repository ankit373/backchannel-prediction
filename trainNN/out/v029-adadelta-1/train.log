2016-12-08 18:49:05 >>> version=v029-adadelta-1
2016-12-08 18:49:05 >>> loading config file extract_pfiles_python/out/v024-gauss-sgd-1-7-g13ff9f6-context40/config.json
2016-12-08 18:49:05 >>> loading numpy file extract_pfiles_python/out/v024-gauss-sgd-1-7-g13ff9f6-context40/train.npz
2016-12-08 18:49:09 >>> loading numpy file extract_pfiles_python/out/v024-gauss-sgd-1-7-g13ff9f6-context40/validate.npz
2016-12-08 18:49:09 >>> Training network with 21452 trainable out of 21452 total params.
2016-12-08 18:49:09 >>> Using adadelta with learning_rate=1.000000
2016-12-08 18:49:09 >>> Compiling theano functions...
2016-12-08 18:49:10 >>> Starting training...
2016-12-08 18:49:30 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-000.pkl
2016-12-08 18:49:31 >>> epoch: 0 took 21.052s
training loss:	0.595053
validation loss:	0.576529
validation error:	0.296636
2016-12-08 18:49:51 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-001.pkl
2016-12-08 18:49:52 >>> epoch: 1 took 20.782s
training loss:	0.578890
validation loss:	0.567302
validation error:	0.287285
2016-12-08 18:50:11 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-002.pkl
2016-12-08 18:50:12 >>> epoch: 2 took 20.559s
training loss:	0.572508
validation loss:	0.563368
validation error:	0.284369
2016-12-08 18:50:33 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-003.pkl
2016-12-08 18:50:34 >>> epoch: 3 took 21.361s
training loss:	0.569183
validation loss:	0.560614
validation error:	0.282729
2016-12-08 18:50:54 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-004.pkl
2016-12-08 18:50:55 >>> epoch: 4 took 21.366s
training loss:	0.566749
validation loss:	0.559891
validation error:	0.282789
2016-12-08 18:51:15 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-005.pkl
2016-12-08 18:51:16 >>> epoch: 5 took 20.914s
training loss:	0.564638
validation loss:	0.558809
validation error:	0.281672
2016-12-08 18:51:36 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-006.pkl
2016-12-08 18:51:37 >>> epoch: 6 took 20.838s
training loss:	0.562867
validation loss:	0.556363
validation error:	0.279769
2016-12-08 18:51:57 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-007.pkl
2016-12-08 18:51:58 >>> epoch: 7 took 20.784s
training loss:	0.561326
validation loss:	0.557667
validation error:	0.281544
2016-12-08 18:52:18 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-008.pkl
2016-12-08 18:52:19 >>> epoch: 8 took 21.175s
training loss:	0.560103
validation loss:	0.554534
validation error:	0.278268
2016-12-08 18:52:40 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-009.pkl
2016-12-08 18:52:41 >>> epoch: 9 took 22.298s
training loss:	0.558954
validation loss:	0.553447
validation error:	0.278260
2016-12-08 18:53:01 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-010.pkl
2016-12-08 18:53:02 >>> epoch: 10 took 21.240s
training loss:	0.557915
validation loss:	0.553730
validation error:	0.278156
2016-12-08 18:53:23 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-011.pkl
2016-12-08 18:53:24 >>> epoch: 11 took 21.894s
training loss:	0.556979
validation loss:	0.551565
validation error:	0.277080
2016-12-08 18:53:46 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-012.pkl
2016-12-08 18:53:47 >>> epoch: 12 took 22.723s
training loss:	0.556128
validation loss:	0.551471
validation error:	0.277006
2016-12-08 18:54:07 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-013.pkl
2016-12-08 18:54:08 >>> epoch: 13 took 20.796s
training loss:	0.555215
validation loss:	0.550852
validation error:	0.276770
2016-12-08 18:54:28 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-014.pkl
2016-12-08 18:54:29 >>> epoch: 14 took 21.156s
training loss:	0.554375
validation loss:	0.548491
validation error:	0.275454
2016-12-08 18:54:49 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-015.pkl
2016-12-08 18:54:51 >>> epoch: 15 took 21.662s
training loss:	0.553629
validation loss:	0.550785
validation error:	0.277135
2016-12-08 18:55:11 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-016.pkl
2016-12-08 18:55:13 >>> epoch: 16 took 21.875s
training loss:	0.552907
validation loss:	0.547137
validation error:	0.274913
2016-12-08 18:55:33 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-017.pkl
2016-12-08 18:55:34 >>> epoch: 17 took 21.249s
training loss:	0.552210
validation loss:	0.551025
validation error:	0.278896
2016-12-08 18:55:54 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-018.pkl
2016-12-08 18:55:55 >>> epoch: 18 took 21.561s
training loss:	0.551608
validation loss:	0.548725
validation error:	0.275692
2016-12-08 18:56:16 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-019.pkl
2016-12-08 18:56:17 >>> epoch: 19 took 22.026s
training loss:	0.551029
validation loss:	0.547938
validation error:	0.274214
2016-12-08 18:56:37 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-020.pkl
2016-12-08 18:56:39 >>> epoch: 20 took 21.299s
training loss:	0.550492
validation loss:	0.546730
validation error:	0.273972
2016-12-08 18:56:59 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-021.pkl
2016-12-08 18:57:00 >>> epoch: 21 took 21.420s
training loss:	0.550026
validation loss:	0.548222
validation error:	0.275947
2016-12-08 18:57:20 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-022.pkl
2016-12-08 18:57:22 >>> epoch: 22 took 21.410s
training loss:	0.549590
validation loss:	0.550287
validation error:	0.277569
2016-12-08 18:57:41 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-023.pkl
2016-12-08 18:57:43 >>> epoch: 23 took 21.047s
training loss:	0.549141
validation loss:	0.546607
validation error:	0.273536
2016-12-08 18:58:02 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-024.pkl
2016-12-08 18:58:03 >>> epoch: 24 took 20.519s
training loss:	0.548660
validation loss:	0.545294
validation error:	0.273531
2016-12-08 18:58:22 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-025.pkl
2016-12-08 18:58:23 >>> epoch: 25 took 20.424s
training loss:	0.548403
validation loss:	0.546220
validation error:	0.274074
2016-12-08 18:58:43 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-026.pkl
2016-12-08 18:58:44 >>> epoch: 26 took 20.523s
training loss:	0.548019
validation loss:	0.545882
validation error:	0.273739
2016-12-08 18:59:03 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-027.pkl
2016-12-08 18:59:05 >>> epoch: 27 took 20.552s
training loss:	0.547674
validation loss:	0.547120
validation error:	0.274107
2016-12-08 18:59:25 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-028.pkl
2016-12-08 18:59:26 >>> epoch: 28 took 21.257s
training loss:	0.547393
validation loss:	0.547164
validation error:	0.273289
2016-12-08 18:59:45 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-029.pkl
2016-12-08 18:59:47 >>> epoch: 29 took 20.710s
training loss:	0.547020
validation loss:	0.546110
validation error:	0.273210
2016-12-08 19:00:06 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-030.pkl
2016-12-08 19:00:07 >>> epoch: 30 took 20.708s
training loss:	0.546717
validation loss:	0.547074
validation error:	0.275651
2016-12-08 19:00:27 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-031.pkl
2016-12-08 19:00:29 >>> epoch: 31 took 21.450s
training loss:	0.546486
validation loss:	0.545626
validation error:	0.273717
2016-12-08 19:00:49 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-032.pkl
2016-12-08 19:00:51 >>> epoch: 32 took 21.853s
training loss:	0.546128
validation loss:	0.546090
validation error:	0.273597
2016-12-08 19:01:11 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-033.pkl
2016-12-08 19:01:12 >>> epoch: 33 took 21.530s
training loss:	0.545864
validation loss:	0.547247
validation error:	0.274598
2016-12-08 19:01:32 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-034.pkl
2016-12-08 19:01:34 >>> epoch: 34 took 21.421s
training loss:	0.545584
validation loss:	0.545303
validation error:	0.273147
2016-12-08 19:01:53 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-035.pkl
2016-12-08 19:01:54 >>> epoch: 35 took 20.950s
training loss:	0.545287
validation loss:	0.545498
validation error:	0.273646
2016-12-08 19:02:14 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-036.pkl
2016-12-08 19:02:15 >>> epoch: 36 took 20.577s
training loss:	0.545082
validation loss:	0.545323
validation error:	0.273147
2016-12-08 19:02:34 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-037.pkl
2016-12-08 19:02:36 >>> epoch: 37 took 20.575s
training loss:	0.544809
validation loss:	0.546088
validation error:	0.274546
2016-12-08 19:02:55 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-038.pkl
2016-12-08 19:02:56 >>> epoch: 38 took 20.654s
training loss:	0.544569
validation loss:	0.544778
validation error:	0.273163
2016-12-08 19:03:17 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-039.pkl
2016-12-08 19:03:18 >>> epoch: 39 took 21.480s
training loss:	0.544330
validation loss:	0.545537
validation error:	0.273800
2016-12-08 19:03:38 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-040.pkl
2016-12-08 19:03:39 >>> epoch: 40 took 21.529s
training loss:	0.544155
validation loss:	0.546764
validation error:	0.273959
2016-12-08 19:03:59 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-041.pkl
2016-12-08 19:04:01 >>> epoch: 41 took 21.349s
training loss:	0.543883
validation loss:	0.544360
validation error:	0.272335
2016-12-08 19:04:20 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-042.pkl
2016-12-08 19:04:21 >>> epoch: 42 took 20.874s
training loss:	0.543736
validation loss:	0.545403
validation error:	0.273229
2016-12-08 19:04:42 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-043.pkl
2016-12-08 19:04:43 >>> epoch: 43 took 21.223s
training loss:	0.543476
validation loss:	0.543748
validation error:	0.271792
2016-12-08 19:05:03 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-044.pkl
2016-12-08 19:05:04 >>> epoch: 44 took 21.208s
training loss:	0.543353
validation loss:	0.546058
validation error:	0.273591
2016-12-08 19:05:23 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-045.pkl
2016-12-08 19:05:25 >>> epoch: 45 took 20.604s
training loss:	0.543139
validation loss:	0.544191
validation error:	0.271739
2016-12-08 19:05:44 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-046.pkl
2016-12-08 19:05:45 >>> epoch: 46 took 20.557s
training loss:	0.543023
validation loss:	0.545498
validation error:	0.274090
2016-12-08 19:06:05 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-047.pkl
2016-12-08 19:06:06 >>> epoch: 47 took 20.896s
training loss:	0.542814
validation loss:	0.544423
validation error:	0.273295
2016-12-08 19:06:25 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-048.pkl
2016-12-08 19:06:27 >>> epoch: 48 took 20.672s
training loss:	0.542651
validation loss:	0.544772
validation error:	0.273040
2016-12-08 19:06:47 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-049.pkl
2016-12-08 19:06:48 >>> epoch: 49 took 21.258s
training loss:	0.542457
validation loss:	0.544308
validation error:	0.273843
2016-12-08 19:07:08 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-050.pkl
2016-12-08 19:07:09 >>> epoch: 50 took 21.334s
training loss:	0.542330
validation loss:	0.545169
validation error:	0.272406
2016-12-08 19:07:29 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-051.pkl
2016-12-08 19:07:30 >>> epoch: 51 took 20.669s
training loss:	0.542140
validation loss:	0.544746
validation error:	0.273484
2016-12-08 19:07:50 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-052.pkl
2016-12-08 19:07:51 >>> epoch: 52 took 21.030s
training loss:	0.542010
validation loss:	0.543107
validation error:	0.272365
2016-12-08 19:08:11 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-053.pkl
2016-12-08 19:08:12 >>> epoch: 53 took 20.785s
training loss:	0.541843
validation loss:	0.544225
validation error:	0.272250
2016-12-08 19:08:31 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-054.pkl
2016-12-08 19:08:32 >>> epoch: 54 took 20.618s
training loss:	0.541697
validation loss:	0.543527
validation error:	0.272082
2016-12-08 19:08:52 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-055.pkl
2016-12-08 19:08:53 >>> epoch: 55 took 20.819s
training loss:	0.541571
validation loss:	0.542840
validation error:	0.272036
2016-12-08 19:09:13 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-056.pkl
2016-12-08 19:09:14 >>> epoch: 56 took 21.222s
training loss:	0.541422
validation loss:	0.542954
validation error:	0.272214
2016-12-08 19:09:34 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-057.pkl
2016-12-08 19:09:35 >>> epoch: 57 took 20.891s
training loss:	0.541275
validation loss:	0.543830
validation error:	0.272760
2016-12-08 19:09:55 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-058.pkl
2016-12-08 19:09:57 >>> epoch: 58 took 21.241s
training loss:	0.541156
validation loss:	0.543797
validation error:	0.272568
2016-12-08 19:10:17 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-059.pkl
2016-12-08 19:10:18 >>> epoch: 59 took 21.386s
training loss:	0.540950
validation loss:	0.544972
validation error:	0.272412
2016-12-08 19:10:37 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-060.pkl
2016-12-08 19:10:39 >>> epoch: 60 took 20.640s
training loss:	0.540870
validation loss:	0.544248
validation error:	0.272428
2016-12-08 19:10:58 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-061.pkl
2016-12-08 19:10:59 >>> epoch: 61 took 20.628s
training loss:	0.540748
validation loss:	0.543973
validation error:	0.272881
2016-12-08 19:11:19 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-062.pkl
2016-12-08 19:11:20 >>> epoch: 62 took 20.581s
training loss:	0.540660
validation loss:	0.542915
validation error:	0.271707
2016-12-08 19:11:39 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-063.pkl
2016-12-08 19:11:40 >>> epoch: 63 took 20.535s
training loss:	0.540513
validation loss:	0.543654
validation error:	0.271734
2016-12-08 19:12:00 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-064.pkl
2016-12-08 19:12:01 >>> epoch: 64 took 20.513s
training loss:	0.540425
validation loss:	0.543970
validation error:	0.272483
2016-12-08 19:12:20 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-065.pkl
2016-12-08 19:12:22 >>> epoch: 65 took 20.850s
training loss:	0.540283
validation loss:	0.543620
validation error:	0.272425
2016-12-08 19:12:42 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-066.pkl
2016-12-08 19:12:43 >>> epoch: 66 took 21.548s
training loss:	0.540165
validation loss:	0.545928
validation error:	0.273476
2016-12-08 19:13:03 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-067.pkl
2016-12-08 19:13:04 >>> epoch: 67 took 20.868s
training loss:	0.540059
validation loss:	0.543352
validation error:	0.271281
2016-12-08 19:13:23 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-068.pkl
2016-12-08 19:13:25 >>> epoch: 68 took 20.519s
training loss:	0.539943
validation loss:	0.545072
validation error:	0.273610
2016-12-08 19:13:44 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-069.pkl
2016-12-08 19:13:45 >>> epoch: 69 took 20.552s
training loss:	0.539824
validation loss:	0.544563
validation error:	0.273591
2016-12-08 19:14:05 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-070.pkl
2016-12-08 19:14:06 >>> epoch: 70 took 20.912s
training loss:	0.539724
validation loss:	0.544273
validation error:	0.272667
2016-12-08 19:14:26 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-071.pkl
2016-12-08 19:14:27 >>> epoch: 71 took 20.977s
training loss:	0.539647
validation loss:	0.543434
validation error:	0.271687
2016-12-08 19:14:47 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-072.pkl
2016-12-08 19:14:48 >>> epoch: 72 took 21.367s
training loss:	0.539527
validation loss:	0.543446
validation error:	0.271761
2016-12-08 19:15:08 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-073.pkl
2016-12-08 19:15:09 >>> epoch: 73 took 21.030s
training loss:	0.539475
validation loss:	0.543641
validation error:	0.272285
2016-12-08 19:15:30 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-074.pkl
2016-12-08 19:15:31 >>> epoch: 74 took 21.212s
training loss:	0.539342
validation loss:	0.543378
validation error:	0.274461
2016-12-08 19:15:50 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-075.pkl
2016-12-08 19:15:51 >>> epoch: 75 took 20.668s
training loss:	0.539281
validation loss:	0.542356
validation error:	0.271309
2016-12-08 19:16:11 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-076.pkl
2016-12-08 19:16:12 >>> epoch: 76 took 21.041s
training loss:	0.539163
validation loss:	0.544998
validation error:	0.275067
2016-12-08 19:16:33 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-077.pkl
2016-12-08 19:16:34 >>> epoch: 77 took 21.462s
training loss:	0.539078
validation loss:	0.542517
validation error:	0.271536
2016-12-08 19:16:53 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-078.pkl
2016-12-08 19:16:55 >>> epoch: 78 took 20.759s
training loss:	0.539032
validation loss:	0.544854
validation error:	0.274595
2016-12-08 19:17:14 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-079.pkl
2016-12-08 19:17:15 >>> epoch: 79 took 20.880s
training loss:	0.538913
validation loss:	0.544861
validation error:	0.273824
2016-12-08 19:17:36 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-080.pkl
2016-12-08 19:17:37 >>> epoch: 80 took 21.205s
training loss:	0.538840
validation loss:	0.544594
validation error:	0.272834
2016-12-08 19:17:57 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-081.pkl
2016-12-08 19:17:58 >>> epoch: 81 took 21.792s
training loss:	0.538794
validation loss:	0.544042
validation error:	0.272864
2016-12-08 19:18:19 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-082.pkl
2016-12-08 19:18:20 >>> epoch: 82 took 21.963s
training loss:	0.538645
validation loss:	0.544432
validation error:	0.271742
2016-12-08 19:18:40 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-083.pkl
2016-12-08 19:18:41 >>> epoch: 83 took 20.821s
training loss:	0.538605
validation loss:	0.542495
validation error:	0.271594
2016-12-08 19:19:01 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-084.pkl
2016-12-08 19:19:02 >>> epoch: 84 took 20.958s
training loss:	0.538515
validation loss:	0.543380
validation error:	0.272198
2016-12-08 19:19:24 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-085.pkl
2016-12-08 19:19:25 >>> epoch: 85 took 22.866s
training loss:	0.538468
validation loss:	0.543233
validation error:	0.271811
2016-12-08 19:19:45 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-086.pkl
2016-12-08 19:19:46 >>> epoch: 86 took 21.341s
training loss:	0.538393
validation loss:	0.543316
validation error:	0.272573
2016-12-08 19:20:06 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-087.pkl
2016-12-08 19:20:08 >>> epoch: 87 took 21.176s
training loss:	0.538304
validation loss:	0.542746
validation error:	0.271709
2016-12-08 19:20:28 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-088.pkl
2016-12-08 19:20:29 >>> epoch: 88 took 21.486s
training loss:	0.538256
validation loss:	0.542399
validation error:	0.271191
2016-12-08 19:20:50 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-089.pkl
2016-12-08 19:20:51 >>> epoch: 89 took 21.800s
training loss:	0.538133
validation loss:	0.545368
validation error:	0.274057
2016-12-08 19:21:11 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-090.pkl
2016-12-08 19:21:12 >>> epoch: 90 took 21.213s
training loss:	0.538088
validation loss:	0.543610
validation error:	0.271912
2016-12-08 19:21:32 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-091.pkl
2016-12-08 19:21:33 >>> epoch: 91 took 20.911s
training loss:	0.538019
validation loss:	0.542928
validation error:	0.271811
2016-12-08 19:21:53 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-092.pkl
2016-12-08 19:21:54 >>> epoch: 92 took 21.381s
training loss:	0.537972
validation loss:	0.544506
validation error:	0.273366
2016-12-08 19:22:15 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-093.pkl
2016-12-08 19:22:16 >>> epoch: 93 took 21.877s
training loss:	0.537954
validation loss:	0.544382
validation error:	0.271904
2016-12-08 19:22:36 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-094.pkl
2016-12-08 19:22:37 >>> epoch: 94 took 20.841s
training loss:	0.537824
validation loss:	0.545538
validation error:	0.273410
2016-12-08 19:22:57 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-095.pkl
2016-12-08 19:22:58 >>> epoch: 95 took 21.232s
training loss:	0.537762
validation loss:	0.543387
validation error:	0.271482
2016-12-08 19:23:18 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-096.pkl
2016-12-08 19:23:19 >>> epoch: 96 took 20.645s
training loss:	0.537699
validation loss:	0.542086
validation error:	0.271671
2016-12-08 19:23:39 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-097.pkl
2016-12-08 19:23:40 >>> epoch: 97 took 20.864s
training loss:	0.537696
validation loss:	0.543916
validation error:	0.272211
2016-12-08 19:23:59 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-098.pkl
2016-12-08 19:24:00 >>> epoch: 98 took 20.627s
training loss:	0.537575
validation loss:	0.543259
validation error:	0.270925
2016-12-08 19:24:20 >>> Saving network params to trainNN/out/v029-adadelta-1/epoch-099.pkl
2016-12-08 19:24:21 >>> epoch: 99 took 20.667s
training loss:	0.537532
validation loss:	0.543893
validation error:	0.273045
2016-12-08 19:24:21 >>> Wrote output to trainNN/out/v029-adadelta-1/config.json
