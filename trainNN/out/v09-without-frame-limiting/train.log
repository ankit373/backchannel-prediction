2016-11-25 19:28:23 >>> version=v09-without-frame-limiting
2016-11-25 19:28:23 >>> loading config file extract_pfiles_python/out/v09-without-frame-limiting-context40/config.json
2016-11-25 19:28:23 >>> loading numpy file extract_pfiles_python/out/v09-without-frame-limiting-context40/train.npz
2016-11-25 19:28:26 >>> loading numpy file extract_pfiles_python/out/v09-without-frame-limiting-context40/validate.npz
2016-11-25 19:28:27 >>> Using fuzzy_newbob as schedulung method.
2016-11-25 19:28:27 >>> Training network with 21452 trainable out of 21452 total params.
2016-11-25 19:28:27 >>> Using adadelta with learning_rate=1.000000
2016-11-25 19:28:27 >>> Compiling theano functions...
2016-11-25 19:28:27 >>> Starting training...
2016-11-25 19:28:47 >>>   training loss:	0.669909
2016-11-25 19:28:47 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-000.pkl
2016-11-25 19:28:48 >>> epoch: 0 validation error:		0.414760
2016-11-25 19:29:06 >>>   training loss:	0.668427
2016-11-25 19:29:06 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-001.pkl
2016-11-25 19:29:07 >>> epoch: 1 validation error:		0.418126
2016-11-25 19:29:07 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-000.pkl
2016-11-25 19:29:07 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.500000
2016-11-25 19:29:07 >>> Re-compiling train function...
2016-11-25 19:29:24 >>>   training loss:	0.667897
2016-11-25 19:29:24 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-002.pkl
2016-11-25 19:29:25 >>> epoch: 2 validation error:		0.414806
2016-11-25 19:29:25 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-000.pkl
2016-11-25 19:29:25 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.250000
2016-11-25 19:29:25 >>> Re-compiling train function...
2016-11-25 19:29:45 >>>   training loss:	0.667601
2016-11-25 19:29:45 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-003.pkl
2016-11-25 19:29:46 >>> epoch: 3 validation error:		0.414408
2016-11-25 19:30:04 >>>   training loss:	0.667418
2016-11-25 19:30:04 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-004.pkl
2016-11-25 19:30:05 >>> epoch: 4 validation error:		0.414370
2016-11-25 19:30:22 >>>   training loss:	0.667318
2016-11-25 19:30:22 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-005.pkl
2016-11-25 19:30:23 >>> epoch: 5 validation error:		0.414103
2016-11-25 19:30:41 >>>   training loss:	0.667224
2016-11-25 19:30:41 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-006.pkl
2016-11-25 19:30:42 >>> epoch: 6 validation error:		0.414051
2016-11-25 19:30:59 >>>   training loss:	0.667178
2016-11-25 19:30:59 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-007.pkl
2016-11-25 19:31:01 >>> epoch: 7 validation error:		0.414402
2016-11-25 19:31:01 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-006.pkl
2016-11-25 19:31:01 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.125000
2016-11-25 19:31:01 >>> Re-compiling train function...
2016-11-25 19:31:20 >>>   training loss:	0.666940
2016-11-25 19:31:20 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-008.pkl
2016-11-25 19:31:21 >>> epoch: 8 validation error:		0.413874
2016-11-25 19:31:39 >>>   training loss:	0.666903
2016-11-25 19:31:39 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-009.pkl
2016-11-25 19:31:40 >>> epoch: 9 validation error:		0.413731
2016-11-25 19:31:56 >>>   training loss:	0.666861
2016-11-25 19:31:56 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:31:58 >>> epoch: 10 validation error:		0.412982
2016-11-25 19:32:14 >>>   training loss:	0.666842
2016-11-25 19:32:14 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-011.pkl
2016-11-25 19:32:15 >>> epoch: 11 validation error:		0.413612
2016-11-25 19:32:15 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:32:16 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.062500
2016-11-25 19:32:16 >>> Re-compiling train function...
2016-11-25 19:32:34 >>>   training loss:	0.666713
2016-11-25 19:32:34 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-012.pkl
2016-11-25 19:32:36 >>> epoch: 12 validation error:		0.414048
2016-11-25 19:32:36 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:32:36 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.031250
2016-11-25 19:32:36 >>> Re-compiling train function...
2016-11-25 19:32:53 >>>   training loss:	0.666652
2016-11-25 19:32:53 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-013.pkl
2016-11-25 19:32:54 >>> epoch: 13 validation error:		0.413580
2016-11-25 19:32:54 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:32:54 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.015625
2016-11-25 19:32:54 >>> Re-compiling train function...
2016-11-25 19:33:12 >>>   training loss:	0.666627
2016-11-25 19:33:12 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-014.pkl
2016-11-25 19:33:13 >>> epoch: 14 validation error:		0.413485
2016-11-25 19:33:13 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:33:13 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.007812
2016-11-25 19:33:13 >>> Re-compiling train function...
2016-11-25 19:33:31 >>>   training loss:	0.666614
2016-11-25 19:33:31 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-015.pkl
2016-11-25 19:33:32 >>> epoch: 15 validation error:		0.413334
2016-11-25 19:33:32 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:33:33 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.003906
2016-11-25 19:33:33 >>> Re-compiling train function...
2016-11-25 19:33:50 >>>   training loss:	0.666614
2016-11-25 19:33:50 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-016.pkl
2016-11-25 19:33:51 >>> epoch: 16 validation error:		0.413502
2016-11-25 19:33:51 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:33:51 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.001953
2016-11-25 19:33:51 >>> Re-compiling train function...
2016-11-25 19:34:08 >>>   training loss:	0.666618
2016-11-25 19:34:08 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-017.pkl
2016-11-25 19:34:10 >>> epoch: 17 validation error:		0.413363
2016-11-25 19:34:10 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:34:10 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000977
2016-11-25 19:34:10 >>> Re-compiling train function...
2016-11-25 19:34:27 >>>   training loss:	0.666625
2016-11-25 19:34:27 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-018.pkl
2016-11-25 19:34:29 >>> epoch: 18 validation error:		0.413441
2016-11-25 19:34:29 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:34:29 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000488
2016-11-25 19:34:29 >>> Re-compiling train function...
2016-11-25 19:34:47 >>>   training loss:	0.666631
2016-11-25 19:34:47 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-019.pkl
2016-11-25 19:34:48 >>> epoch: 19 validation error:		0.413177
2016-11-25 19:34:48 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:34:48 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000244
2016-11-25 19:34:48 >>> Re-compiling train function...
2016-11-25 19:35:06 >>>   training loss:	0.666638
2016-11-25 19:35:06 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-020.pkl
2016-11-25 19:35:07 >>> epoch: 20 validation error:		0.413124
2016-11-25 19:35:07 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:35:07 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000122
2016-11-25 19:35:07 >>> Re-compiling train function...
2016-11-25 19:35:24 >>>   training loss:	0.666643
2016-11-25 19:35:24 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-021.pkl
2016-11-25 19:35:26 >>> epoch: 21 validation error:		0.413113
2016-11-25 19:35:26 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:35:26 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000061
2016-11-25 19:35:26 >>> Re-compiling train function...
2016-11-25 19:35:44 >>>   training loss:	0.666648
2016-11-25 19:35:44 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-022.pkl
2016-11-25 19:35:46 >>> epoch: 22 validation error:		0.413034
2016-11-25 19:35:46 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:35:46 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000031
2016-11-25 19:35:46 >>> Re-compiling train function...
2016-11-25 19:36:04 >>>   training loss:	0.666649
2016-11-25 19:36:04 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-023.pkl
2016-11-25 19:36:05 >>> epoch: 23 validation error:		0.412997
2016-11-25 19:36:05 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:36:05 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000015
2016-11-25 19:36:05 >>> Re-compiling train function...
2016-11-25 19:36:23 >>>   training loss:	0.666651
2016-11-25 19:36:23 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-024.pkl
2016-11-25 19:36:25 >>> epoch: 24 validation error:		0.413043
2016-11-25 19:36:25 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-010.pkl
2016-11-25 19:36:25 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000008
2016-11-25 19:36:25 >>> Re-compiling train function...
2016-11-25 19:36:45 >>>   training loss:	0.666653
2016-11-25 19:36:45 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-025.pkl
2016-11-25 19:36:46 >>> epoch: 25 validation error:		0.412979
2016-11-25 19:37:03 >>>   training loss:	0.666650
2016-11-25 19:37:03 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-026.pkl
2016-11-25 19:37:04 >>> epoch: 26 validation error:		0.413017
2016-11-25 19:37:04 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-025.pkl
2016-11-25 19:37:04 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000004
2016-11-25 19:37:04 >>> Re-compiling train function...
2016-11-25 19:37:23 >>>   training loss:	0.666652
2016-11-25 19:37:23 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-027.pkl
2016-11-25 19:37:24 >>> epoch: 27 validation error:		0.412985
2016-11-25 19:37:24 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-025.pkl
2016-11-25 19:37:24 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000002
2016-11-25 19:37:24 >>> Re-compiling train function...
2016-11-25 19:37:42 >>>   training loss:	0.666652
2016-11-25 19:37:42 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-028.pkl
2016-11-25 19:37:43 >>> epoch: 28 validation error:		0.412968
2016-11-25 19:38:00 >>>   training loss:	0.666652
2016-11-25 19:38:00 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-029.pkl
2016-11-25 19:38:02 >>> epoch: 29 validation error:		0.412997
2016-11-25 19:38:02 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-028.pkl
2016-11-25 19:38:02 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000001
2016-11-25 19:38:02 >>> Re-compiling train function...
2016-11-25 19:38:19 >>>   training loss:	0.666651
2016-11-25 19:38:19 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-030.pkl
2016-11-25 19:38:21 >>> epoch: 30 validation error:		0.412979
2016-11-25 19:38:21 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-028.pkl
2016-11-25 19:38:21 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 19:38:21 >>> Re-compiling train function...
2016-11-25 19:38:38 >>>   training loss:	0.666653
2016-11-25 19:38:38 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-031.pkl
2016-11-25 19:38:39 >>> epoch: 31 validation error:		0.412973
2016-11-25 19:38:39 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-028.pkl
2016-11-25 19:38:39 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 19:38:39 >>> Re-compiling train function...
2016-11-25 19:38:59 >>>   training loss:	0.666652
2016-11-25 19:38:59 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-032.pkl
2016-11-25 19:39:00 >>> epoch: 32 validation error:		0.412953
2016-11-25 19:39:20 >>>   training loss:	0.666653
2016-11-25 19:39:20 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-033.pkl
2016-11-25 19:39:22 >>> epoch: 33 validation error:		0.412950
2016-11-25 19:39:42 >>>   training loss:	0.666651
2016-11-25 19:39:42 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-034.pkl
2016-11-25 19:39:44 >>> epoch: 34 validation error:		0.412953
2016-11-25 19:39:44 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-033.pkl
2016-11-25 19:39:44 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 19:39:44 >>> Re-compiling train function...
2016-11-25 19:40:04 >>>   training loss:	0.666652
2016-11-25 19:40:04 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-035.pkl
2016-11-25 19:40:05 >>> epoch: 35 validation error:		0.412950
2016-11-25 19:40:23 >>>   training loss:	0.666653
2016-11-25 19:40:23 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-036.pkl
2016-11-25 19:40:24 >>> epoch: 36 validation error:		0.412950
2016-11-25 19:40:48 >>>   training loss:	0.666651
2016-11-25 19:40:48 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-037.pkl
2016-11-25 19:40:49 >>> epoch: 37 validation error:		0.412959
2016-11-25 19:40:49 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-033.pkl
2016-11-25 19:40:49 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 19:40:49 >>> Re-compiling train function...
2016-11-25 19:41:15 >>>   training loss:	0.666654
2016-11-25 19:41:15 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-038.pkl
2016-11-25 19:41:17 >>> epoch: 38 validation error:		0.412970
2016-11-25 19:41:17 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-033.pkl
2016-11-25 19:41:17 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 19:41:17 >>> Re-compiling train function...
2016-11-25 19:41:37 >>>   training loss:	0.666653
2016-11-25 19:41:37 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-039.pkl
2016-11-25 19:41:39 >>> epoch: 39 validation error:		0.412959
2016-11-25 19:41:39 >>> Loading old params from trainNN/out/v09-without-frame-limiting/epoch-033.pkl
2016-11-25 19:41:39 >>> fuzzy_newbob: Updating adadelta with learning_rate=0.000000
2016-11-25 19:41:39 >>> Re-compiling train function...
2016-11-25 19:41:57 >>>   training loss:	0.666652
2016-11-25 19:41:57 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-040.pkl
2016-11-25 19:41:58 >>> epoch: 40 validation error:		0.412947
2016-11-25 19:42:16 >>>   training loss:	0.666651
2016-11-25 19:42:16 >>> Saving network params to trainNN/out/v09-without-frame-limiting/epoch-041.pkl
2016-11-25 19:42:17 >>> epoch: 41 validation error:		0.412976
2016-11-25 19:42:17 >>> learning rate below 1e-08, ending
2016-11-25 19:42:17 >>> Wrote output to trainNN/out/v09-without-frame-limiting/config.json
