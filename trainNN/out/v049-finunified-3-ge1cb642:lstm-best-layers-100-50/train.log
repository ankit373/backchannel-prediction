2017-02-05 21:46:39,733 DEBUG    version=v049-finunified-3-ge1cb642:lstm-best-layers-100-50
2017-02-05 21:46:39,734 INFO     Open DBase(data/db/all240302-spk.dat, data/db/all240302-spk.idx, mode='r')
2017-02-05 21:46:51,511 DEBUG    loading cached extracted data from data/cache/extract-7a00b23850576d60137fa78432d8dfda09ea0f343092bd09848f21113b1acfd0.pickle
2017-02-05 21:46:52,946 DEBUG    set input dim to 9
2017-02-05 21:46:52,949 DEBUG    input dim = 9
2017-02-05 21:46:52,970 DEBUG    shuffling batches
2017-02-05 21:46:58,430 DEBUG    shuffling done
2017-02-05 21:46:58,430 DEBUG    loading data into ram
2017-02-05 21:46:58,577 DEBUG    loading data took 0.146s (cpu: 0.146s)
2017-02-05 21:46:58,920 DEBUG    set input dim to 9
2017-02-05 21:46:58,921 DEBUG    input dim = 9
2017-02-05 21:46:58,955 DEBUG    shuffling batches
2017-02-05 21:46:59,512 DEBUG    shuffling done
2017-02-05 21:46:59,512 DEBUG    loading data into ram
2017-02-05 21:46:59,527 DEBUG    loading data took 0.015s (cpu: 0.015s)
2017-02-05 21:46:59,671 DEBUG    Applying L2 regularization with 0.000100
2017-02-05 21:46:59,810 INFO     Training network with 74752 trainable out of 75052 total params.
2017-02-05 21:47:01,217 DEBUG    Using adam with learning_rate=0.001000
2017-02-05 21:47:42,536 DEBUG    Starting training...
2017-02-05 21:49:39,530 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-000.pkl
2017-02-05 21:49:41,912 INFO     epoch: 0 took 119.375s (0.002s in batching)
training loss:	0.632991
validation loss:	0.580326
validation error:	0.304200
2017-02-05 21:49:41,914 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 21:51:37,547 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-001.pkl
2017-02-05 21:51:39,870 INFO     epoch: 1 took 117.958s (0.002s in batching)
training loss:	0.595040
validation loss:	0.564341
validation error:	0.287800
2017-02-05 21:51:39,872 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 21:53:44,509 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-002.pkl
2017-02-05 21:53:46,902 INFO     epoch: 2 took 127.032s (0.002s in batching)
training loss:	0.583524
validation loss:	0.562265
validation error:	0.284400
2017-02-05 21:53:47,002 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 21:55:51,402 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-003.pkl
2017-02-05 21:55:53,800 INFO     epoch: 3 took 126.899s (0.002s in batching)
training loss:	0.577021
validation loss:	0.559921
validation error:	0.283900
2017-02-05 21:55:53,803 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 21:57:58,468 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-004.pkl
2017-02-05 21:58:00,872 INFO     epoch: 4 took 127.072s (0.002s in batching)
training loss:	0.570987
validation loss:	0.555062
validation error:	0.278900
2017-02-05 21:58:00,875 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 21:59:54,046 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-005.pkl
2017-02-05 21:59:55,994 INFO     epoch: 5 took 115.122s (0.002s in batching)
training loss:	0.565874
validation loss:	0.547961
validation error:	0.270900
2017-02-05 21:59:55,997 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:01:45,948 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-006.pkl
2017-02-05 22:01:48,350 INFO     epoch: 6 took 112.355s (0.001s in batching)
training loss:	0.561541
validation loss:	0.547421
validation error:	0.275700
2017-02-05 22:01:48,353 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:03:52,932 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-007.pkl
2017-02-05 22:03:55,333 INFO     epoch: 7 took 126.983s (0.001s in batching)
training loss:	0.558373
validation loss:	0.544418
validation error:	0.272300
2017-02-05 22:03:55,335 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:06:00,102 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-008.pkl
2017-02-05 22:06:02,548 INFO     epoch: 8 took 127.215s (0.001s in batching)
training loss:	0.556967
validation loss:	0.539619
validation error:	0.269600
2017-02-05 22:06:02,550 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:08:07,481 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-009.pkl
2017-02-05 22:08:09,922 INFO     epoch: 9 took 127.374s (0.001s in batching)
training loss:	0.554209
validation loss:	0.539357
validation error:	0.268700
2017-02-05 22:08:09,925 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:10:14,637 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-010.pkl
2017-02-05 22:10:17,074 INFO     epoch: 10 took 127.152s (0.001s in batching)
training loss:	0.552834
validation loss:	0.538481
validation error:	0.268800
2017-02-05 22:10:17,077 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:12:21,666 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-011.pkl
2017-02-05 22:12:24,051 INFO     epoch: 11 took 126.977s (0.001s in batching)
training loss:	0.551236
validation loss:	0.534432
validation error:	0.265800
2017-02-05 22:12:24,054 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:14:29,630 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-012.pkl
2017-02-05 22:14:32,053 INFO     epoch: 12 took 128.001s (0.001s in batching)
training loss:	0.549540
validation loss:	0.534745
validation error:	0.263900
2017-02-05 22:14:32,056 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:16:36,592 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-013.pkl
2017-02-05 22:16:39,021 INFO     epoch: 13 took 126.968s (0.001s in batching)
training loss:	0.548384
validation loss:	0.537164
validation error:	0.265900
2017-02-05 22:16:39,024 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:18:43,888 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-014.pkl
2017-02-05 22:18:46,276 INFO     epoch: 14 took 127.254s (0.001s in batching)
training loss:	0.546899
validation loss:	0.537034
validation error:	0.266700
2017-02-05 22:18:46,279 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:20:51,228 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-015.pkl
2017-02-05 22:20:53,678 INFO     epoch: 15 took 127.402s (0.001s in batching)
training loss:	0.546102
validation loss:	0.532864
validation error:	0.263600
2017-02-05 22:20:53,682 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:22:58,435 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-016.pkl
2017-02-05 22:23:00,836 INFO     epoch: 16 took 127.158s (0.001s in batching)
training loss:	0.545095
validation loss:	0.534989
validation error:	0.264100
2017-02-05 22:23:00,839 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:25:05,770 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-017.pkl
2017-02-05 22:25:08,176 INFO     epoch: 17 took 127.340s (0.002s in batching)
training loss:	0.543654
validation loss:	0.528886
validation error:	0.264200
2017-02-05 22:25:08,180 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:27:12,711 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-018.pkl
2017-02-05 22:27:15,097 INFO     epoch: 18 took 126.921s (0.002s in batching)
training loss:	0.542570
validation loss:	0.532515
validation error:	0.263100
2017-02-05 22:27:15,100 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:29:19,975 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-019.pkl
2017-02-05 22:29:22,305 INFO     epoch: 19 took 127.208s (0.001s in batching)
training loss:	0.541213
validation loss:	0.530243
validation error:	0.260300
2017-02-05 22:29:22,308 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:31:27,116 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-020.pkl
2017-02-05 22:31:29,514 INFO     epoch: 20 took 127.208s (0.001s in batching)
training loss:	0.541134
validation loss:	0.527140
validation error:	0.259700
2017-02-05 22:31:29,517 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:33:33,888 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-021.pkl
2017-02-05 22:33:36,324 INFO     epoch: 21 took 126.811s (0.001s in batching)
training loss:	0.540089
validation loss:	0.528686
validation error:	0.262700
2017-02-05 22:33:36,328 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:35:41,319 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-022.pkl
2017-02-05 22:35:43,699 INFO     epoch: 22 took 127.374s (0.001s in batching)
training loss:	0.539062
validation loss:	0.527740
validation error:	0.263600
2017-02-05 22:35:43,703 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:37:40,219 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-023.pkl
2017-02-05 22:37:42,571 INFO     epoch: 23 took 118.873s (0.001s in batching)
training loss:	0.537954
validation loss:	0.524513
validation error:	0.258100
2017-02-05 22:37:42,575 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:39:46,661 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-024.pkl
2017-02-05 22:39:49,105 INFO     epoch: 24 took 126.533s (0.001s in batching)
training loss:	0.536775
validation loss:	0.525207
validation error:	0.259400
2017-02-05 22:39:49,108 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:41:53,413 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-025.pkl
2017-02-05 22:41:55,802 INFO     epoch: 25 took 126.697s (0.001s in batching)
training loss:	0.537044
validation loss:	0.524350
validation error:	0.261700
2017-02-05 22:41:55,807 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:44:00,268 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-026.pkl
2017-02-05 22:44:02,638 INFO     epoch: 26 took 126.836s (0.001s in batching)
training loss:	0.535944
validation loss:	0.531940
validation error:	0.258200
2017-02-05 22:44:02,642 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:46:06,701 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-027.pkl
2017-02-05 22:46:09,187 INFO     epoch: 27 took 126.549s (0.001s in batching)
training loss:	0.535226
validation loss:	0.528188
validation error:	0.263000
2017-02-05 22:46:09,191 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:48:13,472 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-028.pkl
2017-02-05 22:48:15,957 INFO     epoch: 28 took 126.770s (0.001s in batching)
training loss:	0.534079
validation loss:	0.521928
validation error:	0.256900
2017-02-05 22:48:15,961 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:50:19,813 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-029.pkl
2017-02-05 22:50:22,234 INFO     epoch: 29 took 126.277s (0.001s in batching)
training loss:	0.533590
validation loss:	0.535777
validation error:	0.263800
2017-02-05 22:50:22,238 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:52:27,249 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-030.pkl
2017-02-05 22:52:29,616 INFO     epoch: 30 took 127.381s (0.001s in batching)
training loss:	0.533329
validation loss:	0.521500
validation error:	0.257100
2017-02-05 22:52:29,620 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:54:34,073 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-031.pkl
2017-02-05 22:54:36,471 INFO     epoch: 31 took 126.856s (0.002s in batching)
training loss:	0.532442
validation loss:	0.522628
validation error:	0.257600
2017-02-05 22:54:36,475 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:56:41,068 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-032.pkl
2017-02-05 22:56:43,496 INFO     epoch: 32 took 127.025s (0.001s in batching)
training loss:	0.532703
validation loss:	0.522920
validation error:	0.259600
2017-02-05 22:56:43,500 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 22:58:47,336 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-033.pkl
2017-02-05 22:58:49,805 INFO     epoch: 33 took 126.309s (0.001s in batching)
training loss:	0.531937
validation loss:	0.519947
validation error:	0.256500
2017-02-05 22:58:49,809 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:00:53,919 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-034.pkl
2017-02-05 23:00:56,329 INFO     epoch: 34 took 126.524s (0.002s in batching)
training loss:	0.531666
validation loss:	0.519174
validation error:	0.259200
2017-02-05 23:00:56,333 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:03:00,473 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-035.pkl
2017-02-05 23:03:02,860 INFO     epoch: 35 took 126.531s (0.001s in batching)
training loss:	0.530253
validation loss:	0.522536
validation error:	0.257700
2017-02-05 23:03:02,864 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:05:07,488 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-036.pkl
2017-02-05 23:05:09,906 INFO     epoch: 36 took 127.046s (0.001s in batching)
training loss:	0.530056
validation loss:	0.520321
validation error:	0.257500
2017-02-05 23:05:09,910 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:07:14,418 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-037.pkl
2017-02-05 23:07:16,836 INFO     epoch: 37 took 126.930s (0.001s in batching)
training loss:	0.529228
validation loss:	0.520576
validation error:	0.259200
2017-02-05 23:07:16,840 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:09:21,140 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-038.pkl
2017-02-05 23:09:23,549 INFO     epoch: 38 took 126.713s (0.001s in batching)
training loss:	0.528954
validation loss:	0.521449
validation error:	0.254400
2017-02-05 23:09:23,553 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:11:27,818 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-039.pkl
2017-02-05 23:11:30,226 INFO     epoch: 39 took 126.677s (0.001s in batching)
training loss:	0.528576
validation loss:	0.520631
validation error:	0.257300
2017-02-05 23:11:30,230 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:13:34,726 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-040.pkl
2017-02-05 23:13:37,153 INFO     epoch: 40 took 126.926s (0.001s in batching)
training loss:	0.528436
validation loss:	0.522720
validation error:	0.261000
2017-02-05 23:13:37,157 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:15:41,853 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-041.pkl
2017-02-05 23:15:44,233 INFO     epoch: 41 took 127.080s (0.001s in batching)
training loss:	0.528303
validation loss:	0.518533
validation error:	0.256800
2017-02-05 23:15:44,238 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:17:48,665 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-042.pkl
2017-02-05 23:17:51,082 INFO     epoch: 42 took 126.850s (0.002s in batching)
training loss:	0.527127
validation loss:	0.519257
validation error:	0.257500
2017-02-05 23:17:51,087 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:19:56,156 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-043.pkl
2017-02-05 23:19:58,572 INFO     epoch: 43 took 127.490s (0.001s in batching)
training loss:	0.527255
validation loss:	0.518213
validation error:	0.255900
2017-02-05 23:19:58,577 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:22:03,417 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-044.pkl
2017-02-05 23:22:05,777 INFO     epoch: 44 took 127.205s (0.001s in batching)
training loss:	0.526582
validation loss:	0.516502
validation error:	0.255300
2017-02-05 23:22:05,782 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:24:10,464 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-045.pkl
2017-02-05 23:24:12,867 INFO     epoch: 45 took 127.090s (0.001s in batching)
training loss:	0.526624
validation loss:	0.516943
validation error:	0.254200
2017-02-05 23:24:12,872 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:26:17,315 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-046.pkl
2017-02-05 23:26:19,718 INFO     epoch: 46 took 126.851s (0.001s in batching)
training loss:	0.525856
validation loss:	0.517839
validation error:	0.257800
2017-02-05 23:26:19,722 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:28:24,373 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-047.pkl
2017-02-05 23:28:26,759 INFO     epoch: 47 took 127.041s (0.001s in batching)
training loss:	0.525832
validation loss:	0.519228
validation error:	0.253000
2017-02-05 23:28:26,763 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:30:31,451 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-048.pkl
2017-02-05 23:30:33,884 INFO     epoch: 48 took 127.125s (0.001s in batching)
training loss:	0.525049
validation loss:	0.518234
validation error:	0.256300
2017-02-05 23:30:33,888 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:32:37,866 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-049.pkl
2017-02-05 23:32:40,284 INFO     epoch: 49 took 126.400s (0.001s in batching)
training loss:	0.524736
validation loss:	0.513395
validation error:	0.253500
2017-02-05 23:32:40,289 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:34:44,360 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-050.pkl
2017-02-05 23:34:46,783 INFO     epoch: 50 took 126.499s (0.002s in batching)
training loss:	0.523948
validation loss:	0.513981
validation error:	0.253900
2017-02-05 23:34:46,787 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:36:51,195 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-051.pkl
2017-02-05 23:36:53,606 INFO     epoch: 51 took 126.823s (0.001s in batching)
training loss:	0.524019
validation loss:	0.514026
validation error:	0.252700
2017-02-05 23:36:53,610 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:38:58,097 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-052.pkl
2017-02-05 23:39:00,490 INFO     epoch: 52 took 126.884s (0.001s in batching)
training loss:	0.523523
validation loss:	0.514946
validation error:	0.255000
2017-02-05 23:39:00,494 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:41:04,901 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-053.pkl
2017-02-05 23:41:07,282 INFO     epoch: 53 took 126.793s (0.001s in batching)
training loss:	0.523218
validation loss:	0.512034
validation error:	0.250700
2017-02-05 23:41:07,287 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:43:11,979 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-054.pkl
2017-02-05 23:43:14,357 INFO     epoch: 54 took 127.075s (0.001s in batching)
training loss:	0.522868
validation loss:	0.514563
validation error:	0.251100
2017-02-05 23:43:14,363 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:45:18,739 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-055.pkl
2017-02-05 23:45:21,154 INFO     epoch: 55 took 126.797s (0.002s in batching)
training loss:	0.522635
validation loss:	0.514704
validation error:	0.251400
2017-02-05 23:45:21,159 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:47:25,475 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-056.pkl
2017-02-05 23:47:27,882 INFO     epoch: 56 took 126.727s (0.001s in batching)
training loss:	0.522603
validation loss:	0.517562
validation error:	0.254000
2017-02-05 23:47:27,886 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:49:32,925 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-057.pkl
2017-02-05 23:49:35,260 INFO     epoch: 57 took 127.378s (0.001s in batching)
training loss:	0.522661
validation loss:	0.516118
validation error:	0.256100
2017-02-05 23:49:35,264 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:51:39,281 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-058.pkl
2017-02-05 23:51:41,699 INFO     epoch: 58 took 126.440s (0.001s in batching)
training loss:	0.522399
validation loss:	0.512086
validation error:	0.251800
2017-02-05 23:51:41,704 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:53:38,348 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-059.pkl
2017-02-05 23:53:41,067 INFO     epoch: 59 took 119.367s (0.001s in batching)
training loss:	0.522252
validation loss:	0.516985
validation error:	0.252600
2017-02-05 23:53:41,215 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:55:38,892 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-060.pkl
2017-02-05 23:55:41,277 INFO     epoch: 60 took 120.211s (0.001s in batching)
training loss:	0.521481
validation loss:	0.512757
validation error:	0.252900
2017-02-05 23:55:41,291 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:57:44,709 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-061.pkl
2017-02-05 23:57:47,063 INFO     epoch: 61 took 125.786s (0.001s in batching)
training loss:	0.521397
validation loss:	0.514173
validation error:	0.254600
2017-02-05 23:57:47,068 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-05 23:59:49,905 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-062.pkl
2017-02-05 23:59:52,279 INFO     epoch: 62 took 125.216s (0.001s in batching)
training loss:	0.520544
validation loss:	0.516698
validation error:	0.254700
2017-02-05 23:59:52,284 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:01:56,156 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-063.pkl
2017-02-06 00:01:58,540 INFO     epoch: 63 took 126.261s (0.001s in batching)
training loss:	0.520871
validation loss:	0.514575
validation error:	0.252500
2017-02-06 00:01:58,545 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:04:01,824 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-064.pkl
2017-02-06 00:04:04,215 INFO     epoch: 64 took 125.675s (0.001s in batching)
training loss:	0.520951
validation loss:	0.513580
validation error:	0.250600
2017-02-06 00:04:04,222 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:06:07,773 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-065.pkl
2017-02-06 00:06:10,143 INFO     epoch: 65 took 125.928s (0.001s in batching)
training loss:	0.520496
validation loss:	0.511101
validation error:	0.251600
2017-02-06 00:06:10,150 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:08:13,291 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-066.pkl
2017-02-06 00:08:15,651 INFO     epoch: 66 took 125.508s (0.002s in batching)
training loss:	0.520231
validation loss:	0.518406
validation error:	0.256400
2017-02-06 00:08:15,657 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:10:18,645 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-067.pkl
2017-02-06 00:10:20,990 INFO     epoch: 67 took 125.339s (0.002s in batching)
training loss:	0.520037
validation loss:	0.517276
validation error:	0.251600
2017-02-06 00:10:20,996 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:12:24,141 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-068.pkl
2017-02-06 00:12:26,500 INFO     epoch: 68 took 125.510s (0.001s in batching)
training loss:	0.519636
validation loss:	0.514678
validation error:	0.256200
2017-02-06 00:12:26,505 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:14:29,039 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-069.pkl
2017-02-06 00:14:31,431 INFO     epoch: 69 took 124.931s (0.001s in batching)
training loss:	0.519401
validation loss:	0.514224
validation error:	0.254300
2017-02-06 00:14:31,436 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:16:34,728 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-070.pkl
2017-02-06 00:16:37,129 INFO     epoch: 70 took 125.697s (0.001s in batching)
training loss:	0.519216
validation loss:	0.521042
validation error:	0.259100
2017-02-06 00:16:37,134 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:18:41,383 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-071.pkl
2017-02-06 00:18:43,756 INFO     epoch: 71 took 126.627s (0.001s in batching)
training loss:	0.518659
validation loss:	0.512463
validation error:	0.251800
2017-02-06 00:18:43,761 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:20:46,681 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-072.pkl
2017-02-06 00:20:49,068 INFO     epoch: 72 took 125.312s (0.001s in batching)
training loss:	0.518450
validation loss:	0.511500
validation error:	0.250000
2017-02-06 00:20:49,073 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:22:52,599 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-073.pkl
2017-02-06 00:22:54,980 INFO     epoch: 73 took 125.913s (0.001s in batching)
training loss:	0.518403
validation loss:	0.510302
validation error:	0.250400
2017-02-06 00:22:54,986 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:24:50,113 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-074.pkl
2017-02-06 00:24:52,209 INFO     epoch: 74 took 117.228s (0.001s in batching)
training loss:	0.517939
validation loss:	0.517538
validation error:	0.255300
2017-02-06 00:24:52,214 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:26:48,722 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-075.pkl
2017-02-06 00:26:51,195 INFO     epoch: 75 took 118.986s (0.001s in batching)
training loss:	0.517770
validation loss:	0.511951
validation error:	0.249700
2017-02-06 00:26:51,201 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:28:58,711 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-076.pkl
2017-02-06 00:29:01,174 INFO     epoch: 76 took 129.980s (0.001s in batching)
training loss:	0.517340
validation loss:	0.509641
validation error:	0.250000
2017-02-06 00:29:01,180 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:31:06,841 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-077.pkl
2017-02-06 00:31:09,025 INFO     epoch: 77 took 127.851s (0.001s in batching)
training loss:	0.517883
validation loss:	0.514978
validation error:	0.252800
2017-02-06 00:31:09,031 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:33:18,634 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-078.pkl
2017-02-06 00:33:21,210 INFO     epoch: 78 took 132.185s (0.001s in batching)
training loss:	0.517200
validation loss:	0.511506
validation error:	0.251700
2017-02-06 00:33:21,216 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:35:30,679 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-079.pkl
2017-02-06 00:35:33,194 INFO     epoch: 79 took 131.984s (0.001s in batching)
training loss:	0.517952
validation loss:	0.512911
validation error:	0.252200
2017-02-06 00:35:33,199 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:37:43,162 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-080.pkl
2017-02-06 00:37:45,448 INFO     epoch: 80 took 132.254s (0.001s in batching)
training loss:	0.516941
validation loss:	0.511873
validation error:	0.250500
2017-02-06 00:37:45,454 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:39:55,257 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-081.pkl
2017-02-06 00:39:57,863 INFO     epoch: 81 took 132.415s (0.001s in batching)
training loss:	0.516489
validation loss:	0.513528
validation error:	0.251800
2017-02-06 00:39:57,869 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:42:07,894 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-082.pkl
2017-02-06 00:42:10,480 INFO     epoch: 82 took 132.617s (0.001s in batching)
training loss:	0.516590
validation loss:	0.513030
validation error:	0.252500
2017-02-06 00:42:10,486 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:44:20,240 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-083.pkl
2017-02-06 00:44:22,849 INFO     epoch: 83 took 132.369s (0.001s in batching)
training loss:	0.516230
validation loss:	0.514859
validation error:	0.250000
2017-02-06 00:44:22,855 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:46:32,281 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-084.pkl
2017-02-06 00:46:34,888 INFO     epoch: 84 took 132.039s (0.001s in batching)
training loss:	0.516274
validation loss:	0.513414
validation error:	0.255100
2017-02-06 00:46:34,895 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:48:44,552 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-085.pkl
2017-02-06 00:48:47,185 INFO     epoch: 85 took 132.297s (0.002s in batching)
training loss:	0.515522
validation loss:	0.510920
validation error:	0.247900
2017-02-06 00:48:47,192 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:50:56,748 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-086.pkl
2017-02-06 00:50:59,362 INFO     epoch: 86 took 132.177s (0.001s in batching)
training loss:	0.516259
validation loss:	0.517629
validation error:	0.254200
2017-02-06 00:50:59,367 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:53:09,201 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-087.pkl
2017-02-06 00:53:11,479 INFO     epoch: 87 took 132.118s (0.001s in batching)
training loss:	0.515489
validation loss:	0.511931
validation error:	0.252700
2017-02-06 00:53:11,485 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:55:20,816 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-088.pkl
2017-02-06 00:55:23,440 INFO     epoch: 88 took 131.961s (0.001s in batching)
training loss:	0.515200
validation loss:	0.510992
validation error:	0.250800
2017-02-06 00:55:23,446 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:57:33,819 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-089.pkl
2017-02-06 00:57:36,414 INFO     epoch: 89 took 132.974s (0.001s in batching)
training loss:	0.514861
validation loss:	0.514024
validation error:	0.253500
2017-02-06 00:57:36,421 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 00:59:46,281 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-090.pkl
2017-02-06 00:59:48,896 INFO     epoch: 90 took 132.482s (0.001s in batching)
training loss:	0.514361
validation loss:	0.514639
validation error:	0.253500
2017-02-06 00:59:48,902 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 01:01:58,077 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-091.pkl
2017-02-06 01:02:00,702 INFO     epoch: 91 took 131.806s (0.001s in batching)
training loss:	0.514613
validation loss:	0.513085
validation error:	0.251800
2017-02-06 01:02:00,709 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 01:04:10,601 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-092.pkl
2017-02-06 01:04:13,200 INFO     epoch: 92 took 132.498s (0.001s in batching)
training loss:	0.514920
validation loss:	0.513241
validation error:	0.248200
2017-02-06 01:04:13,207 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 01:06:23,352 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-093.pkl
2017-02-06 01:06:25,565 INFO     epoch: 93 took 132.365s (0.001s in batching)
training loss:	0.514343
validation loss:	0.513572
validation error:	0.248800
2017-02-06 01:06:25,572 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 01:08:35,536 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-094.pkl
2017-02-06 01:08:38,125 INFO     epoch: 94 took 132.560s (0.001s in batching)
training loss:	0.514044
validation loss:	0.512065
validation error:	0.250900
2017-02-06 01:08:38,133 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 01:10:47,362 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-095.pkl
2017-02-06 01:10:49,974 INFO     epoch: 95 took 131.849s (0.002s in batching)
training loss:	0.514313
validation loss:	0.513428
validation error:	0.254700
2017-02-06 01:10:49,982 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 01:13:00,238 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-096.pkl
2017-02-06 01:13:02,849 INFO     epoch: 96 took 132.875s (0.002s in batching)
training loss:	0.513643
validation loss:	0.515466
validation error:	0.255400
2017-02-06 01:13:02,856 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 01:15:12,924 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-097.pkl
2017-02-06 01:15:15,503 INFO     epoch: 97 took 132.654s (0.002s in batching)
training loss:	0.512963
validation loss:	0.515993
validation error:	0.253300
2017-02-06 01:15:15,509 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 01:17:25,520 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-098.pkl
2017-02-06 01:17:28,120 INFO     epoch: 98 took 132.617s (0.001s in batching)
training loss:	0.513187
validation loss:	0.512534
validation error:	0.251000
2017-02-06 01:17:28,126 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
2017-02-06 01:19:38,055 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/epoch-099.pkl
2017-02-06 01:19:40,492 INFO     epoch: 99 took 132.373s (0.001s in batching)
training loss:	0.513250
validation loss:	0.514982
validation error:	0.253200
2017-02-06 01:19:40,499 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100-50/config.json
