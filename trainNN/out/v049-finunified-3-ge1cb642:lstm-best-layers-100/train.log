2017-02-05 21:47:05,713 DEBUG    version=v049-finunified-3-ge1cb642:lstm-best-layers-100
2017-02-05 21:47:05,715 INFO     Open DBase(data/db/all240302-spk.dat, data/db/all240302-spk.idx, mode='r')
2017-02-05 21:47:17,986 DEBUG    loading cached extracted data from data/cache/extract-7a00b23850576d60137fa78432d8dfda09ea0f343092bd09848f21113b1acfd0.pickle
2017-02-05 21:47:19,364 DEBUG    set input dim to 9
2017-02-05 21:47:19,366 DEBUG    input dim = 9
2017-02-05 21:47:19,385 DEBUG    shuffling batches
2017-02-05 21:47:24,644 DEBUG    shuffling done
2017-02-05 21:47:24,644 DEBUG    loading data into ram
2017-02-05 21:47:24,786 DEBUG    loading data took 0.142s (cpu: 0.142s)
2017-02-05 21:47:25,121 DEBUG    set input dim to 9
2017-02-05 21:47:25,121 DEBUG    input dim = 9
2017-02-05 21:47:25,153 DEBUG    shuffling batches
2017-02-05 21:47:25,668 DEBUG    shuffling done
2017-02-05 21:47:25,668 DEBUG    loading data into ram
2017-02-05 21:47:25,683 DEBUG    loading data took 0.014s (cpu: 0.014s)
2017-02-05 21:47:25,761 DEBUG    Applying L2 regularization with 0.000100
2017-02-05 21:47:25,832 INFO     Training network with 44502 trainable out of 44702 total params.
2017-02-05 21:47:26,793 DEBUG    Using adam with learning_rate=0.001000
2017-02-05 21:48:33,157 DEBUG    Starting training...
2017-02-05 21:49:45,671 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-000.pkl
2017-02-05 21:49:47,215 INFO     epoch: 0 took 74.057s (0.001s in batching)
training loss:	0.616750
validation loss:	0.572155
validation error:	0.298100
2017-02-05 21:49:47,218 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 21:50:56,495 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-001.pkl
2017-02-05 21:50:57,809 INFO     epoch: 1 took 70.595s (0.001s in batching)
training loss:	0.589108
validation loss:	0.566036
validation error:	0.288200
2017-02-05 21:50:57,812 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 21:52:05,534 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-002.pkl
2017-02-05 21:52:07,104 INFO     epoch: 2 took 69.295s (0.001s in batching)
training loss:	0.581465
validation loss:	0.563224
validation error:	0.291000
2017-02-05 21:52:07,107 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 21:53:20,198 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-003.pkl
2017-02-05 21:53:21,754 INFO     epoch: 3 took 74.650s (0.001s in batching)
training loss:	0.575473
validation loss:	0.559246
validation error:	0.285800
2017-02-05 21:53:21,858 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 21:54:35,429 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-004.pkl
2017-02-05 21:54:36,996 INFO     epoch: 4 took 75.242s (0.001s in batching)
training loss:	0.569211
validation loss:	0.555057
validation error:	0.279800
2017-02-05 21:54:36,999 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 21:55:49,819 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-005.pkl
2017-02-05 21:55:51,377 INFO     epoch: 5 took 74.381s (0.001s in batching)
training loss:	0.565342
validation loss:	0.552139
validation error:	0.277700
2017-02-05 21:55:51,380 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 21:57:04,469 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-006.pkl
2017-02-05 21:57:06,017 INFO     epoch: 6 took 74.640s (0.001s in batching)
training loss:	0.561912
validation loss:	0.550273
validation error:	0.278000
2017-02-05 21:57:06,020 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 21:58:19,266 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-007.pkl
2017-02-05 21:58:20,814 INFO     epoch: 7 took 74.797s (0.001s in batching)
training loss:	0.559014
validation loss:	0.549451
validation error:	0.270800
2017-02-05 21:58:20,817 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 21:59:29,361 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-008.pkl
2017-02-05 21:59:30,561 INFO     epoch: 8 took 69.747s (0.001s in batching)
training loss:	0.557600
validation loss:	0.544506
validation error:	0.271300
2017-02-05 21:59:30,564 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:00:27,170 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-009.pkl
2017-02-05 22:00:28,363 INFO     epoch: 9 took 57.802s (0.001s in batching)
training loss:	0.555917
validation loss:	0.549117
validation error:	0.277700
2017-02-05 22:00:28,366 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:01:34,904 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-010.pkl
2017-02-05 22:01:36,472 INFO     epoch: 10 took 68.110s (0.001s in batching)
training loss:	0.554274
validation loss:	0.544089
validation error:	0.270400
2017-02-05 22:01:36,476 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:02:49,942 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-011.pkl
2017-02-05 22:02:51,507 INFO     epoch: 11 took 75.035s (0.001s in batching)
training loss:	0.553616
validation loss:	0.545570
validation error:	0.273000
2017-02-05 22:02:51,510 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:04:04,839 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-012.pkl
2017-02-05 22:04:06,375 INFO     epoch: 12 took 74.868s (0.001s in batching)
training loss:	0.552110
validation loss:	0.539742
validation error:	0.269600
2017-02-05 22:04:06,378 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:05:19,778 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-013.pkl
2017-02-05 22:05:21,349 INFO     epoch: 13 took 74.974s (0.001s in batching)
training loss:	0.551110
validation loss:	0.540198
validation error:	0.269300
2017-02-05 22:05:21,352 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:06:34,525 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-014.pkl
2017-02-05 22:06:36,109 INFO     epoch: 14 took 74.760s (0.001s in batching)
training loss:	0.549986
validation loss:	0.538950
validation error:	0.269700
2017-02-05 22:06:36,112 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:07:49,507 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-015.pkl
2017-02-05 22:07:51,082 INFO     epoch: 15 took 74.973s (0.001s in batching)
training loss:	0.548851
validation loss:	0.541672
validation error:	0.268000
2017-02-05 22:07:51,085 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:09:04,332 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-016.pkl
2017-02-05 22:09:05,906 INFO     epoch: 16 took 74.824s (0.001s in batching)
training loss:	0.547995
validation loss:	0.536335
validation error:	0.267800
2017-02-05 22:09:05,909 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:10:18,921 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-017.pkl
2017-02-05 22:10:20,484 INFO     epoch: 17 took 74.578s (0.001s in batching)
training loss:	0.546647
validation loss:	0.537183
validation error:	0.265700
2017-02-05 22:10:20,487 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:11:33,569 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-018.pkl
2017-02-05 22:11:35,127 INFO     epoch: 18 took 74.643s (0.001s in batching)
training loss:	0.546054
validation loss:	0.537334
validation error:	0.264800
2017-02-05 22:11:35,131 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:12:48,489 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-019.pkl
2017-02-05 22:12:50,050 INFO     epoch: 19 took 74.923s (0.001s in batching)
training loss:	0.545225
validation loss:	0.536158
validation error:	0.266700
2017-02-05 22:12:50,053 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:14:03,441 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-020.pkl
2017-02-05 22:14:04,995 INFO     epoch: 20 took 74.945s (0.001s in batching)
training loss:	0.544019
validation loss:	0.532942
validation error:	0.262900
2017-02-05 22:14:04,998 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:15:18,388 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-021.pkl
2017-02-05 22:15:19,962 INFO     epoch: 21 took 74.967s (0.001s in batching)
training loss:	0.543012
validation loss:	0.535372
validation error:	0.268900
2017-02-05 22:15:19,966 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:16:33,371 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-022.pkl
2017-02-05 22:16:34,941 INFO     epoch: 22 took 74.979s (0.001s in batching)
training loss:	0.541702
validation loss:	0.531043
validation error:	0.264300
2017-02-05 22:16:34,945 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:17:48,288 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-023.pkl
2017-02-05 22:17:49,869 INFO     epoch: 23 took 74.928s (0.001s in batching)
training loss:	0.540905
validation loss:	0.531129
validation error:	0.265600
2017-02-05 22:17:49,873 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:19:03,077 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-024.pkl
2017-02-05 22:19:04,671 INFO     epoch: 24 took 74.802s (0.001s in batching)
training loss:	0.539489
validation loss:	0.529337
validation error:	0.261200
2017-02-05 22:19:04,674 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:20:18,562 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-025.pkl
2017-02-05 22:20:20,118 INFO     epoch: 25 took 75.448s (0.001s in batching)
training loss:	0.539286
validation loss:	0.526515
validation error:	0.260100
2017-02-05 22:20:20,122 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:21:33,610 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-026.pkl
2017-02-05 22:21:35,167 INFO     epoch: 26 took 75.049s (0.001s in batching)
training loss:	0.538627
validation loss:	0.527157
validation error:	0.264800
2017-02-05 22:21:35,171 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:22:48,723 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-027.pkl
2017-02-05 22:22:50,296 INFO     epoch: 27 took 75.129s (0.001s in batching)
training loss:	0.537795
validation loss:	0.526818
validation error:	0.259900
2017-02-05 22:22:50,301 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:24:04,189 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-028.pkl
2017-02-05 22:24:05,753 INFO     epoch: 28 took 75.457s (0.001s in batching)
training loss:	0.536582
validation loss:	0.526014
validation error:	0.262100
2017-02-05 22:24:05,758 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:25:19,338 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-029.pkl
2017-02-05 22:25:20,910 INFO     epoch: 29 took 75.156s (0.001s in batching)
training loss:	0.535860
validation loss:	0.527259
validation error:	0.260700
2017-02-05 22:25:20,914 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:26:34,205 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-030.pkl
2017-02-05 22:26:35,755 INFO     epoch: 30 took 74.845s (0.001s in batching)
training loss:	0.535809
validation loss:	0.528033
validation error:	0.264600
2017-02-05 22:26:35,760 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:27:49,197 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-031.pkl
2017-02-05 22:27:50,755 INFO     epoch: 31 took 75.000s (0.001s in batching)
training loss:	0.534267
validation loss:	0.527813
validation error:	0.265300
2017-02-05 22:27:50,760 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:29:04,201 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-032.pkl
2017-02-05 22:29:05,760 INFO     epoch: 32 took 75.005s (0.001s in batching)
training loss:	0.533738
validation loss:	0.525062
validation error:	0.262100
2017-02-05 22:29:05,764 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:30:18,978 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-033.pkl
2017-02-05 22:30:20,535 INFO     epoch: 33 took 74.775s (0.001s in batching)
training loss:	0.533503
validation loss:	0.524705
validation error:	0.262300
2017-02-05 22:30:20,539 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:31:34,158 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-034.pkl
2017-02-05 22:31:35,694 INFO     epoch: 34 took 75.159s (0.001s in batching)
training loss:	0.532494
validation loss:	0.524144
validation error:	0.260500
2017-02-05 22:31:35,698 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:32:49,210 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-035.pkl
2017-02-05 22:32:50,793 INFO     epoch: 35 took 75.099s (0.001s in batching)
training loss:	0.531218
validation loss:	0.523685
validation error:	0.256400
2017-02-05 22:32:50,797 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:34:04,296 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-036.pkl
2017-02-05 22:34:05,884 INFO     epoch: 36 took 75.091s (0.001s in batching)
training loss:	0.531413
validation loss:	0.523048
validation error:	0.263900
2017-02-05 22:34:05,888 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:35:19,223 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-037.pkl
2017-02-05 22:35:20,773 INFO     epoch: 37 took 74.890s (0.001s in batching)
training loss:	0.530210
validation loss:	0.520993
validation error:	0.258000
2017-02-05 22:35:20,779 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:36:34,134 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-038.pkl
2017-02-05 22:36:35,707 INFO     epoch: 38 took 74.934s (0.001s in batching)
training loss:	0.530035
validation loss:	0.521831
validation error:	0.257200
2017-02-05 22:36:35,713 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:37:41,032 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-039.pkl
2017-02-05 22:37:42,506 INFO     epoch: 39 took 66.799s (0.001s in batching)
training loss:	0.529009
validation loss:	0.520267
validation error:	0.255600
2017-02-05 22:37:42,511 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:38:55,116 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-040.pkl
2017-02-05 22:38:56,699 INFO     epoch: 40 took 74.192s (0.001s in batching)
training loss:	0.528912
validation loss:	0.522039
validation error:	0.258500
2017-02-05 22:38:56,703 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:40:10,061 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-041.pkl
2017-02-05 22:40:11,634 INFO     epoch: 41 took 74.935s (0.001s in batching)
training loss:	0.528425
validation loss:	0.520247
validation error:	0.256400
2017-02-05 22:40:11,639 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:41:24,553 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-042.pkl
2017-02-05 22:41:26,128 INFO     epoch: 42 took 74.494s (0.001s in batching)
training loss:	0.527871
validation loss:	0.517531
validation error:	0.254200
2017-02-05 22:41:26,132 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:42:39,233 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-043.pkl
2017-02-05 22:42:40,783 INFO     epoch: 43 took 74.655s (0.001s in batching)
training loss:	0.526830
validation loss:	0.519858
validation error:	0.257100
2017-02-05 22:42:40,787 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:43:53,942 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-044.pkl
2017-02-05 22:43:55,489 INFO     epoch: 44 took 74.706s (0.001s in batching)
training loss:	0.526572
validation loss:	0.517733
validation error:	0.254300
2017-02-05 22:43:55,494 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:45:08,143 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-045.pkl
2017-02-05 22:45:09,697 INFO     epoch: 45 took 74.208s (0.001s in batching)
training loss:	0.526384
validation loss:	0.520231
validation error:	0.254400
2017-02-05 22:45:09,702 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:46:21,915 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-046.pkl
2017-02-05 22:46:23,495 INFO     epoch: 46 took 73.798s (0.001s in batching)
training loss:	0.525662
validation loss:	0.516619
validation error:	0.250800
2017-02-05 22:46:23,500 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:47:36,621 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-047.pkl
2017-02-05 22:47:38,186 INFO     epoch: 47 took 74.691s (0.001s in batching)
training loss:	0.525626
validation loss:	0.516288
validation error:	0.253800
2017-02-05 22:47:38,190 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:48:50,328 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-048.pkl
2017-02-05 22:48:51,884 INFO     epoch: 48 took 73.698s (0.001s in batching)
training loss:	0.524863
validation loss:	0.517111
validation error:	0.258600
2017-02-05 22:48:51,890 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:50:05,075 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-049.pkl
2017-02-05 22:50:06,625 INFO     epoch: 49 took 74.741s (0.001s in batching)
training loss:	0.524844
validation loss:	0.524150
validation error:	0.260700
2017-02-05 22:50:06,630 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:51:19,660 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-050.pkl
2017-02-05 22:51:21,232 INFO     epoch: 50 took 74.607s (0.001s in batching)
training loss:	0.524703
validation loss:	0.522218
validation error:	0.259500
2017-02-05 22:51:21,236 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:52:34,602 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-051.pkl
2017-02-05 22:52:36,156 INFO     epoch: 51 took 74.924s (0.001s in batching)
training loss:	0.523598
validation loss:	0.515696
validation error:	0.254100
2017-02-05 22:52:36,161 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:53:49,682 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-052.pkl
2017-02-05 22:53:51,258 INFO     epoch: 52 took 75.102s (0.001s in batching)
training loss:	0.523680
validation loss:	0.517723
validation error:	0.257800
2017-02-05 22:53:51,263 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:55:04,561 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-053.pkl
2017-02-05 22:55:06,118 INFO     epoch: 53 took 74.860s (0.001s in batching)
training loss:	0.523276
validation loss:	0.516683
validation error:	0.254700
2017-02-05 22:55:06,122 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:56:19,585 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-054.pkl
2017-02-05 22:56:21,110 INFO     epoch: 54 took 74.992s (0.001s in batching)
training loss:	0.523137
validation loss:	0.514375
validation error:	0.251700
2017-02-05 22:56:21,115 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:57:34,243 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-055.pkl
2017-02-05 22:57:35,783 INFO     epoch: 55 took 74.673s (0.002s in batching)
training loss:	0.522450
validation loss:	0.518530
validation error:	0.255800
2017-02-05 22:57:35,788 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 22:58:48,911 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-056.pkl
2017-02-05 22:58:50,480 INFO     epoch: 56 took 74.697s (0.001s in batching)
training loss:	0.521963
validation loss:	0.521632
validation error:	0.257100
2017-02-05 22:58:50,487 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:00:03,465 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-057.pkl
2017-02-05 23:00:05,045 INFO     epoch: 57 took 74.565s (0.001s in batching)
training loss:	0.522103
validation loss:	0.517104
validation error:	0.255000
2017-02-05 23:00:05,051 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:01:18,150 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-058.pkl
2017-02-05 23:01:19,701 INFO     epoch: 58 took 74.656s (0.001s in batching)
training loss:	0.521440
validation loss:	0.518520
validation error:	0.255700
2017-02-05 23:01:19,706 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:02:32,673 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-059.pkl
2017-02-05 23:02:34,271 INFO     epoch: 59 took 74.570s (0.001s in batching)
training loss:	0.521431
validation loss:	0.517653
validation error:	0.252900
2017-02-05 23:02:34,277 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:03:47,503 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-060.pkl
2017-02-05 23:03:49,065 INFO     epoch: 60 took 74.794s (0.001s in batching)
training loss:	0.521101
validation loss:	0.514369
validation error:	0.253100
2017-02-05 23:03:49,070 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:05:02,090 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-061.pkl
2017-02-05 23:05:03,644 INFO     epoch: 61 took 74.579s (0.001s in batching)
training loss:	0.520392
validation loss:	0.515046
validation error:	0.254600
2017-02-05 23:05:03,649 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:06:16,890 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-062.pkl
2017-02-05 23:06:18,454 INFO     epoch: 62 took 74.811s (0.001s in batching)
training loss:	0.520394
validation loss:	0.515872
validation error:	0.253600
2017-02-05 23:06:18,460 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:07:31,763 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-063.pkl
2017-02-05 23:07:33,305 INFO     epoch: 63 took 74.851s (0.001s in batching)
training loss:	0.520139
validation loss:	0.515532
validation error:	0.251900
2017-02-05 23:07:33,310 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:08:46,784 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-064.pkl
2017-02-05 23:08:48,359 INFO     epoch: 64 took 75.053s (0.001s in batching)
training loss:	0.519809
validation loss:	0.519171
validation error:	0.255400
2017-02-05 23:08:48,364 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:10:01,727 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-065.pkl
2017-02-05 23:10:03,296 INFO     epoch: 65 took 74.938s (0.001s in batching)
training loss:	0.520105
validation loss:	0.515336
validation error:	0.254800
2017-02-05 23:10:03,301 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:11:16,538 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-066.pkl
2017-02-05 23:11:18,103 INFO     epoch: 66 took 74.807s (0.001s in batching)
training loss:	0.518985
validation loss:	0.515255
validation error:	0.251400
2017-02-05 23:11:18,112 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:12:31,379 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-067.pkl
2017-02-05 23:12:32,928 INFO     epoch: 67 took 74.826s (0.001s in batching)
training loss:	0.519999
validation loss:	0.514819
validation error:	0.254100
2017-02-05 23:12:32,934 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:13:46,134 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-068.pkl
2017-02-05 23:13:47,695 INFO     epoch: 68 took 74.767s (0.001s in batching)
training loss:	0.518375
validation loss:	0.516731
validation error:	0.252200
2017-02-05 23:13:47,701 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:15:00,813 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-069.pkl
2017-02-05 23:15:02,371 INFO     epoch: 69 took 74.676s (0.001s in batching)
training loss:	0.518176
validation loss:	0.513969
validation error:	0.253500
2017-02-05 23:15:02,377 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:16:15,484 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-070.pkl
2017-02-05 23:16:17,044 INFO     epoch: 70 took 74.673s (0.001s in batching)
training loss:	0.517526
validation loss:	0.519516
validation error:	0.255600
2017-02-05 23:16:17,049 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:17:30,192 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-071.pkl
2017-02-05 23:17:31,765 INFO     epoch: 71 took 74.720s (0.001s in batching)
training loss:	0.517796
validation loss:	0.514850
validation error:	0.249600
2017-02-05 23:17:31,771 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:18:45,150 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-072.pkl
2017-02-05 23:18:46,727 INFO     epoch: 72 took 74.963s (0.001s in batching)
training loss:	0.517311
validation loss:	0.515587
validation error:	0.254300
2017-02-05 23:18:46,733 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:19:59,974 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-073.pkl
2017-02-05 23:20:01,530 INFO     epoch: 73 took 74.802s (0.001s in batching)
training loss:	0.517485
validation loss:	0.516083
validation error:	0.255300
2017-02-05 23:20:01,535 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:21:15,099 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-074.pkl
2017-02-05 23:21:16,650 INFO     epoch: 74 took 75.120s (0.001s in batching)
training loss:	0.516758
validation loss:	0.516762
validation error:	0.256700
2017-02-05 23:21:16,656 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:22:30,313 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-075.pkl
2017-02-05 23:22:31,910 INFO     epoch: 75 took 75.260s (0.001s in batching)
training loss:	0.516581
validation loss:	0.518348
validation error:	0.256200
2017-02-05 23:22:31,915 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:23:45,115 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-076.pkl
2017-02-05 23:23:46,678 INFO     epoch: 76 took 74.768s (0.001s in batching)
training loss:	0.516804
validation loss:	0.515402
validation error:	0.253100
2017-02-05 23:23:46,684 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:24:59,905 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-077.pkl
2017-02-05 23:25:01,454 INFO     epoch: 77 took 74.776s (0.001s in batching)
training loss:	0.516206
validation loss:	0.521560
validation error:	0.259300
2017-02-05 23:25:01,460 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:26:14,895 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-078.pkl
2017-02-05 23:26:16,432 INFO     epoch: 78 took 74.979s (0.001s in batching)
training loss:	0.516301
validation loss:	0.516163
validation error:	0.252800
2017-02-05 23:26:16,440 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:27:29,641 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-079.pkl
2017-02-05 23:27:31,184 INFO     epoch: 79 took 74.752s (0.001s in batching)
training loss:	0.516217
validation loss:	0.515011
validation error:	0.253300
2017-02-05 23:27:31,190 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:28:44,565 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-080.pkl
2017-02-05 23:28:46,128 INFO     epoch: 80 took 74.944s (0.001s in batching)
training loss:	0.516062
validation loss:	0.515971
validation error:	0.253100
2017-02-05 23:28:46,135 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:29:59,290 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-081.pkl
2017-02-05 23:30:00,864 INFO     epoch: 81 took 74.736s (0.001s in batching)
training loss:	0.514571
validation loss:	0.516898
validation error:	0.251000
2017-02-05 23:30:00,870 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:31:14,187 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-082.pkl
2017-02-05 23:31:15,730 INFO     epoch: 82 took 74.866s (0.001s in batching)
training loss:	0.515148
validation loss:	0.514561
validation error:	0.253600
2017-02-05 23:31:15,736 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:32:28,760 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-083.pkl
2017-02-05 23:32:30,309 INFO     epoch: 83 took 74.579s (0.001s in batching)
training loss:	0.514581
validation loss:	0.520620
validation error:	0.257700
2017-02-05 23:32:30,315 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:33:43,689 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-084.pkl
2017-02-05 23:33:45,271 INFO     epoch: 84 took 74.962s (0.001s in batching)
training loss:	0.515206
validation loss:	0.517091
validation error:	0.251600
2017-02-05 23:33:45,279 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:34:58,252 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-085.pkl
2017-02-05 23:34:59,834 INFO     epoch: 85 took 74.563s (0.001s in batching)
training loss:	0.514360
validation loss:	0.516776
validation error:	0.253400
2017-02-05 23:34:59,840 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:36:12,659 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-086.pkl
2017-02-05 23:36:14,238 INFO     epoch: 86 took 74.404s (0.001s in batching)
training loss:	0.514186
validation loss:	0.518797
validation error:	0.254900
2017-02-05 23:36:14,245 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:37:27,312 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-087.pkl
2017-02-05 23:37:28,865 INFO     epoch: 87 took 74.627s (0.001s in batching)
training loss:	0.514116
validation loss:	0.517250
validation error:	0.251500
2017-02-05 23:37:28,873 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:38:42,087 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-088.pkl
2017-02-05 23:38:43,668 INFO     epoch: 88 took 74.803s (0.001s in batching)
training loss:	0.513977
validation loss:	0.514743
validation error:	0.250400
2017-02-05 23:38:43,674 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:39:56,677 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-089.pkl
2017-02-05 23:39:58,249 INFO     epoch: 89 took 74.581s (0.001s in batching)
training loss:	0.514094
validation loss:	0.516830
validation error:	0.252900
2017-02-05 23:39:58,257 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:41:11,450 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-090.pkl
2017-02-05 23:41:12,997 INFO     epoch: 90 took 74.749s (0.001s in batching)
training loss:	0.513311
validation loss:	0.522839
validation error:	0.255500
2017-02-05 23:41:13,004 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:42:26,332 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-091.pkl
2017-02-05 23:42:27,892 INFO     epoch: 91 took 74.894s (0.001s in batching)
training loss:	0.513419
validation loss:	0.517346
validation error:	0.252900
2017-02-05 23:42:27,898 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:43:41,951 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-092.pkl
2017-02-05 23:43:43,512 INFO     epoch: 92 took 75.620s (0.001s in batching)
training loss:	0.513829
validation loss:	0.520506
validation error:	0.256000
2017-02-05 23:43:43,519 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:44:56,319 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-093.pkl
2017-02-05 23:44:57,875 INFO     epoch: 93 took 74.364s (0.001s in batching)
training loss:	0.512955
validation loss:	0.516661
validation error:	0.253700
2017-02-05 23:44:57,882 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:46:11,103 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-094.pkl
2017-02-05 23:46:12,632 INFO     epoch: 94 took 74.757s (0.001s in batching)
training loss:	0.513280
validation loss:	0.519401
validation error:	0.252800
2017-02-05 23:46:12,638 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:47:25,768 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-095.pkl
2017-02-05 23:47:27,299 INFO     epoch: 95 took 74.666s (0.001s in batching)
training loss:	0.512664
validation loss:	0.515601
validation error:	0.254700
2017-02-05 23:47:27,305 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:48:40,864 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-096.pkl
2017-02-05 23:48:42,406 INFO     epoch: 96 took 75.107s (0.001s in batching)
training loss:	0.512373
validation loss:	0.514812
validation error:	0.253500
2017-02-05 23:48:42,412 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:49:55,747 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-097.pkl
2017-02-05 23:49:57,316 INFO     epoch: 97 took 74.910s (0.001s in batching)
training loss:	0.512514
validation loss:	0.518064
validation error:	0.251200
2017-02-05 23:49:57,324 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:51:10,816 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-098.pkl
2017-02-05 23:51:12,399 INFO     epoch: 98 took 75.083s (0.002s in batching)
training loss:	0.511879
validation loss:	0.516475
validation error:	0.252300
2017-02-05 23:51:12,405 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
2017-02-05 23:52:25,589 INFO     Saving network params to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/epoch-099.pkl
2017-02-05 23:52:27,126 INFO     epoch: 99 took 74.727s (0.001s in batching)
training loss:	0.512622
validation loss:	0.517126
validation error:	0.250300
2017-02-05 23:52:27,133 INFO     Wrote output to trainNN/out/v049-finunified-3-ge1cb642:lstm-best-layers-100/config.json
