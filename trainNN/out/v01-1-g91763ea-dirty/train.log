2016-11-22 01:10:42 >>> loading config from ../extract_pfiles_python/out/v1-1-g91763ea-context40/train-config.json
2016-11-22 01:10:42 >>> loading numpy file ../extract_pfiles_python/out/v1-1-g91763ea-context40/train.npz
2016-11-22 01:10:45 >>> loading numpy file ../extract_pfiles_python/out/v1-1-g91763ea-context40/validate.npz
2016-11-22 01:10:45 >>> Using fuzzy_newbob as schedulung method.
2016-11-22 01:10:45 >>> Training network with 21452 trainable out of 21452 total params.
2016-11-22 01:10:45 >>> Using adadelta with learning_rate=1.000000
2016-11-22 01:10:45 >>> Compiling theano functions...
2016-11-22 01:10:46 >>> Starting training...
2016-11-22 01:11:03 >>>   training loss:	0.671648
2016-11-22 01:11:03 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-0.pkl
2016-11-22 01:11:05 >>> epoch: 0 validation error:		0.430619
2016-11-22 01:11:21 >>>   training loss:	0.622934
2016-11-22 01:11:21 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-1.pkl
2016-11-22 01:11:22 >>> epoch: 1 validation error:		0.339726
2016-11-22 01:11:39 >>>   training loss:	0.587205
2016-11-22 01:11:39 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-2.pkl
2016-11-22 01:11:40 >>> epoch: 2 validation error:		0.279194
2016-11-22 01:11:56 >>>   training loss:	0.556922
2016-11-22 01:11:56 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-3.pkl
2016-11-22 01:11:58 >>> epoch: 3 validation error:		0.177774
2016-11-22 01:12:15 >>>   training loss:	0.535723
2016-11-22 01:12:15 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-4.pkl
2016-11-22 01:12:16 >>> epoch: 4 validation error:		0.182879
2016-11-22 01:12:16 >>> fuzzy_newbob: Setting learning rate to: 0.500000
2016-11-22 01:12:16 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-3.pkl
2016-11-22 01:12:16 >>> Updating adadelta with learning_rate=0.500000
2016-11-22 01:12:16 >>> Re-compiling train function...
2016-11-22 01:12:36 >>>   training loss:	0.432723
2016-11-22 01:12:36 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-5.pkl
2016-11-22 01:12:37 >>> epoch: 5 validation error:		0.226849
2016-11-22 01:12:37 >>> fuzzy_newbob: Setting learning rate to: 0.250000
2016-11-22 01:12:37 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-3.pkl
2016-11-22 01:12:37 >>> Updating adadelta with learning_rate=0.250000
2016-11-22 01:12:37 >>> Re-compiling train function...
2016-11-22 01:12:55 >>>   training loss:	0.357353
2016-11-22 01:12:55 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-6.pkl
2016-11-22 01:12:56 >>> epoch: 6 validation error:		0.129479
2016-11-22 01:13:13 >>>   training loss:	0.296851
2016-11-22 01:13:13 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-7.pkl
2016-11-22 01:13:14 >>> epoch: 7 validation error:		0.113839
2016-11-22 01:13:31 >>>   training loss:	0.269759
2016-11-22 01:13:31 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-8.pkl
2016-11-22 01:13:32 >>> epoch: 8 validation error:		0.076847
2016-11-22 01:13:48 >>>   training loss:	0.258582
2016-11-22 01:13:48 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-9.pkl
2016-11-22 01:13:49 >>> epoch: 9 validation error:		0.130465
2016-11-22 01:13:49 >>> fuzzy_newbob: Setting learning rate to: 0.125000
2016-11-22 01:13:49 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-8.pkl
2016-11-22 01:13:49 >>> Updating adadelta with learning_rate=0.125000
2016-11-22 01:13:49 >>> Re-compiling train function...
2016-11-22 01:14:06 >>>   training loss:	0.207061
2016-11-22 01:14:06 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-10.pkl
2016-11-22 01:14:07 >>> epoch: 10 validation error:		0.075509
2016-11-22 01:14:24 >>>   training loss:	0.201936
2016-11-22 01:14:24 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-11.pkl
2016-11-22 01:14:25 >>> epoch: 11 validation error:		0.072832
2016-11-22 01:14:41 >>>   training loss:	0.197737
2016-11-22 01:14:41 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-12.pkl
2016-11-22 01:14:42 >>> epoch: 12 validation error:		0.069622
2016-11-22 01:14:59 >>>   training loss:	0.193848
2016-11-22 01:14:59 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-13.pkl
2016-11-22 01:15:00 >>> epoch: 13 validation error:		0.072456
2016-11-22 01:15:00 >>> fuzzy_newbob: Setting learning rate to: 0.062500
2016-11-22 01:15:00 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-12.pkl
2016-11-22 01:15:00 >>> Updating adadelta with learning_rate=0.062500
2016-11-22 01:15:00 >>> Re-compiling train function...
2016-11-22 01:15:18 >>>   training loss:	0.182049
2016-11-22 01:15:18 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-14.pkl
2016-11-22 01:15:18 >>> epoch: 14 validation error:		0.065843
2016-11-22 01:15:35 >>>   training loss:	0.179686
2016-11-22 01:15:35 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-15.pkl
2016-11-22 01:15:36 >>> epoch: 15 validation error:		0.065300
2016-11-22 01:15:52 >>>   training loss:	0.177340
2016-11-22 01:15:52 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-16.pkl
2016-11-22 01:15:53 >>> epoch: 16 validation error:		0.066406
2016-11-22 01:15:53 >>> fuzzy_newbob: Setting learning rate to: 0.031250
2016-11-22 01:15:53 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-15.pkl
2016-11-22 01:15:53 >>> Updating adadelta with learning_rate=0.031250
2016-11-22 01:15:53 >>> Re-compiling train function...
2016-11-22 01:16:11 >>>   training loss:	0.174594
2016-11-22 01:16:11 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-17.pkl
2016-11-22 01:16:12 >>> epoch: 17 validation error:		0.064953
2016-11-22 01:16:28 >>>   training loss:	0.173331
2016-11-22 01:16:28 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:16:29 >>> epoch: 18 validation error:		0.064363
2016-11-22 01:16:47 >>>   training loss:	0.172218
2016-11-22 01:16:47 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-19.pkl
2016-11-22 01:16:47 >>> epoch: 19 validation error:		0.065107
2016-11-22 01:16:47 >>> fuzzy_newbob: Setting learning rate to: 0.015625
2016-11-22 01:16:47 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:16:48 >>> Updating adadelta with learning_rate=0.015625
2016-11-22 01:16:48 >>> Re-compiling train function...
2016-11-22 01:17:05 >>>   training loss:	0.171428
2016-11-22 01:17:05 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-20.pkl
2016-11-22 01:17:06 >>> epoch: 20 validation error:		0.067241
2016-11-22 01:17:06 >>> fuzzy_newbob: Setting learning rate to: 0.007812
2016-11-22 01:17:06 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:17:06 >>> Updating adadelta with learning_rate=0.007812
2016-11-22 01:17:06 >>> Re-compiling train function...
2016-11-22 01:17:23 >>>   training loss:	0.171104
2016-11-22 01:17:23 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-21.pkl
2016-11-22 01:17:24 >>> epoch: 21 validation error:		0.065819
2016-11-22 01:17:24 >>> fuzzy_newbob: Setting learning rate to: 0.003906
2016-11-22 01:17:24 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:17:24 >>> Updating adadelta with learning_rate=0.003906
2016-11-22 01:17:24 >>> Re-compiling train function...
2016-11-22 01:17:42 >>>   training loss:	0.171011
2016-11-22 01:17:42 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-22.pkl
2016-11-22 01:17:43 >>> epoch: 22 validation error:		0.064815
2016-11-22 01:17:43 >>> fuzzy_newbob: Setting learning rate to: 0.001953
2016-11-22 01:17:43 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:17:43 >>> Updating adadelta with learning_rate=0.001953
2016-11-22 01:17:43 >>> Re-compiling train function...
2016-11-22 01:18:02 >>>   training loss:	0.170955
2016-11-22 01:18:02 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-23.pkl
2016-11-22 01:18:03 >>> epoch: 23 validation error:		0.064794
2016-11-22 01:18:03 >>> fuzzy_newbob: Setting learning rate to: 0.000977
2016-11-22 01:18:03 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:18:03 >>> Updating adadelta with learning_rate=0.000977
2016-11-22 01:18:03 >>> Re-compiling train function...
2016-11-22 01:18:21 >>>   training loss:	0.170936
2016-11-22 01:18:21 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-24.pkl
2016-11-22 01:18:22 >>> epoch: 24 validation error:		0.064595
2016-11-22 01:18:22 >>> fuzzy_newbob: Setting learning rate to: 0.000488
2016-11-22 01:18:22 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:18:22 >>> Updating adadelta with learning_rate=0.000488
2016-11-22 01:18:22 >>> Re-compiling train function...
2016-11-22 01:18:39 >>>   training loss:	0.170932
2016-11-22 01:18:39 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-25.pkl
2016-11-22 01:18:41 >>> epoch: 25 validation error:		0.064770
2016-11-22 01:18:41 >>> fuzzy_newbob: Setting learning rate to: 0.000244
2016-11-22 01:18:41 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:18:41 >>> Updating adadelta with learning_rate=0.000244
2016-11-22 01:18:41 >>> Re-compiling train function...
2016-11-22 01:18:58 >>>   training loss:	0.170931
2016-11-22 01:18:58 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-26.pkl
2016-11-22 01:18:59 >>> epoch: 26 validation error:		0.064712
2016-11-22 01:18:59 >>> fuzzy_newbob: Setting learning rate to: 0.000122
2016-11-22 01:18:59 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:18:59 >>> Updating adadelta with learning_rate=0.000122
2016-11-22 01:18:59 >>> Re-compiling train function...
2016-11-22 01:19:17 >>>   training loss:	0.170943
2016-11-22 01:19:17 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-27.pkl
2016-11-22 01:19:18 >>> epoch: 27 validation error:		0.064788
2016-11-22 01:19:18 >>> fuzzy_newbob: Setting learning rate to: 0.000061
2016-11-22 01:19:18 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:19:18 >>> Updating adadelta with learning_rate=0.000061
2016-11-22 01:19:18 >>> Re-compiling train function...
2016-11-22 01:19:36 >>>   training loss:	0.170956
2016-11-22 01:19:36 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-28.pkl
2016-11-22 01:19:37 >>> epoch: 28 validation error:		0.064794
2016-11-22 01:19:37 >>> fuzzy_newbob: Setting learning rate to: 0.000031
2016-11-22 01:19:37 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:19:37 >>> Updating adadelta with learning_rate=0.000031
2016-11-22 01:19:37 >>> Re-compiling train function...
2016-11-22 01:19:55 >>>   training loss:	0.170979
2016-11-22 01:19:55 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-29.pkl
2016-11-22 01:19:56 >>> epoch: 29 validation error:		0.064706
2016-11-22 01:19:56 >>> fuzzy_newbob: Setting learning rate to: 0.000015
2016-11-22 01:19:56 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:19:56 >>> Updating adadelta with learning_rate=0.000015
2016-11-22 01:19:56 >>> Re-compiling train function...
2016-11-22 01:20:13 >>>   training loss:	0.170989
2016-11-22 01:20:13 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-30.pkl
2016-11-22 01:20:14 >>> epoch: 30 validation error:		0.064812
2016-11-22 01:20:14 >>> fuzzy_newbob: Setting learning rate to: 0.000008
2016-11-22 01:20:14 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:20:14 >>> Updating adadelta with learning_rate=0.000008
2016-11-22 01:20:14 >>> Re-compiling train function...
2016-11-22 01:20:32 >>>   training loss:	0.171019
2016-11-22 01:20:32 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-31.pkl
2016-11-22 01:20:33 >>> epoch: 31 validation error:		0.064715
2016-11-22 01:20:33 >>> fuzzy_newbob: Setting learning rate to: 0.000004
2016-11-22 01:20:33 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:20:33 >>> Updating adadelta with learning_rate=0.000004
2016-11-22 01:20:33 >>> Re-compiling train function...
2016-11-22 01:20:50 >>>   training loss:	0.171069
2016-11-22 01:20:50 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-32.pkl
2016-11-22 01:20:51 >>> epoch: 32 validation error:		0.064649
2016-11-22 01:20:51 >>> fuzzy_newbob: Setting learning rate to: 0.000002
2016-11-22 01:20:51 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:20:51 >>> Updating adadelta with learning_rate=0.000002
2016-11-22 01:20:51 >>> Re-compiling train function...
2016-11-22 01:21:09 >>>   training loss:	0.171155
2016-11-22 01:21:09 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-33.pkl
2016-11-22 01:21:10 >>> epoch: 33 validation error:		0.064631
2016-11-22 01:21:10 >>> fuzzy_newbob: Setting learning rate to: 0.000001
2016-11-22 01:21:10 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:21:10 >>> Updating adadelta with learning_rate=0.000001
2016-11-22 01:21:10 >>> Re-compiling train function...
2016-11-22 01:21:27 >>>   training loss:	0.171222
2016-11-22 01:21:27 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-34.pkl
2016-11-22 01:21:28 >>> epoch: 34 validation error:		0.064525
2016-11-22 01:21:28 >>> fuzzy_newbob: Setting learning rate to: 0.000000
2016-11-22 01:21:28 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:21:28 >>> Updating adadelta with learning_rate=0.000000
2016-11-22 01:21:28 >>> Re-compiling train function...
2016-11-22 01:21:45 >>>   training loss:	0.171274
2016-11-22 01:21:45 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-35.pkl
2016-11-22 01:21:46 >>> epoch: 35 validation error:		0.064480
2016-11-22 01:21:46 >>> fuzzy_newbob: Setting learning rate to: 0.000000
2016-11-22 01:21:46 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:21:46 >>> Updating adadelta with learning_rate=0.000000
2016-11-22 01:21:46 >>> Re-compiling train function...
2016-11-22 01:22:03 >>>   training loss:	0.171301
2016-11-22 01:22:03 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-36.pkl
2016-11-22 01:22:04 >>> epoch: 36 validation error:		0.064387
2016-11-22 01:22:04 >>> fuzzy_newbob: Setting learning rate to: 0.000000
2016-11-22 01:22:04 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-18.pkl
2016-11-22 01:22:04 >>> Updating adadelta with learning_rate=0.000000
2016-11-22 01:22:04 >>> Re-compiling train function...
2016-11-22 01:22:22 >>>   training loss:	0.171326
2016-11-22 01:22:22 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-37.pkl
2016-11-22 01:22:23 >>> epoch: 37 validation error:		0.064333
2016-11-22 01:22:39 >>>   training loss:	0.171291
2016-11-22 01:22:39 >>> Saving network params to out/v1-1-g91763ea-dirty/epoch-38.pkl
2016-11-22 01:22:40 >>> epoch: 38 validation error:		0.064360
2016-11-22 01:22:40 >>> fuzzy_newbob: Setting learning rate to: 0.000000
2016-11-22 01:22:40 >>> Loading old params from out/v1-1-g91763ea-dirty/epoch-37.pkl
2016-11-22 01:22:40 >>> Updating adadelta with learning_rate=0.000000
2016-11-22 01:22:40 >>> Re-compiling train function...
