2017-02-10 19:33:27,243 DEBUG    version=v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50
2017-02-10 19:33:27,244 INFO     Open DBase(data/db/all240302-spk.dat, data/db/all240302-spk.idx, mode='r')
2017-02-10 19:33:27,527 INFO     loading uttdb cache
2017-02-10 19:33:45,434 DEBUG    loading cached extracted data from data/cache/extract-7a00b23850576d60137fa78432d8dfda09ea0f343092bd09848f21113b1acfd0.pickle
2017-02-10 19:33:52,485 DEBUG    set input dim to 9
2017-02-10 19:33:52,486 DEBUG    input dim = 9
2017-02-10 19:33:52,494 DEBUG    shuffling batches
2017-02-10 19:33:54,835 DEBUG    shuffling done
2017-02-10 19:33:54,835 DEBUG    loading data into ram
2017-02-10 19:33:54,914 DEBUG    loading data took 0.078s (cpu: 0.078s)
2017-02-10 19:33:55,065 DEBUG    set input dim to 9
2017-02-10 19:33:55,065 DEBUG    input dim = 9
2017-02-10 19:33:55,087 DEBUG    shuffling batches
2017-02-10 19:33:55,309 DEBUG    shuffling done
2017-02-10 19:33:55,309 DEBUG    loading data into ram
2017-02-10 19:33:55,317 DEBUG    loading data took 0.007s (cpu: 0.007s)
2017-02-10 19:33:55,414 DEBUG    Applying L2 regularization with 0.000100
2017-02-10 19:33:55,516 INFO     Training network with 95102 trainable out of 95502 total params.
2017-02-10 19:33:56,374 DEBUG    Using adam with learning_rate=0.001000
2017-02-10 19:34:35,312 DEBUG    Starting training...
2017-02-10 19:35:16,568 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-000.pkl
2017-02-10 19:35:17,440 INFO     epoch: 0 took 42.128s (0.001s in batching)
training loss:	0.646255
validation loss:	0.582232
validation error:	0.304600
2017-02-10 19:35:17,441 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:35:58,408 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-001.pkl
2017-02-10 19:35:59,276 INFO     epoch: 1 took 41.836s (0.001s in batching)
training loss:	0.602116
validation loss:	0.571056
validation error:	0.298100
2017-02-10 19:35:59,276 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:36:40,439 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-002.pkl
2017-02-10 19:36:41,309 INFO     epoch: 2 took 42.033s (0.001s in batching)
training loss:	0.590413
validation loss:	0.573338
validation error:	0.292100
2017-02-10 19:36:41,310 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:37:22,651 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-003.pkl
2017-02-10 19:37:23,522 INFO     epoch: 3 took 42.212s (0.001s in batching)
training loss:	0.583571
validation loss:	0.565793
validation error:	0.288900
2017-02-10 19:37:23,522 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:38:04,983 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-004.pkl
2017-02-10 19:38:05,863 INFO     epoch: 4 took 42.342s (0.001s in batching)
training loss:	0.578516
validation loss:	0.561549
validation error:	0.287000
2017-02-10 19:38:05,864 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:38:47,165 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-005.pkl
2017-02-10 19:38:48,035 INFO     epoch: 5 took 42.172s (0.001s in batching)
training loss:	0.573750
validation loss:	0.558211
validation error:	0.282700
2017-02-10 19:38:48,036 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:39:29,324 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-006.pkl
2017-02-10 19:39:30,242 INFO     epoch: 6 took 42.207s (0.001s in batching)
training loss:	0.568886
validation loss:	0.551754
validation error:	0.275200
2017-02-10 19:39:30,242 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:40:11,538 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-007.pkl
2017-02-10 19:40:12,406 INFO     epoch: 7 took 42.164s (0.001s in batching)
training loss:	0.563946
validation loss:	0.547679
validation error:	0.274900
2017-02-10 19:40:12,407 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:40:53,573 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-008.pkl
2017-02-10 19:40:54,435 INFO     epoch: 8 took 42.029s (0.001s in batching)
training loss:	0.561386
validation loss:	0.545390
validation error:	0.274900
2017-02-10 19:40:54,436 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:41:35,326 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-009.pkl
2017-02-10 19:41:36,191 INFO     epoch: 9 took 41.756s (0.001s in batching)
training loss:	0.558373
validation loss:	0.543435
validation error:	0.273500
2017-02-10 19:41:36,192 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:42:16,937 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-010.pkl
2017-02-10 19:42:17,812 INFO     epoch: 10 took 41.621s (0.001s in batching)
training loss:	0.556055
validation loss:	0.543036
validation error:	0.272100
2017-02-10 19:42:17,813 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:42:59,140 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-011.pkl
2017-02-10 19:43:00,003 INFO     epoch: 11 took 42.191s (0.001s in batching)
training loss:	0.554585
validation loss:	0.552045
validation error:	0.282400
2017-02-10 19:43:00,004 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:43:41,272 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-012.pkl
2017-02-10 19:43:42,146 INFO     epoch: 12 took 42.143s (0.001s in batching)
training loss:	0.553155
validation loss:	0.538841
validation error:	0.270600
2017-02-10 19:43:42,147 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:44:23,447 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-013.pkl
2017-02-10 19:44:24,327 INFO     epoch: 13 took 42.180s (0.001s in batching)
training loss:	0.551683
validation loss:	0.540178
validation error:	0.271200
2017-02-10 19:44:24,328 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:45:05,525 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-014.pkl
2017-02-10 19:45:06,386 INFO     epoch: 14 took 42.059s (0.001s in batching)
training loss:	0.550500
validation loss:	0.542091
validation error:	0.270700
2017-02-10 19:45:06,387 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:45:47,876 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-015.pkl
2017-02-10 19:45:48,743 INFO     epoch: 15 took 42.357s (0.001s in batching)
training loss:	0.549021
validation loss:	0.535996
validation error:	0.266700
2017-02-10 19:45:48,744 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:46:29,778 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-016.pkl
2017-02-10 19:46:30,643 INFO     epoch: 16 took 41.900s (0.001s in batching)
training loss:	0.547941
validation loss:	0.536810
validation error:	0.269400
2017-02-10 19:46:30,644 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:47:11,638 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-017.pkl
2017-02-10 19:47:12,502 INFO     epoch: 17 took 41.859s (0.001s in batching)
training loss:	0.547364
validation loss:	0.534941
validation error:	0.266600
2017-02-10 19:47:12,502 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:47:53,433 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-018.pkl
2017-02-10 19:47:54,292 INFO     epoch: 18 took 41.790s (0.001s in batching)
training loss:	0.546609
validation loss:	0.536952
validation error:	0.267800
2017-02-10 19:47:54,293 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:48:35,055 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-019.pkl
2017-02-10 19:48:35,914 INFO     epoch: 19 took 41.623s (0.001s in batching)
training loss:	0.544697
validation loss:	0.536581
validation error:	0.265200
2017-02-10 19:48:35,915 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:49:17,408 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-020.pkl
2017-02-10 19:49:18,272 INFO     epoch: 20 took 42.357s (0.001s in batching)
training loss:	0.544249
validation loss:	0.537713
validation error:	0.263000
2017-02-10 19:49:18,272 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:49:59,747 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-021.pkl
2017-02-10 19:50:00,634 INFO     epoch: 21 took 42.362s (0.001s in batching)
training loss:	0.543939
validation loss:	0.530169
validation error:	0.263400
2017-02-10 19:50:00,635 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:50:41,725 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-022.pkl
2017-02-10 19:50:42,591 INFO     epoch: 22 took 41.957s (0.001s in batching)
training loss:	0.542696
validation loss:	0.540144
validation error:	0.269700
2017-02-10 19:50:42,592 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:51:23,514 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-023.pkl
2017-02-10 19:51:24,381 INFO     epoch: 23 took 41.790s (0.001s in batching)
training loss:	0.541477
validation loss:	0.543224
validation error:	0.267500
2017-02-10 19:51:24,382 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:52:05,184 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-024.pkl
2017-02-10 19:52:06,041 INFO     epoch: 24 took 41.660s (0.001s in batching)
training loss:	0.540309
validation loss:	0.524869
validation error:	0.263600
2017-02-10 19:52:06,042 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:52:46,992 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-025.pkl
2017-02-10 19:52:47,858 INFO     epoch: 25 took 41.817s (0.001s in batching)
training loss:	0.538524
validation loss:	0.526852
validation error:	0.262800
2017-02-10 19:52:47,859 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:53:29,536 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-026.pkl
2017-02-10 19:53:30,426 INFO     epoch: 26 took 42.569s (0.001s in batching)
training loss:	0.537839
validation loss:	0.528085
validation error:	0.263400
2017-02-10 19:53:30,427 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:54:11,579 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-027.pkl
2017-02-10 19:54:12,449 INFO     epoch: 27 took 42.023s (0.001s in batching)
training loss:	0.536411
validation loss:	0.523684
validation error:	0.260000
2017-02-10 19:54:12,450 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:54:53,726 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-028.pkl
2017-02-10 19:54:54,633 INFO     epoch: 28 took 42.184s (0.001s in batching)
training loss:	0.535368
validation loss:	0.524670
validation error:	0.261900
2017-02-10 19:54:54,634 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:55:36,001 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-029.pkl
2017-02-10 19:55:36,860 INFO     epoch: 29 took 42.227s (0.001s in batching)
training loss:	0.534699
validation loss:	0.521860
validation error:	0.258100
2017-02-10 19:55:36,861 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:56:18,262 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-030.pkl
2017-02-10 19:56:19,131 INFO     epoch: 30 took 42.271s (0.001s in batching)
training loss:	0.533380
validation loss:	0.527006
validation error:	0.259400
2017-02-10 19:56:19,132 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:57:00,149 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-031.pkl
2017-02-10 19:57:01,016 INFO     epoch: 31 took 41.885s (0.001s in batching)
training loss:	0.532785
validation loss:	0.520644
validation error:	0.260000
2017-02-10 19:57:01,018 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:57:42,389 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-032.pkl
2017-02-10 19:57:43,274 INFO     epoch: 32 took 42.257s (0.001s in batching)
training loss:	0.531920
validation loss:	0.525333
validation error:	0.259200
2017-02-10 19:57:43,275 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:58:24,720 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-033.pkl
2017-02-10 19:58:25,604 INFO     epoch: 33 took 42.330s (0.001s in batching)
training loss:	0.531092
validation loss:	0.519499
validation error:	0.257400
2017-02-10 19:58:25,605 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:59:07,261 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-034.pkl
2017-02-10 19:59:08,134 INFO     epoch: 34 took 42.530s (0.001s in batching)
training loss:	0.530399
validation loss:	0.522633
validation error:	0.258500
2017-02-10 19:59:08,136 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 19:59:49,677 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-035.pkl
2017-02-10 19:59:50,559 INFO     epoch: 35 took 42.424s (0.001s in batching)
training loss:	0.529959
validation loss:	0.519497
validation error:	0.258700
2017-02-10 19:59:50,560 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:00:31,894 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-036.pkl
2017-02-10 20:00:32,767 INFO     epoch: 36 took 42.209s (0.001s in batching)
training loss:	0.529748
validation loss:	0.515263
validation error:	0.251400
2017-02-10 20:00:32,769 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:01:14,508 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-037.pkl
2017-02-10 20:01:15,571 INFO     epoch: 37 took 42.804s (0.001s in batching)
training loss:	0.529606
validation loss:	0.518726
validation error:	0.258200
2017-02-10 20:01:15,573 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:01:56,793 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-038.pkl
2017-02-10 20:01:57,665 INFO     epoch: 38 took 42.094s (0.001s in batching)
training loss:	0.528732
validation loss:	0.518867
validation error:	0.254900
2017-02-10 20:01:57,666 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:02:38,770 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-039.pkl
2017-02-10 20:02:39,643 INFO     epoch: 39 took 41.978s (0.001s in batching)
training loss:	0.527626
validation loss:	0.516913
validation error:	0.255700
2017-02-10 20:02:39,644 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:03:20,766 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-040.pkl
2017-02-10 20:03:21,636 INFO     epoch: 40 took 41.994s (0.001s in batching)
training loss:	0.526673
validation loss:	0.514024
validation error:	0.253900
2017-02-10 20:03:21,638 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:04:02,957 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-041.pkl
2017-02-10 20:04:03,842 INFO     epoch: 41 took 42.205s (0.001s in batching)
training loss:	0.526814
validation loss:	0.519043
validation error:	0.255500
2017-02-10 20:04:03,843 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:04:45,437 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-042.pkl
2017-02-10 20:04:46,317 INFO     epoch: 42 took 42.476s (0.001s in batching)
training loss:	0.526727
validation loss:	0.514497
validation error:	0.257400
2017-02-10 20:04:46,319 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:05:27,680 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-043.pkl
2017-02-10 20:05:28,558 INFO     epoch: 43 took 42.241s (0.001s in batching)
training loss:	0.526084
validation loss:	0.517325
validation error:	0.254700
2017-02-10 20:05:28,559 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:06:10,035 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-044.pkl
2017-02-10 20:06:10,913 INFO     epoch: 44 took 42.355s (0.001s in batching)
training loss:	0.525260
validation loss:	0.513144
validation error:	0.253500
2017-02-10 20:06:10,915 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:06:52,339 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-045.pkl
2017-02-10 20:06:53,222 INFO     epoch: 45 took 42.308s (0.001s in batching)
training loss:	0.525052
validation loss:	0.515449
validation error:	0.253200
2017-02-10 20:06:53,223 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:07:34,616 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-046.pkl
2017-02-10 20:07:35,493 INFO     epoch: 46 took 42.271s (0.001s in batching)
training loss:	0.525109
validation loss:	0.515138
validation error:	0.253400
2017-02-10 20:07:35,494 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:08:16,980 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-047.pkl
2017-02-10 20:08:17,865 INFO     epoch: 47 took 42.372s (0.001s in batching)
training loss:	0.524272
validation loss:	0.516756
validation error:	0.253900
2017-02-10 20:08:17,866 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:08:59,303 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-048.pkl
2017-02-10 20:09:00,188 INFO     epoch: 48 took 42.323s (0.001s in batching)
training loss:	0.523368
validation loss:	0.512212
validation error:	0.252700
2017-02-10 20:09:00,189 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:09:42,023 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-049.pkl
2017-02-10 20:09:42,933 INFO     epoch: 49 took 42.746s (0.001s in batching)
training loss:	0.523074
validation loss:	0.516036
validation error:	0.254200
2017-02-10 20:09:42,935 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:10:24,760 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-050.pkl
2017-02-10 20:10:25,643 INFO     epoch: 50 took 42.709s (0.001s in batching)
training loss:	0.522561
validation loss:	0.510031
validation error:	0.250900
2017-02-10 20:10:25,644 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:11:07,063 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-051.pkl
2017-02-10 20:11:07,952 INFO     epoch: 51 took 42.309s (0.001s in batching)
training loss:	0.523493
validation loss:	0.514710
validation error:	0.252800
2017-02-10 20:11:07,954 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:11:49,371 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-052.pkl
2017-02-10 20:11:50,258 INFO     epoch: 52 took 42.306s (0.001s in batching)
training loss:	0.522300
validation loss:	0.512267
validation error:	0.252100
2017-02-10 20:11:50,260 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:12:31,650 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-053.pkl
2017-02-10 20:12:32,527 INFO     epoch: 53 took 42.269s (0.001s in batching)
training loss:	0.522058
validation loss:	0.514777
validation error:	0.252800
2017-02-10 20:12:32,528 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:13:13,903 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-054.pkl
2017-02-10 20:13:14,784 INFO     epoch: 54 took 42.257s (0.001s in batching)
training loss:	0.521745
validation loss:	0.512438
validation error:	0.251200
2017-02-10 20:13:14,786 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:13:56,148 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-055.pkl
2017-02-10 20:13:57,027 INFO     epoch: 55 took 42.243s (0.001s in batching)
training loss:	0.520638
validation loss:	0.510925
validation error:	0.252200
2017-02-10 20:13:57,029 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:14:38,304 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-056.pkl
2017-02-10 20:14:39,180 INFO     epoch: 56 took 42.153s (0.001s in batching)
training loss:	0.519868
validation loss:	0.511545
validation error:	0.249900
2017-02-10 20:14:39,181 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:15:20,503 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-057.pkl
2017-02-10 20:15:21,399 INFO     epoch: 57 took 42.220s (0.001s in batching)
training loss:	0.520313
validation loss:	0.512481
validation error:	0.249800
2017-02-10 20:15:21,401 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:16:02,726 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-058.pkl
2017-02-10 20:16:03,607 INFO     epoch: 58 took 42.208s (0.001s in batching)
training loss:	0.520253
validation loss:	0.515242
validation error:	0.255400
2017-02-10 20:16:03,609 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:16:45,293 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-059.pkl
2017-02-10 20:16:46,171 INFO     epoch: 59 took 42.564s (0.001s in batching)
training loss:	0.520235
validation loss:	0.514203
validation error:	0.252600
2017-02-10 20:16:46,173 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:17:27,797 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-060.pkl
2017-02-10 20:17:28,680 INFO     epoch: 60 took 42.509s (0.001s in batching)
training loss:	0.519454
validation loss:	0.511570
validation error:	0.249100
2017-02-10 20:17:28,682 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:18:10,032 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-061.pkl
2017-02-10 20:18:10,903 INFO     epoch: 61 took 42.223s (0.001s in batching)
training loss:	0.520036
validation loss:	0.511533
validation error:	0.249000
2017-02-10 20:18:10,905 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:18:52,189 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-062.pkl
2017-02-10 20:18:53,062 INFO     epoch: 62 took 42.159s (0.001s in batching)
training loss:	0.518681
validation loss:	0.510762
validation error:	0.253100
2017-02-10 20:18:53,063 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:19:34,258 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-063.pkl
2017-02-10 20:19:35,130 INFO     epoch: 63 took 42.069s (0.001s in batching)
training loss:	0.518502
validation loss:	0.511200
validation error:	0.250000
2017-02-10 20:19:35,132 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:20:16,378 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-064.pkl
2017-02-10 20:20:17,256 INFO     epoch: 64 took 42.125s (0.001s in batching)
training loss:	0.517691
validation loss:	0.514136
validation error:	0.252800
2017-02-10 20:20:17,258 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:20:58,521 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-065.pkl
2017-02-10 20:20:59,400 INFO     epoch: 65 took 42.144s (0.001s in batching)
training loss:	0.518158
validation loss:	0.512461
validation error:	0.251500
2017-02-10 20:20:59,402 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:21:40,710 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-066.pkl
2017-02-10 20:21:41,590 INFO     epoch: 66 took 42.190s (0.001s in batching)
training loss:	0.517892
validation loss:	0.511911
validation error:	0.252200
2017-02-10 20:21:41,591 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:22:22,923 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-067.pkl
2017-02-10 20:22:23,803 INFO     epoch: 67 took 42.214s (0.001s in batching)
training loss:	0.517600
validation loss:	0.510745
validation error:	0.250200
2017-02-10 20:22:23,805 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:23:05,096 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-068.pkl
2017-02-10 20:23:05,975 INFO     epoch: 68 took 42.171s (0.001s in batching)
training loss:	0.516976
validation loss:	0.513247
validation error:	0.248900
2017-02-10 20:23:05,976 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:23:47,248 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-069.pkl
2017-02-10 20:23:48,119 INFO     epoch: 69 took 42.144s (0.001s in batching)
training loss:	0.516657
validation loss:	0.509694
validation error:	0.250800
2017-02-10 20:23:48,121 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:24:29,235 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-070.pkl
2017-02-10 20:24:30,096 INFO     epoch: 70 took 41.977s (0.001s in batching)
training loss:	0.516812
validation loss:	0.514083
validation error:	0.251400
2017-02-10 20:24:30,098 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:25:11,028 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-071.pkl
2017-02-10 20:25:11,889 INFO     epoch: 71 took 41.793s (0.001s in batching)
training loss:	0.516508
validation loss:	0.509849
validation error:	0.250300
2017-02-10 20:25:11,890 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:25:52,710 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-072.pkl
2017-02-10 20:25:53,573 INFO     epoch: 72 took 41.685s (0.001s in batching)
training loss:	0.515999
validation loss:	0.511486
validation error:	0.249400
2017-02-10 20:25:53,575 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:26:34,662 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-073.pkl
2017-02-10 20:26:35,545 INFO     epoch: 73 took 41.972s (0.001s in batching)
training loss:	0.516079
validation loss:	0.509706
validation error:	0.251100
2017-02-10 20:26:35,546 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:27:17,031 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-074.pkl
2017-02-10 20:27:17,915 INFO     epoch: 74 took 42.371s (0.001s in batching)
training loss:	0.515353
validation loss:	0.513864
validation error:	0.253400
2017-02-10 20:27:17,918 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:27:59,580 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-075.pkl
2017-02-10 20:28:00,482 INFO     epoch: 75 took 42.566s (0.001s in batching)
training loss:	0.514835
validation loss:	0.510507
validation error:	0.252300
2017-02-10 20:28:00,484 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:28:42,110 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-076.pkl
2017-02-10 20:28:42,994 INFO     epoch: 76 took 42.513s (0.001s in batching)
training loss:	0.515009
validation loss:	0.512273
validation error:	0.251700
2017-02-10 20:28:42,996 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:29:24,537 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-077.pkl
2017-02-10 20:29:25,420 INFO     epoch: 77 took 42.426s (0.001s in batching)
training loss:	0.514977
validation loss:	0.511954
validation error:	0.249200
2017-02-10 20:29:25,422 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:30:06,829 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-078.pkl
2017-02-10 20:30:07,701 INFO     epoch: 78 took 42.281s (0.001s in batching)
training loss:	0.515036
validation loss:	0.515155
validation error:	0.253600
2017-02-10 20:30:07,703 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:30:49,446 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-079.pkl
2017-02-10 20:30:50,345 INFO     epoch: 79 took 42.644s (0.001s in batching)
training loss:	0.514004
validation loss:	0.515033
validation error:	0.254300
2017-02-10 20:30:50,347 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:31:32,195 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-080.pkl
2017-02-10 20:31:33,082 INFO     epoch: 80 took 42.736s (0.001s in batching)
training loss:	0.513263
validation loss:	0.514210
validation error:	0.251400
2017-02-10 20:31:33,084 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:32:14,716 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-081.pkl
2017-02-10 20:32:15,599 INFO     epoch: 81 took 42.517s (0.001s in batching)
training loss:	0.513201
validation loss:	0.512180
validation error:	0.251500
2017-02-10 20:32:15,601 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:32:57,145 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-082.pkl
2017-02-10 20:32:58,029 INFO     epoch: 82 took 42.430s (0.001s in batching)
training loss:	0.513094
validation loss:	0.513842
validation error:	0.251600
2017-02-10 20:32:58,031 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:33:39,629 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-083.pkl
2017-02-10 20:33:40,511 INFO     epoch: 83 took 42.481s (0.001s in batching)
training loss:	0.513147
validation loss:	0.512314
validation error:	0.251600
2017-02-10 20:33:40,513 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:34:21,792 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-084.pkl
2017-02-10 20:34:22,661 INFO     epoch: 84 took 42.150s (0.001s in batching)
training loss:	0.513116
validation loss:	0.515964
validation error:	0.253200
2017-02-10 20:34:22,663 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:35:03,639 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-085.pkl
2017-02-10 20:35:04,502 INFO     epoch: 85 took 41.842s (0.001s in batching)
training loss:	0.512679
validation loss:	0.516605
validation error:	0.252900
2017-02-10 20:35:04,504 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:35:45,349 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-086.pkl
2017-02-10 20:35:46,211 INFO     epoch: 86 took 41.709s (0.001s in batching)
training loss:	0.512445
validation loss:	0.513784
validation error:	0.252400
2017-02-10 20:35:46,213 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:36:27,161 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-087.pkl
2017-02-10 20:36:28,027 INFO     epoch: 87 took 41.816s (0.001s in batching)
training loss:	0.511643
validation loss:	0.511795
validation error:	0.253500
2017-02-10 20:36:28,029 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:37:08,947 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-088.pkl
2017-02-10 20:37:09,811 INFO     epoch: 88 took 41.784s (0.001s in batching)
training loss:	0.511969
validation loss:	0.511364
validation error:	0.249500
2017-02-10 20:37:09,813 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:37:51,297 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-089.pkl
2017-02-10 20:37:52,180 INFO     epoch: 89 took 42.368s (0.001s in batching)
training loss:	0.512093
validation loss:	0.510688
validation error:	0.248300
2017-02-10 20:37:52,182 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:38:33,780 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-090.pkl
2017-02-10 20:38:34,659 INFO     epoch: 90 took 42.480s (0.001s in batching)
training loss:	0.512006
validation loss:	0.513261
validation error:	0.251700
2017-02-10 20:38:34,661 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:39:16,390 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-091.pkl
2017-02-10 20:39:17,266 INFO     epoch: 91 took 42.607s (0.001s in batching)
training loss:	0.510628
validation loss:	0.512247
validation error:	0.250900
2017-02-10 20:39:17,269 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:39:58,703 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-092.pkl
2017-02-10 20:39:59,583 INFO     epoch: 92 took 42.316s (0.001s in batching)
training loss:	0.511079
validation loss:	0.513807
validation error:	0.250400
2017-02-10 20:39:59,585 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:40:41,029 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-093.pkl
2017-02-10 20:40:41,912 INFO     epoch: 93 took 42.330s (0.001s in batching)
training loss:	0.511012
validation loss:	0.518283
validation error:	0.255500
2017-02-10 20:40:41,915 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:41:23,311 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-094.pkl
2017-02-10 20:41:24,187 INFO     epoch: 94 took 42.274s (0.001s in batching)
training loss:	0.510738
validation loss:	0.514603
validation error:	0.249400
2017-02-10 20:41:24,189 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:42:05,555 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-095.pkl
2017-02-10 20:42:06,434 INFO     epoch: 95 took 42.247s (0.001s in batching)
training loss:	0.509613
validation loss:	0.518579
validation error:	0.251500
2017-02-10 20:42:06,436 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:42:47,780 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-096.pkl
2017-02-10 20:42:48,663 INFO     epoch: 96 took 42.229s (0.001s in batching)
training loss:	0.510080
validation loss:	0.514240
validation error:	0.249400
2017-02-10 20:42:48,665 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:43:30,034 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-097.pkl
2017-02-10 20:43:30,915 INFO     epoch: 97 took 42.252s (0.001s in batching)
training loss:	0.509670
validation loss:	0.512727
validation error:	0.251800
2017-02-10 20:43:30,917 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:44:12,298 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-098.pkl
2017-02-10 20:44:13,178 INFO     epoch: 98 took 42.263s (0.001s in batching)
training loss:	0.509423
validation loss:	0.514685
validation error:	0.250100
2017-02-10 20:44:13,180 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
2017-02-10 20:44:54,652 INFO     Saving network params to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/epoch-099.pkl
2017-02-10 20:44:55,534 INFO     epoch: 99 took 42.356s (0.001s in batching)
training loss:	0.508746
validation loss:	0.517116
validation error:	0.252300
2017-02-10 20:44:55,537 INFO     Wrote output to trainNN/out/v050-finunified-9-g022b29a-dirty:lstm-best-layers-100-50-50/config.json
