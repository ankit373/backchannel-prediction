2016-11-30 21:42:40 >>> version=v020-begin-at-bc-max-power-dirty
2016-11-30 21:42:40 >>> loading config file extract_pfiles_python/out/v020-begin-at-bc-max-power-context40/config.json
2016-11-30 21:42:40 >>> loading numpy file extract_pfiles_python/out/v020-begin-at-bc-max-power-context40/train.npz
2016-11-30 21:42:45 >>> loading numpy file extract_pfiles_python/out/v020-begin-at-bc-max-power-context40/validate.npz
2016-11-30 21:42:46 >>> Using fuzzy_newbob as schedulung method.
2016-11-30 21:42:46 >>> Training network with 21452 trainable out of 21452 total params.
2016-11-30 21:42:46 >>> Using adam with learning_rate=0.001000
2016-11-30 21:42:46 >>> Compiling theano functions...
2016-11-30 21:42:47 >>> Starting training...
2016-11-30 21:43:09 >>>   training loss:	0.573090
2016-11-30 21:43:09 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-000.pkl
2016-11-30 21:43:10 >>> epoch: 0 took 23.584s validation error:		0.282422
2016-11-30 21:43:31 >>>   training loss:	0.554187
2016-11-30 21:43:31 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-001.pkl
2016-11-30 21:43:32 >>> epoch: 1 took 21.835s validation error:		0.276315
2016-11-30 21:43:52 >>>   training loss:	0.548498
2016-11-30 21:43:52 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-002.pkl
2016-11-30 21:43:53 >>> epoch: 2 took 20.776s validation error:		0.274967
2016-11-30 21:44:15 >>>   training loss:	0.545133
2016-11-30 21:44:15 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-003.pkl
2016-11-30 21:44:16 >>> epoch: 3 took 23.216s validation error:		0.273080
2016-11-30 21:44:37 >>>   training loss:	0.542431
2016-11-30 21:44:37 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-004.pkl
2016-11-30 21:44:38 >>> epoch: 4 took 21.837s validation error:		0.272337
2016-11-30 21:45:01 >>>   training loss:	0.540302
2016-11-30 21:45:01 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-005.pkl
2016-11-30 21:45:04 >>> epoch: 5 took 25.622s validation error:		0.271311
2016-11-30 21:45:26 >>>   training loss:	0.538572
2016-11-30 21:45:26 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-006.pkl
2016-11-30 21:45:27 >>> epoch: 6 took 23.186s validation error:		0.270235
2016-11-30 21:45:50 >>>   training loss:	0.537151
2016-11-30 21:45:50 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-007.pkl
2016-11-30 21:45:51 >>> epoch: 7 took 24.053s validation error:		0.271160
2016-11-30 21:45:51 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-006.pkl
2016-11-30 21:45:51 >>> fuzzy_newbob: Updated adam with learning_rate=0.000800
2016-11-30 21:45:51 >>> Re-compiling train function...
2016-11-30 21:46:16 >>>   training loss:	0.536473
2016-11-30 21:46:16 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-008.pkl
2016-11-30 21:46:17 >>> epoch: 8 took 25.916s validation error:		0.270645
2016-11-30 21:46:17 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-006.pkl
2016-11-30 21:46:17 >>> fuzzy_newbob: Updated adam with learning_rate=0.000640
2016-11-30 21:46:17 >>> Re-compiling train function...
2016-11-30 21:46:43 >>>   training loss:	0.535949
2016-11-30 21:46:43 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-009.pkl
2016-11-30 21:46:44 >>> epoch: 9 took 27.047s validation error:		0.270700
2016-11-30 21:46:44 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-006.pkl
2016-11-30 21:46:44 >>> fuzzy_newbob: Updated adam with learning_rate=0.000512
2016-11-30 21:46:44 >>> Re-compiling train function...
2016-11-30 21:47:08 >>>   training loss:	0.535608
2016-11-30 21:47:08 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-010.pkl
2016-11-30 21:47:09 >>> epoch: 10 took 25.525s validation error:		0.268593
2016-11-30 21:47:33 >>>   training loss:	0.534743
2016-11-30 21:47:33 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-011.pkl
2016-11-30 21:47:34 >>> epoch: 11 took 24.622s validation error:		0.268637
2016-11-30 21:47:34 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-010.pkl
2016-11-30 21:47:34 >>> fuzzy_newbob: Updated adam with learning_rate=0.000410
2016-11-30 21:47:34 >>> Re-compiling train function...
2016-11-30 21:47:58 >>>   training loss:	0.534399
2016-11-30 21:47:58 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-012.pkl
2016-11-30 21:47:59 >>> epoch: 12 took 24.936s validation error:		0.269391
2016-11-30 21:47:59 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-010.pkl
2016-11-30 21:47:59 >>> fuzzy_newbob: Updated adam with learning_rate=0.000328
2016-11-30 21:47:59 >>> Re-compiling train function...
2016-11-30 21:48:21 >>>   training loss:	0.534126
2016-11-30 21:48:21 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-013.pkl
2016-11-30 21:48:23 >>> epoch: 13 took 23.578s validation error:		0.268027
2016-11-30 21:48:47 >>>   training loss:	0.533586
2016-11-30 21:48:47 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-014.pkl
2016-11-30 21:48:48 >>> epoch: 14 took 25.693s validation error:		0.268593
2016-11-30 21:48:48 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-013.pkl
2016-11-30 21:48:48 >>> fuzzy_newbob: Updated adam with learning_rate=0.000262
2016-11-30 21:48:48 >>> Re-compiling train function...
2016-11-30 21:49:13 >>>   training loss:	0.533378
2016-11-30 21:49:13 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-015.pkl
2016-11-30 21:49:14 >>> epoch: 15 took 25.993s validation error:		0.268604
2016-11-30 21:49:14 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-013.pkl
2016-11-30 21:49:14 >>> fuzzy_newbob: Updated adam with learning_rate=0.000210
2016-11-30 21:49:14 >>> Re-compiling train function...
2016-11-30 21:49:39 >>>   training loss:	0.533166
2016-11-30 21:49:39 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-016.pkl
2016-11-30 21:49:40 >>> epoch: 16 took 25.893s validation error:		0.268068
2016-11-30 21:49:40 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-013.pkl
2016-11-30 21:49:40 >>> fuzzy_newbob: Updated adam with learning_rate=0.000168
2016-11-30 21:49:40 >>> Re-compiling train function...
2016-11-30 21:50:00 >>>   training loss:	0.533029
2016-11-30 21:50:00 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-017.pkl
2016-11-30 21:50:01 >>> epoch: 17 took 21.147s validation error:		0.268453
2016-11-30 21:50:01 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-013.pkl
2016-11-30 21:50:01 >>> fuzzy_newbob: Updated adam with learning_rate=0.000134
2016-11-30 21:50:01 >>> Re-compiling train function...
2016-11-30 21:50:26 >>>   training loss:	0.532904
2016-11-30 21:50:26 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-018.pkl
2016-11-30 21:50:27 >>> epoch: 18 took 26.067s validation error:		0.268194
2016-11-30 21:50:27 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-013.pkl
2016-11-30 21:50:27 >>> fuzzy_newbob: Updated adam with learning_rate=0.000107
2016-11-30 21:50:27 >>> Re-compiling train function...
2016-11-30 21:50:49 >>>   training loss:	0.532817
2016-11-30 21:50:49 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-019.pkl
2016-11-30 21:50:51 >>> epoch: 19 took 23.592s validation error:		0.268021
2016-11-30 21:51:24 >>>   training loss:	0.532593
2016-11-30 21:51:24 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-020.pkl
2016-11-30 21:51:26 >>> epoch: 20 took 34.733s validation error:		0.268087
2016-11-30 21:51:26 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-019.pkl
2016-11-30 21:51:26 >>> fuzzy_newbob: Updated adam with learning_rate=0.000086
2016-11-30 21:51:26 >>> Re-compiling train function...
2016-11-30 21:51:48 >>>   training loss:	0.532520
2016-11-30 21:51:48 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-021.pkl
2016-11-30 21:51:49 >>> epoch: 21 took 23.239s validation error:		0.268181
2016-11-30 21:51:49 >>> Loading old params from trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-019.pkl
2016-11-30 21:51:49 >>> fuzzy_newbob: Updated adam with learning_rate=0.000069
2016-11-30 21:51:49 >>> Re-compiling train function...
2016-11-30 21:52:22 >>>   training loss:	0.532459
2016-11-30 21:52:22 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-022.pkl
2016-11-30 21:52:23 >>> epoch: 22 took 34.194s validation error:		0.267941
2016-11-30 21:52:35 >>> received exit signal, waiting for epoch to finish...
2016-11-30 21:52:44 >>>   training loss:	0.532326
2016-11-30 21:52:44 >>> Saving network params to trainNN/out/v020-begin-at-bc-max-power-dirty/epoch-023.pkl
2016-11-30 21:52:45 >>> epoch: 23 took 21.768s validation error:		0.267870
2016-11-30 21:52:45 >>> Wrote output to trainNN/out/v020-begin-at-bc-max-power-dirty/config.json
