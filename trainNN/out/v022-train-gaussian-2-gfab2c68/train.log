2016-12-01 01:27:06 >>> version=v022-train-gaussian-2-gfab2c68
2016-12-01 01:27:06 >>> loading config file extract_pfiles_python/out/v021-extract-gaussian-context40/config.json
2016-12-01 01:27:06 >>> loading numpy file extract_pfiles_python/out/v021-extract-gaussian-context40/train.npz
2016-12-01 01:27:10 >>> loading numpy file extract_pfiles_python/out/v021-extract-gaussian-context40/validate.npz
2016-12-01 01:27:11 >>> Using fuzzy_newbob as schedulung method.
2016-12-01 01:27:11 >>> Training network with 21401 trainable out of 21401 total params.
2016-12-01 01:27:11 >>> Using adam with learning_rate=0.001000
2016-12-01 01:27:11 >>> Compiling theano functions...
2016-12-01 01:27:11 >>> Starting training...
2016-12-01 01:27:32 >>>   training loss:	0.111721
2016-12-01 01:27:32 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-000.pkl
2016-12-01 01:27:33 >>> epoch: 0 took 21.873s, validation loss:		0.111696
2016-12-01 01:27:54 >>>   training loss:	0.111665
2016-12-01 01:27:54 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-001.pkl
2016-12-01 01:27:55 >>> epoch: 1 took 22.143s, validation loss:		0.111633
2016-12-01 01:28:16 >>>   training loss:	0.111658
2016-12-01 01:28:16 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-002.pkl
2016-12-01 01:28:17 >>> epoch: 2 took 21.975s, validation loss:		0.111636
2016-12-01 01:28:17 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-001.pkl
2016-12-01 01:28:17 >>> fuzzy_newbob: Updated adam with learning_rate=0.000800
2016-12-01 01:28:17 >>> Re-compiling train function...
2016-12-01 01:28:37 >>>   training loss:	0.111654
2016-12-01 01:28:37 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-003.pkl
2016-12-01 01:28:39 >>> epoch: 3 took 21.373s, validation loss:		0.111641
2016-12-01 01:28:39 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-001.pkl
2016-12-01 01:28:39 >>> fuzzy_newbob: Updated adam with learning_rate=0.000640
2016-12-01 01:28:39 >>> Re-compiling train function...
2016-12-01 01:28:59 >>>   training loss:	0.111650
2016-12-01 01:28:59 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-004.pkl
2016-12-01 01:29:01 >>> epoch: 4 took 21.785s, validation loss:		0.111652
2016-12-01 01:29:01 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-001.pkl
2016-12-01 01:29:01 >>> fuzzy_newbob: Updated adam with learning_rate=0.000512
2016-12-01 01:29:01 >>> Re-compiling train function...
2016-12-01 01:29:21 >>>   training loss:	0.111648
2016-12-01 01:29:21 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-005.pkl
2016-12-01 01:29:22 >>> epoch: 5 took 21.353s, validation loss:		0.111634
2016-12-01 01:29:22 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-001.pkl
2016-12-01 01:29:22 >>> fuzzy_newbob: Updated adam with learning_rate=0.000410
2016-12-01 01:29:22 >>> Re-compiling train function...
2016-12-01 01:29:42 >>>   training loss:	0.111644
2016-12-01 01:29:42 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-006.pkl
2016-12-01 01:29:43 >>> epoch: 6 took 21.270s, validation loss:		0.111636
2016-12-01 01:29:43 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-001.pkl
2016-12-01 01:29:43 >>> fuzzy_newbob: Updated adam with learning_rate=0.000328
2016-12-01 01:29:43 >>> Re-compiling train function...
2016-12-01 01:30:03 >>>   training loss:	0.111643
2016-12-01 01:30:03 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-007.pkl
2016-12-01 01:30:05 >>> epoch: 7 took 21.348s, validation loss:		0.111643
2016-12-01 01:30:05 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-001.pkl
2016-12-01 01:30:05 >>> fuzzy_newbob: Updated adam with learning_rate=0.000262
2016-12-01 01:30:05 >>> Re-compiling train function...
2016-12-01 01:30:25 >>>   training loss:	0.111642
2016-12-01 01:30:25 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-008.pkl
2016-12-01 01:30:26 >>> epoch: 8 took 21.368s, validation loss:		0.111628
2016-12-01 01:30:46 >>>   training loss:	0.111642
2016-12-01 01:30:46 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-009.pkl
2016-12-01 01:30:47 >>> epoch: 9 took 21.358s, validation loss:		0.111645
2016-12-01 01:30:47 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-008.pkl
2016-12-01 01:30:47 >>> fuzzy_newbob: Updated adam with learning_rate=0.000210
2016-12-01 01:30:47 >>> Re-compiling train function...
2016-12-01 01:31:07 >>>   training loss:	0.111640
2016-12-01 01:31:07 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-010.pkl
2016-12-01 01:31:09 >>> epoch: 10 took 21.386s, validation loss:		0.111631
2016-12-01 01:31:09 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-008.pkl
2016-12-01 01:31:09 >>> fuzzy_newbob: Updated adam with learning_rate=0.000168
2016-12-01 01:31:09 >>> Re-compiling train function...
2016-12-01 01:31:29 >>>   training loss:	0.111639
2016-12-01 01:31:29 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-011.pkl
2016-12-01 01:31:30 >>> epoch: 11 took 21.449s, validation loss:		0.111634
2016-12-01 01:31:30 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-008.pkl
2016-12-01 01:31:30 >>> fuzzy_newbob: Updated adam with learning_rate=0.000134
2016-12-01 01:31:30 >>> Re-compiling train function...
2016-12-01 01:31:50 >>>   training loss:	0.111638
2016-12-01 01:31:50 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-012.pkl
2016-12-01 01:31:51 >>> epoch: 12 took 21.306s, validation loss:		0.111656
2016-12-01 01:31:51 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-008.pkl
2016-12-01 01:31:51 >>> fuzzy_newbob: Updated adam with learning_rate=0.000107
2016-12-01 01:31:51 >>> Re-compiling train function...
2016-12-01 01:32:11 >>>   training loss:	0.111638
2016-12-01 01:32:11 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-013.pkl
2016-12-01 01:32:13 >>> epoch: 13 took 21.212s, validation loss:		0.111635
2016-12-01 01:32:13 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-008.pkl
2016-12-01 01:32:13 >>> fuzzy_newbob: Updated adam with learning_rate=0.000086
2016-12-01 01:32:13 >>> Re-compiling train function...
2016-12-01 01:32:33 >>>   training loss:	0.111637
2016-12-01 01:32:33 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-014.pkl
2016-12-01 01:32:34 >>> epoch: 14 took 21.362s, validation loss:		0.111633
2016-12-01 01:32:34 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-008.pkl
2016-12-01 01:32:34 >>> fuzzy_newbob: Updated adam with learning_rate=0.000069
2016-12-01 01:32:34 >>> Re-compiling train function...
2016-12-01 01:32:54 >>>   training loss:	0.111637
2016-12-01 01:32:54 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-015.pkl
2016-12-01 01:32:55 >>> epoch: 15 took 21.288s, validation loss:		0.111628
2016-12-01 01:32:55 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-008.pkl
2016-12-01 01:32:55 >>> fuzzy_newbob: Updated adam with learning_rate=0.000055
2016-12-01 01:32:55 >>> Re-compiling train function...
2016-12-01 01:33:15 >>>   training loss:	0.111636
2016-12-01 01:33:15 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:33:17 >>> epoch: 16 took 21.228s, validation loss:		0.111625
2016-12-01 01:33:36 >>>   training loss:	0.111637
2016-12-01 01:33:36 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-017.pkl
2016-12-01 01:33:38 >>> epoch: 17 took 21.278s, validation loss:		0.111634
2016-12-01 01:33:38 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:33:38 >>> fuzzy_newbob: Updated adam with learning_rate=0.000044
2016-12-01 01:33:38 >>> Re-compiling train function...
2016-12-01 01:33:58 >>>   training loss:	0.111636
2016-12-01 01:33:58 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-018.pkl
2016-12-01 01:33:59 >>> epoch: 18 took 21.217s, validation loss:		0.111629
2016-12-01 01:33:59 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:33:59 >>> fuzzy_newbob: Updated adam with learning_rate=0.000035
2016-12-01 01:33:59 >>> Re-compiling train function...
2016-12-01 01:34:19 >>>   training loss:	0.111636
2016-12-01 01:34:19 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-019.pkl
2016-12-01 01:34:20 >>> epoch: 19 took 21.205s, validation loss:		0.111629
2016-12-01 01:34:20 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:34:20 >>> fuzzy_newbob: Updated adam with learning_rate=0.000028
2016-12-01 01:34:20 >>> Re-compiling train function...
2016-12-01 01:34:40 >>>   training loss:	0.111636
2016-12-01 01:34:40 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-020.pkl
2016-12-01 01:34:41 >>> epoch: 20 took 21.222s, validation loss:		0.111628
2016-12-01 01:34:41 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:34:41 >>> fuzzy_newbob: Updated adam with learning_rate=0.000023
2016-12-01 01:34:41 >>> Re-compiling train function...
2016-12-01 01:35:01 >>>   training loss:	0.111636
2016-12-01 01:35:01 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-021.pkl
2016-12-01 01:35:03 >>> epoch: 21 took 21.201s, validation loss:		0.111631
2016-12-01 01:35:03 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:35:03 >>> fuzzy_newbob: Updated adam with learning_rate=0.000018
2016-12-01 01:35:03 >>> Re-compiling train function...
2016-12-01 01:35:22 >>>   training loss:	0.111635
2016-12-01 01:35:22 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-022.pkl
2016-12-01 01:35:24 >>> epoch: 22 took 21.216s, validation loss:		0.111635
2016-12-01 01:35:24 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:35:24 >>> fuzzy_newbob: Updated adam with learning_rate=0.000014
2016-12-01 01:35:24 >>> Re-compiling train function...
2016-12-01 01:35:44 >>>   training loss:	0.111636
2016-12-01 01:35:44 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-023.pkl
2016-12-01 01:35:45 >>> epoch: 23 took 21.197s, validation loss:		0.111635
2016-12-01 01:35:45 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:35:45 >>> fuzzy_newbob: Updated adam with learning_rate=0.000012
2016-12-01 01:35:45 >>> Re-compiling train function...
2016-12-01 01:36:05 >>>   training loss:	0.111636
2016-12-01 01:36:05 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-024.pkl
2016-12-01 01:36:06 >>> epoch: 24 took 21.229s, validation loss:		0.111627
2016-12-01 01:36:06 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:36:06 >>> fuzzy_newbob: Updated adam with learning_rate=0.000009
2016-12-01 01:36:06 >>> Re-compiling train function...
2016-12-01 01:36:26 >>>   training loss:	0.111636
2016-12-01 01:36:26 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-025.pkl
2016-12-01 01:36:28 >>> epoch: 25 took 21.222s, validation loss:		0.111636
2016-12-01 01:36:28 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:36:28 >>> fuzzy_newbob: Updated adam with learning_rate=0.000007
2016-12-01 01:36:28 >>> Re-compiling train function...
2016-12-01 01:36:47 >>>   training loss:	0.111636
2016-12-01 01:36:47 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-026.pkl
2016-12-01 01:36:49 >>> epoch: 26 took 21.335s, validation loss:		0.111628
2016-12-01 01:36:49 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:36:49 >>> fuzzy_newbob: Updated adam with learning_rate=0.000006
2016-12-01 01:36:49 >>> Re-compiling train function...
2016-12-01 01:37:09 >>>   training loss:	0.111635
2016-12-01 01:37:09 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-027.pkl
2016-12-01 01:37:10 >>> epoch: 27 took 21.224s, validation loss:		0.111628
2016-12-01 01:37:10 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:37:10 >>> fuzzy_newbob: Updated adam with learning_rate=0.000005
2016-12-01 01:37:10 >>> Re-compiling train function...
2016-12-01 01:37:30 >>>   training loss:	0.111635
2016-12-01 01:37:30 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-028.pkl
2016-12-01 01:37:31 >>> epoch: 28 took 21.213s, validation loss:		0.111637
2016-12-01 01:37:31 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:37:31 >>> fuzzy_newbob: Updated adam with learning_rate=0.000004
2016-12-01 01:37:31 >>> Re-compiling train function...
2016-12-01 01:37:51 >>>   training loss:	0.111635
2016-12-01 01:37:51 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-029.pkl
2016-12-01 01:37:53 >>> epoch: 29 took 21.307s, validation loss:		0.111634
2016-12-01 01:37:53 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:37:53 >>> fuzzy_newbob: Updated adam with learning_rate=0.000003
2016-12-01 01:37:53 >>> Re-compiling train function...
2016-12-01 01:38:12 >>>   training loss:	0.111635
2016-12-01 01:38:12 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-030.pkl
2016-12-01 01:38:14 >>> epoch: 30 took 21.235s, validation loss:		0.111630
2016-12-01 01:38:14 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:38:14 >>> fuzzy_newbob: Updated adam with learning_rate=0.000002
2016-12-01 01:38:14 >>> Re-compiling train function...
2016-12-01 01:38:34 >>>   training loss:	0.111635
2016-12-01 01:38:34 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-031.pkl
2016-12-01 01:38:35 >>> epoch: 31 took 21.306s, validation loss:		0.111630
2016-12-01 01:38:35 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:38:35 >>> fuzzy_newbob: Updated adam with learning_rate=0.000002
2016-12-01 01:38:35 >>> Re-compiling train function...
2016-12-01 01:38:55 >>>   training loss:	0.111635
2016-12-01 01:38:55 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-032.pkl
2016-12-01 01:38:56 >>> epoch: 32 took 21.298s, validation loss:		0.111630
2016-12-01 01:38:56 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:38:56 >>> fuzzy_newbob: Updated adam with learning_rate=0.000002
2016-12-01 01:38:56 >>> Re-compiling train function...
2016-12-01 01:39:16 >>>   training loss:	0.111635
2016-12-01 01:39:16 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-033.pkl
2016-12-01 01:39:18 >>> epoch: 33 took 21.226s, validation loss:		0.111630
2016-12-01 01:39:18 >>> Loading old params from trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-016.pkl
2016-12-01 01:39:18 >>> fuzzy_newbob: Updated adam with learning_rate=0.000001
2016-12-01 01:39:18 >>> Re-compiling train function...
2016-12-01 01:39:37 >>>   training loss:	0.111635
2016-12-01 01:39:37 >>> Saving network params to trainNN/out/v022-train-gaussian-2-gfab2c68/epoch-034.pkl
2016-12-01 01:39:39 >>> epoch: 34 took 21.236s, validation loss:		0.111631
2016-12-01 01:39:39 >>> learning rate below 1e-06, ending
2016-12-01 01:39:39 >>> Wrote output to trainNN/out/v022-train-gaussian-2-gfab2c68/config.json
